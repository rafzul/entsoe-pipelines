[2023-01-30T10:54:37.992+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-30T10:54:38.319+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-30T10:54:38.320+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-30T10:54:38.320+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-30T10:54:38.321+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-30T10:54:38.874+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 04:00:00+00:00
[2023-01-30T10:54:38.978+0000] {standard_task_runner.py:55} INFO - Started process 216 to run task
[2023-01-30T10:54:39.095+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T04:00:00+00:00', '--job-id', '528', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpgngm0tlf']
[2023-01-30T10:54:39.186+0000] {standard_task_runner.py:83} INFO - Job 528: Subtask stage_total_generation
[2023-01-30T10:54:40.608+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [running]> on host 5b6f5d29e531
[2023-01-30T10:54:41.632+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T04:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T04:00:00+00:00
[2023-01-30T10:54:41.735+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-30T10:54:41.738+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 398, in submit
    spark_submit_cmd = self._build_spark_submit_command(application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 344, in _build_spark_submit_command
    self.log.info("Spark-Submit cmd: %s", self._mask_cmd(connection_cmd))
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 256, in _mask_cmd
    " ".join(connection_cmd),
TypeError: sequence item 5: expected str instance, module found
[2023-01-30T10:54:41.852+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata, task_id=stage_total_generation, execution_date=20210101T040000, start_date=20230130T105437, end_date=20230130T105441
[2023-01-30T10:54:41.965+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 528 for task stage_total_generation (sequence item 5: expected str instance, module found; 216)
[2023-01-30T10:54:42.047+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-30T10:54:42.175+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-30T10:57:29.075+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-30T10:57:29.136+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-30T10:57:29.136+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-30T10:57:29.137+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-30T10:57:29.137+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-30T10:57:29.230+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 04:00:00+00:00
[2023-01-30T10:57:29.265+0000] {standard_task_runner.py:55} INFO - Started process 511 to run task
[2023-01-30T10:57:29.285+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T04:00:00+00:00', '--job-id', '547', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpw__5i3vm']
[2023-01-30T10:57:29.302+0000] {standard_task_runner.py:83} INFO - Job 547: Subtask stage_total_generation
[2023-01-30T10:57:29.630+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [running]> on host 5b6f5d29e531
[2023-01-30T10:57:30.021+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T04:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T04:00:00+00:00
[2023-01-30T10:57:30.066+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-30T10:57:30.070+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET
[2023-01-30T10:58:06.151+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:06 INFO SparkContext: Running Spark version 3.3.1
[2023-01-30T10:58:06.672+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-30T10:58:07.127+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:07 INFO ResourceUtils: ==============================================================
[2023-01-30T10:58:07.137+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:07 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-30T10:58:07.145+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:07 INFO ResourceUtils: ==============================================================
[2023-01-30T10:58:07.148+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:07 INFO SparkContext: Submitted application: gcp_playground
[2023-01-30T10:58:07.332+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-30T10:58:07.391+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:07 INFO ResourceProfile: Limiting resource is cpu
[2023-01-30T10:58:07.405+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:07 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-30T10:58:07.824+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:07 INFO SecurityManager: Changing view acls to: ***
[2023-01-30T10:58:07.832+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:07 INFO SecurityManager: Changing modify acls to: ***
[2023-01-30T10:58:07.843+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:07 INFO SecurityManager: Changing view acls groups to:
[2023-01-30T10:58:07.850+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:07 INFO SecurityManager: Changing modify acls groups to:
[2023-01-30T10:58:07.866+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-30T10:58:11.672+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:11 INFO Utils: Successfully started service 'sparkDriver' on port 36105.
[2023-01-30T10:58:12.035+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:12 INFO SparkEnv: Registering MapOutputTracker
[2023-01-30T10:58:12.465+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:12 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-30T10:58:12.936+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:12 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-30T10:58:12.949+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:12 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-30T10:58:12.989+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:12 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-30T10:58:13.146+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-122ae528-59e4-4fb7-b032-dc0eb869dc21
[2023-01-30T10:58:13.202+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:13 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-30T10:58:13.308+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:13 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-30T10:58:14.991+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-30T10:58:14.995+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:14 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-30T10:58:14.999+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:14 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-30T10:58:15.001+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:14 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-30T10:58:15.002+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:14 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-30T10:58:15.063+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:15 INFO Utils: Successfully started service 'SparkUI' on port 4045.
[2023-01-30T10:58:16.039+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:16 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://5b6f5d29e531:36105/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675076286128
[2023-01-30T10:58:16.046+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:16 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://5b6f5d29e531:36105/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675076286128
[2023-01-30T10:58:17.069+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:17 INFO Executor: Starting executor ID driver on host 5b6f5d29e531
[2023-01-30T10:58:17.231+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:17 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-30T10:58:17.451+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:17 INFO Executor: Fetching spark://5b6f5d29e531:36105/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675076286128
[2023-01-30T10:58:18.000+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:17 INFO TransportClientFactory: Successfully created connection to 5b6f5d29e531/172.21.0.7:36105 after 316 ms (0 ms spent in bootstraps)
[2023-01-30T10:58:18.129+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:18 INFO Utils: Fetching spark://5b6f5d29e531:36105/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-be50bf2f-38bb-4a9d-b8b5-475b974868fe/userFiles-1bb8e340-ff97-43a9-b95c-47c1acf3598d/fetchFileTemp12343677005978119264.tmp
[2023-01-30T10:58:20.884+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:20 INFO Executor: Adding file:/tmp/spark-be50bf2f-38bb-4a9d-b8b5-475b974868fe/userFiles-1bb8e340-ff97-43a9-b95c-47c1acf3598d/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-30T10:58:20.888+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:20 INFO Executor: Fetching spark://5b6f5d29e531:36105/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675076286128
[2023-01-30T10:58:20.913+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:20 INFO Utils: Fetching spark://5b6f5d29e531:36105/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-be50bf2f-38bb-4a9d-b8b5-475b974868fe/userFiles-1bb8e340-ff97-43a9-b95c-47c1acf3598d/fetchFileTemp6621125228229599947.tmp
[2023-01-30T10:58:22.687+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:22 INFO Executor: Adding file:/tmp/spark-be50bf2f-38bb-4a9d-b8b5-475b974868fe/userFiles-1bb8e340-ff97-43a9-b95c-47c1acf3598d/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-30T10:58:22.963+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:22 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33081.
[2023-01-30T10:58:22.964+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:22 INFO NettyBlockTransferService: Server created on 5b6f5d29e531:33081
[2023-01-30T10:58:22.984+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:22 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-30T10:58:23.062+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 5b6f5d29e531, 33081, None)
[2023-01-30T10:58:23.092+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:23 INFO BlockManagerMasterEndpoint: Registering block manager 5b6f5d29e531:33081 with 434.4 MiB RAM, BlockManagerId(driver, 5b6f5d29e531, 33081, None)
[2023-01-30T10:58:23.121+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 5b6f5d29e531, 33081, None)
[2023-01-30T10:58:23.127+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 5b6f5d29e531, 33081, None)
[2023-01-30T10:58:30.270+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:30 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-30T10:58:30.373+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:30 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-30T10:58:48.856+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-30T10:58:48.857+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 233, in <module>
[2023-01-30T10:58:48.857+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-30T10:58:48.858+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 220, in main
[2023-01-30T10:58:48.858+0000] {spark_submit.py:495} INFO - run_method = method(metrics_label, start, end, country_code, **params)
[2023-01-30T10:58:48.858+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 176, in transform_generation
[2023-01-30T10:58:48.859+0000] {spark_submit.py:495} INFO - full_df = self._base_timeseries(metrics_label, start, end, country_code)
[2023-01-30T10:58:48.866+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 82, in _base_timeseries
[2023-01-30T10:58:48.867+0000] {spark_submit.py:495} INFO - raise e
[2023-01-30T10:58:48.870+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 79, in _base_timeseries
[2023-01-30T10:58:48.876+0000] {spark_submit.py:495} INFO - .load(path)
[2023-01-30T10:58:48.879+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 177, in load
[2023-01-30T10:58:48.881+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
[2023-01-30T10:58:48.882+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 196, in deco
[2023-01-30T10:58:48.924+0000] {spark_submit.py:495} INFO - pyspark.sql.utils.AnalysisException: Path does not exist: gs://entsoe_analytics_1009/TEST_total_generation__DE_TENNET__202101010400__202101010500.json
[2023-01-30T10:58:49.739+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:49 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-30T10:58:49.984+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:49 INFO SparkUI: Stopped Spark web UI at http://5b6f5d29e531:4045
[2023-01-30T10:58:50.328+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:50 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-30T10:58:50.430+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:50 INFO MemoryStore: MemoryStore cleared
[2023-01-30T10:58:50.433+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:50 INFO BlockManager: BlockManager stopped
[2023-01-30T10:58:50.482+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:50 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-30T10:58:50.499+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-30T10:58:50.606+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:50 INFO SparkContext: Successfully stopped SparkContext
[2023-01-30T10:58:50.610+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:50 INFO ShutdownHookManager: Shutdown hook called
[2023-01-30T10:58:50.616+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-be50bf2f-38bb-4a9d-b8b5-475b974868fe/pyspark-3bd96951-a912-4b8c-b284-46eadf6e3c66
[2023-01-30T10:58:50.649+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-99fe8b5e-f1ad-4918-b9fe-81857de7e5ba
[2023-01-30T10:58:50.672+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-be50bf2f-38bb-4a9d-b8b5-475b974868fe
[2023-01-30T10:58:50.966+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET. Error code is: 1.
[2023-01-30T10:58:50.972+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata, task_id=stage_total_generation, execution_date=20210101T040000, start_date=20230130T105729, end_date=20230130T105850
[2023-01-30T10:58:51.001+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 547 for task stage_total_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET. Error code is: 1.; 511)
[2023-01-30T10:58:51.056+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-30T10:58:51.096+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-30T18:09:26.373+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-30T18:09:26.442+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-30T18:09:26.443+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-30T18:09:26.445+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-30T18:09:26.446+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-30T18:09:26.647+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 04:00:00+00:00
[2023-01-30T18:09:26.732+0000] {standard_task_runner.py:55} INFO - Started process 13044 to run task
[2023-01-30T18:09:26.847+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T04:00:00+00:00', '--job-id', '576', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmphvofcd1c']
[2023-01-30T18:09:26.877+0000] {standard_task_runner.py:83} INFO - Job 576: Subtask stage_total_generation
[2023-01-30T18:09:27.413+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [running]> on host 64b72c0214de
[2023-01-30T18:09:27.788+0000] {abstractoperator.py:592} ERROR - Exception rendering Jinja template for task 'stage_total_generation', field '_application_args'. Template: ['total_generation', "{{ data_interval_start.format('YYYYMMDDHHmm') }}", "{{ data_interval_end.format('YYYYMMDDHHmm') }}", '{{ params.country_code }}', '{{ start_date }}', '{{ end_date }}']
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/abstractoperator.py", line 585, in _do_render_template_fields
    rendered_content = self.render_template(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/abstractoperator.py", line 657, in render_template
    return [self.render_template(element, context, jinja_env, oids) for element in value]
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/abstractoperator.py", line 657, in <listcomp>
    return [self.render_template(element, context, jinja_env, oids) for element in value]
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/abstractoperator.py", line 646, in render_template
    return render_template_to_string(template, context)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/helpers.py", line 288, in render_template_to_string
    return render_template(template, cast(MutableMapping[str, Any], context), native=False)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/helpers.py", line 283, in render_template
    return "".join(nodes)
  File "<template>", line 12, in root
  File "/home/airflow/.local/lib/python3.10/site-packages/jinja2/runtime.py", line 852, in _fail_with_undefined_error
    raise self._undefined_exception(self._undefined_message)
jinja2.exceptions.UndefinedError: 'start_date' is undefined
[2023-01-30T18:09:27.795+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 1378, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 1497, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 2119, in render_templates
    original_task.render_template_fields(context)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1196, in render_template_fields
    self._do_render_template_fields(self, self.template_fields, context, jinja_env, set())
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/abstractoperator.py", line 585, in _do_render_template_fields
    rendered_content = self.render_template(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/abstractoperator.py", line 657, in render_template
    return [self.render_template(element, context, jinja_env, oids) for element in value]
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/abstractoperator.py", line 657, in <listcomp>
    return [self.render_template(element, context, jinja_env, oids) for element in value]
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/abstractoperator.py", line 646, in render_template
    return render_template_to_string(template, context)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/helpers.py", line 288, in render_template_to_string
    return render_template(template, cast(MutableMapping[str, Any], context), native=False)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/helpers.py", line 283, in render_template
    return "".join(nodes)
  File "<template>", line 12, in root
  File "/home/airflow/.local/lib/python3.10/site-packages/jinja2/runtime.py", line 852, in _fail_with_undefined_error
    raise self._undefined_exception(self._undefined_message)
jinja2.exceptions.UndefinedError: 'start_date' is undefined
[2023-01-30T18:09:27.863+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata, task_id=stage_total_generation, execution_date=20210101T040000, start_date=20230130T180926, end_date=20230130T180927
[2023-01-30T18:09:27.939+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 576 for task stage_total_generation ('start_date' is undefined; 13044)
[2023-01-30T18:09:28.009+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-30T18:09:28.151+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-30T18:17:26.508+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-30T18:17:26.532+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-30T18:17:26.532+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-30T18:17:26.532+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-30T18:17:26.532+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-30T18:17:26.567+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 04:00:00+00:00
[2023-01-30T18:17:26.587+0000] {standard_task_runner.py:55} INFO - Started process 14136 to run task
[2023-01-30T18:17:26.607+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T04:00:00+00:00', '--job-id', '599', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpdjcyr4hw']
[2023-01-30T18:17:26.612+0000] {standard_task_runner.py:83} INFO - Job 599: Subtask stage_total_generation
[2023-01-30T18:17:26.782+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [running]> on host 64b72c0214de
[2023-01-30T18:17:27.123+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T04:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T04:00:00+00:00
[2023-01-30T18:17:27.152+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-30T18:17:27.155+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET
[2023-01-30T18:17:48.239+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:48 INFO SparkContext: Running Spark version 3.3.1
[2023-01-30T18:17:49.048+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-30T18:17:50.010+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:50 INFO ResourceUtils: ==============================================================
[2023-01-30T18:17:50.013+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:50 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-30T18:17:50.015+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:50 INFO ResourceUtils: ==============================================================
[2023-01-30T18:17:50.018+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:50 INFO SparkContext: Submitted application: gcp_playground
[2023-01-30T18:17:50.132+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:50 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-30T18:17:50.167+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:50 INFO ResourceProfile: Limiting resource is cpu
[2023-01-30T18:17:50.177+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:50 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-30T18:17:50.711+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:50 INFO SecurityManager: Changing view acls to: ***
[2023-01-30T18:17:50.731+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:50 INFO SecurityManager: Changing modify acls to: ***
[2023-01-30T18:17:50.741+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:50 INFO SecurityManager: Changing view acls groups to:
[2023-01-30T18:17:50.744+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:50 INFO SecurityManager: Changing modify acls groups to:
[2023-01-30T18:17:50.771+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-30T18:17:55.374+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:55 INFO Utils: Successfully started service 'sparkDriver' on port 42405.
[2023-01-30T18:17:55.699+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:55 INFO SparkEnv: Registering MapOutputTracker
[2023-01-30T18:17:56.119+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:56 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-30T18:17:56.278+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:56 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-30T18:17:56.283+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:56 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-30T18:17:56.305+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:56 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-30T18:17:56.429+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:56 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-77785f7c-42d2-40d9-8592-78af85c6ccfa
[2023-01-30T18:17:56.659+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:56 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-30T18:17:58.047+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:58 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-30T18:18:04.080+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-30T18:18:04.084+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:04 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-30T18:18:04.105+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:04 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-30T18:18:04.112+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:04 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-30T18:18:04.148+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:04 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-30T18:18:04.327+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:04 INFO Utils: Successfully started service 'SparkUI' on port 4045.
[2023-01-30T18:18:05.383+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:05 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://64b72c0214de:42405/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675102668193
[2023-01-30T18:18:05.390+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:05 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://64b72c0214de:42405/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675102668193
[2023-01-30T18:18:06.319+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:06 INFO Executor: Starting executor ID driver on host 64b72c0214de
[2023-01-30T18:18:06.448+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:06 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-30T18:18:06.704+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:06 INFO Executor: Fetching spark://64b72c0214de:42405/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675102668193
[2023-01-30T18:18:07.542+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:07 INFO TransportClientFactory: Successfully created connection to 64b72c0214de/172.19.0.6:42405 after 148 ms (0 ms spent in bootstraps)
[2023-01-30T18:18:07.682+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:07 INFO Utils: Fetching spark://64b72c0214de:42405/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-ac8828dc-5708-40bb-9ed1-5f50dd6b651b/userFiles-6b3d80ee-4690-429c-9c95-90cd656019cb/fetchFileTemp12243271132552776159.tmp
[2023-01-30T18:18:12.697+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:12 INFO Executor: Adding file:/tmp/spark-ac8828dc-5708-40bb-9ed1-5f50dd6b651b/userFiles-6b3d80ee-4690-429c-9c95-90cd656019cb/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-30T18:18:12.701+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:12 INFO Executor: Fetching spark://64b72c0214de:42405/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675102668193
[2023-01-30T18:18:12.714+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:12 INFO Utils: Fetching spark://64b72c0214de:42405/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-ac8828dc-5708-40bb-9ed1-5f50dd6b651b/userFiles-6b3d80ee-4690-429c-9c95-90cd656019cb/fetchFileTemp14770005604245965228.tmp
[2023-01-30T18:18:15.737+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:15 INFO Executor: Adding file:/tmp/spark-ac8828dc-5708-40bb-9ed1-5f50dd6b651b/userFiles-6b3d80ee-4690-429c-9c95-90cd656019cb/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-30T18:18:15.852+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37187.
[2023-01-30T18:18:15.853+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:15 INFO NettyBlockTransferService: Server created on 64b72c0214de:37187
[2023-01-30T18:18:15.870+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:15 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-30T18:18:15.935+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 64b72c0214de, 37187, None)
[2023-01-30T18:18:16.018+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:15 INFO BlockManagerMasterEndpoint: Registering block manager 64b72c0214de:37187 with 434.4 MiB RAM, BlockManagerId(driver, 64b72c0214de, 37187, None)
[2023-01-30T18:18:16.086+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 64b72c0214de, 37187, None)
[2023-01-30T18:18:16.092+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 64b72c0214de, 37187, None)
[2023-01-30T18:18:24.370+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:24 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-30T18:18:24.402+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:24 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-30T18:18:38.794+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-30T18:18:38.796+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 266, in <module>
[2023-01-30T18:18:38.796+0000] {spark_submit.py:495} INFO - main(
[2023-01-30T18:18:38.797+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 243, in main
[2023-01-30T18:18:38.797+0000] {spark_submit.py:495} INFO - run_method = method(
[2023-01-30T18:18:38.798+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 185, in transform_generation
[2023-01-30T18:18:38.799+0000] {spark_submit.py:495} INFO - full_df = self._base_timeseries(
[2023-01-30T18:18:38.800+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 84, in _base_timeseries
[2023-01-30T18:18:38.800+0000] {spark_submit.py:495} INFO - raise e
[2023-01-30T18:18:38.801+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 81, in _base_timeseries
[2023-01-30T18:18:38.801+0000] {spark_submit.py:495} INFO - .load(path)
[2023-01-30T18:18:38.802+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 177, in load
[2023-01-30T18:18:38.802+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
[2023-01-30T18:18:38.802+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 196, in deco
[2023-01-30T18:18:38.863+0000] {spark_submit.py:495} INFO - pyspark.sql.utils.AnalysisException: Path does not exist: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010600.json
[2023-01-30T18:18:39.200+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:39 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-30T18:18:39.536+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:39 INFO SparkUI: Stopped Spark web UI at http://64b72c0214de:4045
[2023-01-30T18:18:39.708+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-30T18:18:39.860+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:39 INFO MemoryStore: MemoryStore cleared
[2023-01-30T18:18:39.864+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:39 INFO BlockManager: BlockManager stopped
[2023-01-30T18:18:39.898+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:39 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-30T18:18:39.915+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-30T18:18:40.021+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:40 INFO SparkContext: Successfully stopped SparkContext
[2023-01-30T18:18:40.029+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:40 INFO ShutdownHookManager: Shutdown hook called
[2023-01-30T18:18:40.032+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-5356b892-e8de-4e91-a83e-e4a3651482ca
[2023-01-30T18:18:40.093+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-ac8828dc-5708-40bb-9ed1-5f50dd6b651b/pyspark-f6d2d9be-32c6-4d91-9fc5-b4961402ab1f
[2023-01-30T18:18:40.105+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-ac8828dc-5708-40bb-9ed1-5f50dd6b651b
[2023-01-30T18:18:40.387+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET. Error code is: 1.
[2023-01-30T18:18:40.422+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata, task_id=stage_total_generation, execution_date=20210101T040000, start_date=20230130T181726, end_date=20230130T181840
[2023-01-30T18:18:40.541+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 599 for task stage_total_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET. Error code is: 1.; 14136)
[2023-01-30T18:18:40.608+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-30T18:18:40.673+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-30T18:24:56.936+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-30T18:24:56.977+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-30T18:24:56.979+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-30T18:24:56.980+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-30T18:24:56.980+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-30T18:24:57.030+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 04:00:00+00:00
[2023-01-30T18:24:57.055+0000] {standard_task_runner.py:55} INFO - Started process 16528 to run task
[2023-01-30T18:24:57.077+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T04:00:00+00:00', '--job-id', '619', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmptt8t8zaq']
[2023-01-30T18:24:57.095+0000] {standard_task_runner.py:83} INFO - Job 619: Subtask stage_total_generation
[2023-01-30T18:24:57.455+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [running]> on host 64b72c0214de
[2023-01-30T18:24:57.753+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T04:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T04:00:00+00:00
[2023-01-30T18:24:57.796+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-30T18:24:57.804+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET
[2023-01-30T18:25:17.838+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:17 INFO SparkContext: Running Spark version 3.3.1
[2023-01-30T18:25:18.746+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-30T18:25:19.442+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:19 INFO ResourceUtils: ==============================================================
[2023-01-30T18:25:19.449+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:19 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-30T18:25:19.450+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:19 INFO ResourceUtils: ==============================================================
[2023-01-30T18:25:19.452+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:19 INFO SparkContext: Submitted application: gcp_playground
[2023-01-30T18:25:19.609+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:19 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-30T18:25:19.639+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:19 INFO ResourceProfile: Limiting resource is cpu
[2023-01-30T18:25:19.658+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:19 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-30T18:25:20.177+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:20 INFO SecurityManager: Changing view acls to: ***
[2023-01-30T18:25:20.189+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:20 INFO SecurityManager: Changing modify acls to: ***
[2023-01-30T18:25:20.201+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:20 INFO SecurityManager: Changing view acls groups to:
[2023-01-30T18:25:20.205+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:20 INFO SecurityManager: Changing modify acls groups to:
[2023-01-30T18:25:20.220+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-30T18:25:22.858+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:22 INFO Utils: Successfully started service 'sparkDriver' on port 33545.
[2023-01-30T18:25:23.285+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:23 INFO SparkEnv: Registering MapOutputTracker
[2023-01-30T18:25:23.541+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:23 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-30T18:25:23.640+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-30T18:25:23.652+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-30T18:25:23.675+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-30T18:25:23.754+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f0ee95fe-aead-4290-a26a-3855545daf42
[2023-01-30T18:25:23.877+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:23 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-30T18:25:24.026+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:24 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-30T18:25:25.381+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-30T18:25:25.392+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:25 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-30T18:25:25.395+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:25 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-30T18:25:25.397+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:25 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-30T18:25:25.404+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:25 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-30T18:25:25.426+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:25 INFO Utils: Successfully started service 'SparkUI' on port 4045.
[2023-01-30T18:25:25.599+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:25 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://64b72c0214de:33545/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675103117727
[2023-01-30T18:25:25.605+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:25 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://64b72c0214de:33545/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675103117727
[2023-01-30T18:25:25.925+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:25 INFO Executor: Starting executor ID driver on host 64b72c0214de
[2023-01-30T18:25:25.939+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:25 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-30T18:25:25.969+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:25 INFO Executor: Fetching spark://64b72c0214de:33545/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675103117727
[2023-01-30T18:25:26.175+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:26 INFO TransportClientFactory: Successfully created connection to 64b72c0214de/172.19.0.6:33545 after 159 ms (0 ms spent in bootstraps)
[2023-01-30T18:25:26.280+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:26 INFO Utils: Fetching spark://64b72c0214de:33545/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-58957c94-2482-407d-9dd0-4f4be2f3b7c6/userFiles-3285344e-cb50-4a2e-b338-93de51fa8f69/fetchFileTemp125199189865678431.tmp
[2023-01-30T18:25:27.383+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:27 INFO Executor: Adding file:/tmp/spark-58957c94-2482-407d-9dd0-4f4be2f3b7c6/userFiles-3285344e-cb50-4a2e-b338-93de51fa8f69/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-30T18:25:27.383+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:27 INFO Executor: Fetching spark://64b72c0214de:33545/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675103117727
[2023-01-30T18:25:27.389+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:27 INFO Utils: Fetching spark://64b72c0214de:33545/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-58957c94-2482-407d-9dd0-4f4be2f3b7c6/userFiles-3285344e-cb50-4a2e-b338-93de51fa8f69/fetchFileTemp7782263546369881245.tmp
[2023-01-30T18:25:27.980+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:27 INFO Executor: Adding file:/tmp/spark-58957c94-2482-407d-9dd0-4f4be2f3b7c6/userFiles-3285344e-cb50-4a2e-b338-93de51fa8f69/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-30T18:25:28.008+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33449.
[2023-01-30T18:25:28.008+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:28 INFO NettyBlockTransferService: Server created on 64b72c0214de:33449
[2023-01-30T18:25:28.010+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:28 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-30T18:25:28.037+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 64b72c0214de, 33449, None)
[2023-01-30T18:25:28.068+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:28 INFO BlockManagerMasterEndpoint: Registering block manager 64b72c0214de:33449 with 434.4 MiB RAM, BlockManagerId(driver, 64b72c0214de, 33449, None)
[2023-01-30T18:25:28.076+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 64b72c0214de, 33449, None)
[2023-01-30T18:25:28.078+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:28 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 64b72c0214de, 33449, None)
[2023-01-30T18:25:30.679+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:30 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-30T18:25:30.710+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:30 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-30T18:25:38.519+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:38 INFO InMemoryFileIndex: It took 400 ms to list leaf files for 1 paths.
[2023-01-30T18:25:39.406+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-30T18:25:40.024+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-30T18:25:40.039+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 64b72c0214de:33449 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-30T18:25:40.074+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:40 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-30T18:25:43.196+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:43 INFO FileInputFormat: Total input files to process : 1
[2023-01-30T18:25:43.310+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:43 INFO FileInputFormat: Total input files to process : 1
[2023-01-30T18:25:43.424+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:43 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-30T18:25:43.508+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:43 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-30T18:25:43.510+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:43 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-30T18:25:43.513+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:43 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:25:43.520+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:43 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:25:43.536+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:43 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-30T18:25:43.947+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:43 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-30T18:25:43.964+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:43 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-30T18:25:43.966+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 64b72c0214de:33449 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-30T18:25:43.969+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:43 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:25:44.047+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:25:44.060+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:44 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-30T18:25:44.326+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:44 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (64b72c0214de, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-30T18:25:44.409+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:44 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-30T18:25:44.733+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:44 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010400__202101010500.json:0+9364
[2023-01-30T18:25:45.584+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:45 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-30T18:25:45.735+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:45 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1463 ms on 64b72c0214de (executor driver) (1/1)
[2023-01-30T18:25:45.758+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:45 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-30T18:25:45.811+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:45 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 2.181 s
[2023-01-30T18:25:45.835+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:45 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:25:45.839+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-30T18:25:45.848+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:45 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 2.423418 s
[2023-01-30T18:25:47.059+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:47 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 64b72c0214de:33449 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-30T18:25:47.152+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:47 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 64b72c0214de:33449 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-30T18:35:24.927+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-30T18:35:24.951+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-30T18:35:24.952+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-30T18:35:24.952+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-30T18:35:24.952+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-30T18:35:24.985+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 04:00:00+00:00
[2023-01-30T18:35:25.005+0000] {standard_task_runner.py:55} INFO - Started process 193 to run task
[2023-01-30T18:35:25.013+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T04:00:00+00:00', '--job-id', '652', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp3spbk37v']
[2023-01-30T18:35:25.018+0000] {standard_task_runner.py:83} INFO - Job 652: Subtask stage_total_generation
[2023-01-30T18:35:25.178+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [running]> on host 767c6fe0d003
[2023-01-30T18:35:25.353+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T04:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T04:00:00+00:00
[2023-01-30T18:35:25.371+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-30T18:35:25.373+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET
[2023-01-30T18:35:44.961+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:44 INFO SparkContext: Running Spark version 3.3.1
[2023-01-30T18:35:45.394+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-30T18:35:46.143+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:46 INFO ResourceUtils: ==============================================================
[2023-01-30T18:35:46.156+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:46 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-30T18:35:46.159+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:46 INFO ResourceUtils: ==============================================================
[2023-01-30T18:35:46.159+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:46 INFO SparkContext: Submitted application: gcp_playground
[2023-01-30T18:35:46.349+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:46 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-30T18:35:46.389+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:46 INFO ResourceProfile: Limiting resource is cpu
[2023-01-30T18:35:46.397+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:46 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-30T18:35:46.602+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:46 INFO SecurityManager: Changing view acls to: ***
[2023-01-30T18:35:46.611+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:46 INFO SecurityManager: Changing modify acls to: ***
[2023-01-30T18:35:46.620+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:46 INFO SecurityManager: Changing view acls groups to:
[2023-01-30T18:35:46.630+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:46 INFO SecurityManager: Changing modify acls groups to:
[2023-01-30T18:35:46.632+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-30T18:35:48.175+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:48 INFO Utils: Successfully started service 'sparkDriver' on port 34697.
[2023-01-30T18:35:48.533+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:48 INFO SparkEnv: Registering MapOutputTracker
[2023-01-30T18:35:48.685+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:48 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-30T18:35:48.801+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-30T18:35:48.815+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-30T18:35:48.815+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:48 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-30T18:35:48.899+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b27fffcd-faa9-4bb1-a7b4-3ffedc427691
[2023-01-30T18:35:49.041+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:49 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-30T18:35:49.219+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:49 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-30T18:35:51.161+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-30T18:35:51.218+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:51 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-30T18:35:51.219+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:51 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-30T18:35:51.510+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:51 INFO Utils: Successfully started service 'SparkUI' on port 4043.
[2023-01-30T18:35:51.943+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:51 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://767c6fe0d003:34697/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675103744857
[2023-01-30T18:35:51.944+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:51 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://767c6fe0d003:34697/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675103744857
[2023-01-30T18:35:52.771+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:52 INFO Executor: Starting executor ID driver on host 767c6fe0d003
[2023-01-30T18:35:52.815+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:52 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-30T18:35:53.008+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:53 INFO Executor: Fetching spark://767c6fe0d003:34697/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675103744857
[2023-01-30T18:35:53.491+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:53 INFO TransportClientFactory: Successfully created connection to 767c6fe0d003/172.19.0.9:34697 after 278 ms (0 ms spent in bootstraps)
[2023-01-30T18:35:53.564+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:53 INFO Utils: Fetching spark://767c6fe0d003:34697/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-460b3df8-92d3-4d8c-9cd6-887fc0f9b2db/userFiles-1a4f4627-f600-4775-a678-00156759f53d/fetchFileTemp12389075243921542327.tmp
[2023-01-30T18:35:55.153+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:55 INFO Executor: Adding file:/tmp/spark-460b3df8-92d3-4d8c-9cd6-887fc0f9b2db/userFiles-1a4f4627-f600-4775-a678-00156759f53d/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-30T18:35:55.154+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:55 INFO Executor: Fetching spark://767c6fe0d003:34697/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675103744857
[2023-01-30T18:35:55.154+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:55 INFO Utils: Fetching spark://767c6fe0d003:34697/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-460b3df8-92d3-4d8c-9cd6-887fc0f9b2db/userFiles-1a4f4627-f600-4775-a678-00156759f53d/fetchFileTemp11545197748750663215.tmp
[2023-01-30T18:35:56.049+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:56 INFO Executor: Adding file:/tmp/spark-460b3df8-92d3-4d8c-9cd6-887fc0f9b2db/userFiles-1a4f4627-f600-4775-a678-00156759f53d/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-30T18:35:56.131+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33173.
[2023-01-30T18:35:56.134+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:56 INFO NettyBlockTransferService: Server created on 767c6fe0d003:33173
[2023-01-30T18:35:56.142+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-30T18:35:56.191+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 767c6fe0d003, 33173, None)
[2023-01-30T18:35:56.208+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:56 INFO BlockManagerMasterEndpoint: Registering block manager 767c6fe0d003:33173 with 434.4 MiB RAM, BlockManagerId(driver, 767c6fe0d003, 33173, None)
[2023-01-30T18:35:56.223+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 767c6fe0d003, 33173, None)
[2023-01-30T18:35:56.234+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 767c6fe0d003, 33173, None)
[2023-01-30T18:36:04.103+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:04 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-30T18:36:04.126+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:04 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-30T18:36:22.725+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:22 INFO InMemoryFileIndex: It took 1290 ms to list leaf files for 1 paths.
[2023-01-30T18:36:25.932+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-30T18:36:26.713+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-30T18:36:26.724+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 767c6fe0d003:33173 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-30T18:36:26.739+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:26 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-30T18:36:30.443+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:30 INFO FileInputFormat: Total input files to process : 1
[2023-01-30T18:36:30.620+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:30 INFO FileInputFormat: Total input files to process : 1
[2023-01-30T18:36:30.715+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:30 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-30T18:36:30.867+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:30 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-30T18:36:30.867+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:30 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-30T18:36:30.875+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:36:30.882+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:30 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:36:30.932+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:30 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-30T18:36:31.342+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:31 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-30T18:36:31.352+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:31 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-30T18:36:31.357+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 767c6fe0d003:33173 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-30T18:36:31.363+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:31 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:36:31.430+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:36:31.434+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-30T18:36:32.135+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:32 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-30T18:36:32.361+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:32 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-30T18:36:33.563+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:33 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010400__202101010500.json:0+9364
[2023-01-30T18:36:35.995+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:35 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-30T18:36:36.222+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:36 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4331 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:36:36.270+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:36 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-30T18:36:36.281+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:36 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 5.242 s
[2023-01-30T18:36:36.369+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:36 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:36:36.422+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-30T18:36:36.424+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:36 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 5.707623 s
[2023-01-30T18:36:38.105+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:38 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 767c6fe0d003:33173 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-30T18:36:38.210+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:38 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 767c6fe0d003:33173 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-30T18:37:07.087+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:07 INFO FileSourceStrategy: Pushed Filters:
[2023-01-30T18:37:07.095+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:07 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-30T18:37:07.103+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:07 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-30T18:37:10.107+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:10 INFO CodeGenerator: Code generated in 2033.872122 ms
[2023-01-30T18:37:10.170+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:10 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-30T18:37:10.290+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:10 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-30T18:37:10.297+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:10 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 767c6fe0d003:33173 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-30T18:37:10.305+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:10 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:42
[2023-01-30T18:37:10.354+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-30T18:37:10.577+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:10 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:42
[2023-01-30T18:37:10.586+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:10 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:42) with 1 output partitions
[2023-01-30T18:37:10.587+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:10 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:42)
[2023-01-30T18:37:10.590+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:10 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:37:10.592+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:10 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:37:10.602+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:10 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:42), which has no missing parents
[2023-01-30T18:37:10.775+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:10 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.9 KiB, free 434.2 MiB)
[2023-01-30T18:37:10.878+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:10 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 434.1 MiB)
[2023-01-30T18:37:10.880+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 767c6fe0d003:33173 (size: 7.8 KiB, free: 434.4 MiB)
[2023-01-30T18:37:10.887+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:10 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:37:10.907+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:42) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:37:10.908+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:10 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-30T18:37:10.946+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:10 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-30T18:37:10.970+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:10 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-30T18:37:11.924+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:11 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-30T18:37:12.975+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:12 INFO CodeGenerator: Code generated in 718.307722 ms
[2023-01-30T18:37:13.905+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:13 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-30T18:37:13.916+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:13 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2987 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:37:13.927+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-30T18:37:13.931+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:13 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:42) finished in 3.253 s
[2023-01-30T18:37:13.931+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:13 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:37:13.932+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-30T18:37:13.933+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:13 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:42, took 3.354275 s
[2023-01-30T18:37:15.121+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:15 INFO FileSourceStrategy: Pushed Filters:
[2023-01-30T18:37:15.122+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:15 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-30T18:37:15.126+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:15 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-30T18:37:15.798+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:15 INFO CodeGenerator: Code generated in 468.882645 ms
[2023-01-30T18:37:15.861+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:15 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 433.9 MiB)
[2023-01-30T18:37:16.030+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-30T18:37:16.031+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 767c6fe0d003:33173 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-30T18:37:16.032+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:45
[2023-01-30T18:37:16.037+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-30T18:37:16.215+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:45
[2023-01-30T18:37:16.230+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:45) with 1 output partitions
[2023-01-30T18:37:16.230+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:45)
[2023-01-30T18:37:16.231+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:37:16.231+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:37:16.238+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:45), which has no missing parents
[2023-01-30T18:37:16.256+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.9 KiB, free 433.9 MiB)
[2023-01-30T18:37:16.279+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.9 MiB)
[2023-01-30T18:37:16.288+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 767c6fe0d003:33173 (size: 7.8 KiB, free: 434.3 MiB)
[2023-01-30T18:37:16.305+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:37:16.308+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:45) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:37:16.314+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-30T18:37:16.319+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-30T18:37:16.320+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-30T18:37:16.413+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-30T18:37:16.842+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-30T18:37:16.870+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 553 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:37:16.877+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-30T18:37:16.883+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:45) finished in 0.644 s
[2023-01-30T18:37:16.884+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:37:16.884+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-30T18:37:16.890+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:45, took 0.671409 s
[2023-01-30T18:37:17.871+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:17 INFO FileSourceStrategy: Pushed Filters:
[2023-01-30T18:37:17.872+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:17 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-30T18:37:17.873+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:17 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-30T18:37:18.450+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:18 INFO CodeGenerator: Code generated in 395.443856 ms
[2023-01-30T18:37:18.535+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:18 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-30T18:37:18.838+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:18 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-30T18:37:18.843+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:18 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 767c6fe0d003:33173 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-30T18:37:18.853+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:18 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-30T18:37:18.864+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-30T18:37:19.189+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:19 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-30T18:37:19.198+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:19 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-30T18:37:19.199+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:19 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-30T18:37:19.199+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:19 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:37:19.199+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:19 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:37:19.209+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:19 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-30T18:37:19.245+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:19 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-01-30T18:37:19.301+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:19 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-30T18:37:19.333+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:19 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 767c6fe0d003:33173 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-30T18:37:19.345+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:19 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:37:19.351+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:37:19.355+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:19 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-30T18:37:19.386+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:19 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-30T18:37:19.387+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:19 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-30T18:37:19.564+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:19 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-30T18:37:20.050+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:20 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-30T18:37:20.083+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:20 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 704 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:37:20.088+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:20 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-30T18:37:20.097+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:20 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.873 s
[2023-01-30T18:37:20.098+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:20 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:37:20.098+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-30T18:37:20.100+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:20 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.905203 s
[2023-01-30T18:37:20.608+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:20 INFO FileSourceStrategy: Pushed Filters:
[2023-01-30T18:37:20.609+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:20 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-30T18:37:20.611+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:20 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-30T18:37:20.895+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:20 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 767c6fe0d003:33173 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-30T18:37:20.991+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:20 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 767c6fe0d003:33173 in memory (size: 7.8 KiB, free: 434.3 MiB)
[2023-01-30T18:37:21.050+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO CodeGenerator: Code generated in 178.548111 ms
[2023-01-30T18:37:21.072+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 767c6fe0d003:33173 in memory (size: 7.8 KiB, free: 434.3 MiB)
[2023-01-30T18:37:21.089+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-30T18:37:21.179+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.5 MiB)
[2023-01-30T18:37:21.182+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 767c6fe0d003:33173 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-30T18:37:21.183+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:63
[2023-01-30T18:37:21.194+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-30T18:37:21.283+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:63
[2023-01-30T18:37:21.286+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:63) with 1 output partitions
[2023-01-30T18:37:21.287+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:63)
[2023-01-30T18:37:21.287+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:37:21.289+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:37:21.300+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:63), which has no missing parents
[2023-01-30T18:37:21.318+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.7 KiB, free 433.5 MiB)
[2023-01-30T18:37:21.334+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.5 MiB)
[2023-01-30T18:37:21.336+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 767c6fe0d003:33173 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-30T18:37:21.338+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:37:21.339+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:63) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:37:21.340+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-30T18:37:21.344+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-30T18:37:21.345+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-30T18:37:21.376+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-30T18:37:21.656+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1585 bytes result sent to driver
[2023-01-30T18:37:21.672+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 318 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:37:21.699+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-30T18:37:21.700+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:63) finished in 0.382 s
[2023-01-30T18:37:21.704+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:37:21.704+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-30T18:37:21.712+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:63, took 0.421572 s
[2023-01-30T18:37:23.301+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO FileSourceStrategy: Pushed Filters:
[2023-01-30T18:37:23.305+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-30T18:37:23.306+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-30T18:37:23.563+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO CodeGenerator: Code generated in 154.251414 ms
[2023-01-30T18:37:23.570+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-30T18:37:23.637+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-30T18:37:23.640+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 767c6fe0d003:33173 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-30T18:37:23.652+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-30T18:37:23.664+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-30T18:37:23.769+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-30T18:37:23.786+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-30T18:37:23.786+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-30T18:37:23.790+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:37:23.790+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:37:23.807+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-30T18:37:23.823+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-30T18:37:23.842+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-30T18:37:23.844+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 767c6fe0d003:33173 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-30T18:37:23.846+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:37:23.851+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:37:23.853+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-30T18:37:23.861+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-30T18:37:23.863+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-30T18:37:23.899+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-30T18:37:24.020+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2537 bytes result sent to driver
[2023-01-30T18:37:24.024+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 164 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:37:24.024+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-30T18:37:24.029+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.215 s
[2023-01-30T18:37:24.029+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:37:24.029+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-30T18:37:24.029+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.252813 s
[2023-01-30T18:37:24.191+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO FileSourceStrategy: Pushed Filters:
[2023-01-30T18:37:24.191+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-30T18:37:24.192+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-30T18:37:24.367+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO CodeGenerator: Code generated in 146.728097 ms
[2023-01-30T18:37:24.379+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-30T18:37:24.393+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-01-30T18:37:24.395+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 767c6fe0d003:33173 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-30T18:37:24.399+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-30T18:37:24.399+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-30T18:37:24.435+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-30T18:37:24.438+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-30T18:37:24.438+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-30T18:37:24.438+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:37:24.439+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:37:24.440+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-30T18:37:24.450+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-30T18:37:24.455+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-30T18:37:24.464+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 767c6fe0d003:33173 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-30T18:37:24.465+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:37:24.466+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:37:24.466+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-30T18:37:24.467+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-30T18:37:24.468+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-30T18:37:24.491+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-30T18:37:24.607+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-30T18:37:24.608+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 140 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:37:24.608+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-30T18:37:24.609+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.168 s
[2023-01-30T18:37:24.609+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:37:24.610+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-30T18:37:24.610+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.174074 s
[2023-01-30T18:37:24.787+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO FileSourceStrategy: Pushed Filters:
[2023-01-30T18:37:24.788+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-30T18:37:24.789+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-30T18:37:24.958+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO CodeGenerator: Code generated in 72.565426 ms
[2023-01-30T18:37:24.965+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-30T18:37:24.999+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-30T18:37:25.004+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 767c6fe0d003:33173 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-30T18:37:25.007+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-30T18:37:25.007+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-30T18:37:25.048+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-30T18:37:25.050+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-30T18:37:25.050+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-30T18:37:25.050+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:37:25.051+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:37:25.055+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-30T18:37:25.069+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.6 MiB)
[2023-01-30T18:37:25.086+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-01-30T18:37:25.088+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 767c6fe0d003:33173 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-30T18:37:25.091+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:37:25.092+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:37:25.092+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-30T18:37:25.104+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-30T18:37:25.107+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-30T18:37:25.134+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-30T18:37:25.336+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-30T18:37:25.341+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 238 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:37:25.342+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-30T18:37:25.346+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.286 s
[2023-01-30T18:37:25.347+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:37:25.347+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-30T18:37:25.349+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.298938 s
[2023-01-30T18:37:25.711+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 767c6fe0d003:33173 in memory (size: 13.4 KiB, free: 434.1 MiB)
[2023-01-30T18:37:25.784+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 767c6fe0d003:33173 in memory (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-30T18:37:25.868+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 767c6fe0d003:33173 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-30T18:37:25.925+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 767c6fe0d003:33173 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-30T18:37:27.441+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:27 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 767c6fe0d003:33173 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-30T18:37:27.525+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:27 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 767c6fe0d003:33173 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-30T18:37:27.612+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:27 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 767c6fe0d003:33173 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-30T18:37:27.705+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:27 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 767c6fe0d003:33173 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-30T18:37:27.877+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:27 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 767c6fe0d003:33173 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-30T18:37:27.978+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:27 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 767c6fe0d003:33173 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-30T18:37:36.978+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:36 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-30T18:37:37.043+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-30T18:37:37.044+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-30T18:37:37.054+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-30T18:37:37.055+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-30T18:37:37.061+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-30T18:37:37.103+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-30T18:37:38.073+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-30T18:37:38.073+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-30T18:37:38.074+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-30T18:37:38.082+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:37:38.082+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:37:38.083+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-30T18:37:38.177+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-30T18:37:38.186+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-30T18:37:38.189+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 767c6fe0d003:33173 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-30T18:37:38.191+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:37:38.201+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:37:38.204+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-30T18:37:38.212+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-30T18:37:38.233+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-30T18:37:38.569+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-30T18:37:38.572+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-30T18:37:38.576+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-30T18:37:38.580+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-30T18:37:38.581+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-30T18:37:38.591+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-30T18:37:38.623+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO CodecConfig: Compression: SNAPPY
[2023-01-30T18:37:38.635+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO CodecConfig: Compression: SNAPPY
[2023-01-30T18:37:38.856+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-30T18:37:38.862+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO ParquetOutputFormat: Validation is off
[2023-01-30T18:37:38.862+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-30T18:37:38.865+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-30T18:37:38.865+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-30T18:37:38.865+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-30T18:37:38.866+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-30T18:37:38.866+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-30T18:37:38.866+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-30T18:37:38.866+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-30T18:37:38.866+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-30T18:37:38.867+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-30T18:37:38.867+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-30T18:37:38.867+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-30T18:37:38.867+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-30T18:37:38.867+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-30T18:37:38.867+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-30T18:37:38.867+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-30T18:37:39.163+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:39 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-30T18:37:39.164+0000] {spark_submit.py:495} INFO - {
[2023-01-30T18:37:39.164+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-30T18:37:39.164+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-30T18:37:39.164+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-30T18:37:39.164+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-30T18:37:39.165+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-30T18:37:39.165+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.165+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.165+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-30T18:37:39.165+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-30T18:37:39.166+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-30T18:37:39.166+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.166+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.166+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-30T18:37:39.166+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.166+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.167+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.167+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.167+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-30T18:37:39.167+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.167+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.167+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.167+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.168+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-30T18:37:39.168+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.168+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.168+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.168+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.168+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-30T18:37:39.168+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.169+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.169+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.169+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.169+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-30T18:37:39.169+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.169+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.179+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.179+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.179+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-30T18:37:39.180+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.180+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.184+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.185+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.197+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-30T18:37:39.198+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.198+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.215+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.215+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.216+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-30T18:37:39.216+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.216+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.227+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.227+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.227+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-30T18:37:39.227+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.227+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.228+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.228+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.228+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-30T18:37:39.263+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.264+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.264+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.264+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.276+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-30T18:37:39.276+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.276+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.277+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.277+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.277+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-30T18:37:39.277+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.277+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.277+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.278+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.278+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-30T18:37:39.278+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.278+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.278+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.278+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.278+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-30T18:37:39.279+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.279+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.279+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.279+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.279+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-30T18:37:39.279+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.280+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.280+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.280+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.280+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-30T18:37:39.280+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.280+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.280+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.281+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.281+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-30T18:37:39.359+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.359+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.359+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.360+0000] {spark_submit.py:495} INFO - } ]
[2023-01-30T18:37:39.360+0000] {spark_submit.py:495} INFO - }
[2023-01-30T18:37:39.360+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-30T18:37:39.360+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-30T18:37:39.360+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-30T18:37:39.360+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-30T18:37:39.375+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-30T18:37:39.376+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-30T18:37:39.376+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-30T18:37:39.376+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-30T18:37:39.376+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-30T18:37:39.376+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-30T18:37:39.376+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-30T18:37:39.376+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-30T18:37:39.377+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-30T18:37:39.377+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-30T18:37:39.377+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-30T18:37:39.393+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-30T18:37:39.393+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-30T18:37:39.393+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-30T18:37:39.393+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-30T18:37:39.393+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-30T18:37:39.405+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-30T18:37:39.406+0000] {spark_submit.py:495} INFO - }
[2023-01-30T18:37:39.406+0000] {spark_submit.py:495} INFO - 
[2023-01-30T18:37:39.406+0000] {spark_submit.py:495} INFO - 
[2023-01-30T18:37:40.222+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:40 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-30T18:37:43.819+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:43 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675103752483-d5ddf314-6920-4f00-bcc8-11fb58b0895f/_temporary/0/_temporary/' directory.
[2023-01-30T18:37:43.819+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:43 INFO FileOutputCommitter: Saved output of task 'attempt_20230130183737808797956220510671_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675103752483-d5ddf314-6920-4f00-bcc8-11fb58b0895f/_temporary/0/task_20230130183737808797956220510671_0008_m_000000
[2023-01-30T18:37:43.820+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:43 INFO SparkHadoopMapRedUtil: attempt_20230130183737808797956220510671_0008_m_000000_8: Committed. Elapsed time: 896 ms.
[2023-01-30T18:37:43.834+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:43 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-30T18:37:43.843+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:43 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 5634 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:37:43.849+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:43 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-30T18:37:43.849+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:43 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 5.772 s
[2023-01-30T18:37:43.849+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:43 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:37:43.850+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-30T18:37:43.850+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:43 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 5.783917 s
[2023-01-30T18:37:43.857+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:43 INFO FileFormatWriter: Start to commit write Job e8c44ad4-96af-401c-964a-110142a1244c.
[2023-01-30T18:37:44.692+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:44 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675103752483-d5ddf314-6920-4f00-bcc8-11fb58b0895f/_temporary/0/task_20230130183737808797956220510671_0008_m_000000/' directory.
[2023-01-30T18:37:45.019+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:45 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675103752483-d5ddf314-6920-4f00-bcc8-11fb58b0895f/' directory.
[2023-01-30T18:37:46.499+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:46 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 767c6fe0d003:33173 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-30T18:37:46.577+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:46 INFO FileFormatWriter: Write Job e8c44ad4-96af-401c-964a-110142a1244c committed. Elapsed time: 2708 ms.
[2023-01-30T18:37:46.703+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:46 INFO FileFormatWriter: Finished processing stats for write job e8c44ad4-96af-401c-964a-110142a1244c.
[2023-01-30T18:37:48.489+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:48 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675103752483-d5ddf314-6920-4f00-bcc8-11fb58b0895f/part-00000-0db77376-e659-4e8d-89c5-487cbf4a7633-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=42e93e6b-9be5-4d03-b577-7344e41bc31e, location=US}
[2023-01-30T18:37:53.857+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:53 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=42e93e6b-9be5-4d03-b577-7344e41bc31e, location=US}
[2023-01-30T18:37:54.475+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-30T18:37:54.768+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:54 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-30T18:37:54.800+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:54 INFO SparkUI: Stopped Spark web UI at http://767c6fe0d003:4043
[2023-01-30T18:37:54.850+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:54 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-30T18:37:54.900+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:54 INFO MemoryStore: MemoryStore cleared
[2023-01-30T18:37:54.901+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:54 INFO BlockManager: BlockManager stopped
[2023-01-30T18:37:54.914+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:54 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-30T18:37:54.931+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:54 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-30T18:37:54.973+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:54 INFO SparkContext: Successfully stopped SparkContext
[2023-01-30T18:37:54.974+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:54 INFO ShutdownHookManager: Shutdown hook called
[2023-01-30T18:37:54.978+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-1f056760-0c3a-45e3-93d4-77e220bd7f35
[2023-01-30T18:37:54.991+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-460b3df8-92d3-4d8c-9cd6-887fc0f9b2db
[2023-01-30T18:37:55.003+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-460b3df8-92d3-4d8c-9cd6-887fc0f9b2db/pyspark-d4b22487-a7d0-4e86-bc81-490014659d64
[2023-01-30T18:37:55.258+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata, task_id=stage_total_generation, execution_date=20210101T040000, start_date=20230130T183524, end_date=20230130T183755
[2023-01-30T18:37:55.323+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-30T18:37:55.377+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-30T18:44:43.290+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-30T18:44:43.496+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-30T18:44:43.511+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-30T18:44:43.512+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-30T18:44:43.513+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-30T18:44:43.655+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 04:00:00+00:00
[2023-01-30T18:44:43.687+0000] {standard_task_runner.py:55} INFO - Started process 2948 to run task
[2023-01-30T18:44:43.718+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T04:00:00+00:00', '--job-id', '672', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp2r1w_02c']
[2023-01-30T18:44:43.730+0000] {standard_task_runner.py:83} INFO - Job 672: Subtask stage_total_generation
[2023-01-30T18:44:44.525+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [running]> on host 767c6fe0d003
[2023-01-30T18:44:46.325+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T04:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T04:00:00+00:00
[2023-01-30T18:44:46.470+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-30T18:44:46.485+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET
[2023-01-30T18:45:26.918+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:26 INFO SparkContext: Running Spark version 3.3.1
[2023-01-30T18:45:28.403+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-30T18:45:29.403+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:29 INFO ResourceUtils: ==============================================================
[2023-01-30T18:45:29.417+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:29 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-30T18:45:29.427+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:29 INFO ResourceUtils: ==============================================================
[2023-01-30T18:45:29.436+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:29 INFO SparkContext: Submitted application: gcp_playground
[2023-01-30T18:45:29.670+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:29 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-30T18:45:29.694+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:29 INFO ResourceProfile: Limiting resource is cpu
[2023-01-30T18:45:29.698+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:29 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-30T18:45:30.128+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:30 INFO SecurityManager: Changing view acls to: ***
[2023-01-30T18:45:30.129+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:30 INFO SecurityManager: Changing modify acls to: ***
[2023-01-30T18:45:30.129+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:30 INFO SecurityManager: Changing view acls groups to:
[2023-01-30T18:45:30.130+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:30 INFO SecurityManager: Changing modify acls groups to:
[2023-01-30T18:45:30.135+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-30T18:45:32.686+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:32 INFO Utils: Successfully started service 'sparkDriver' on port 35283.
[2023-01-30T18:45:34.127+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:34 INFO SparkEnv: Registering MapOutputTracker
[2023-01-30T18:45:35.127+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:35 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-30T18:45:35.595+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:35 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-30T18:45:35.604+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:35 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-30T18:45:35.655+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:35 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-30T18:45:36.161+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:36 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-48d80097-1808-4bb0-839e-4d8d46ec4045
[2023-01-30T18:45:36.335+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:36 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-30T18:45:36.607+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:36 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-30T18:45:42.435+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-30T18:45:42.439+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:42 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-30T18:45:42.471+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:42 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-30T18:45:42.479+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:42 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-30T18:45:42.483+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:42 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-30T18:45:42.563+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:42 INFO Utils: Successfully started service 'SparkUI' on port 4045.
[2023-01-30T18:45:43.767+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:43 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://767c6fe0d003:35283/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675104326858
[2023-01-30T18:45:43.797+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:43 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://767c6fe0d003:35283/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675104326858
[2023-01-30T18:45:46.438+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:46 INFO Executor: Starting executor ID driver on host 767c6fe0d003
[2023-01-30T18:45:46.635+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:46 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-30T18:45:46.831+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:46 INFO Executor: Fetching spark://767c6fe0d003:35283/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675104326858
[2023-01-30T18:45:48.110+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:48 INFO TransportClientFactory: Successfully created connection to 767c6fe0d003/172.19.0.9:35283 after 1096 ms (0 ms spent in bootstraps)
[2023-01-30T18:45:48.317+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:48 INFO Utils: Fetching spark://767c6fe0d003:35283/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-b19cf80c-e1dd-4993-aa2d-40133f7ea118/userFiles-7b0838dd-b8dd-4868-a35c-73f1e9cfde6f/fetchFileTemp13923225739770979455.tmp
[2023-01-30T18:45:53.127+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:53 INFO Executor: Adding file:/tmp/spark-b19cf80c-e1dd-4993-aa2d-40133f7ea118/userFiles-7b0838dd-b8dd-4868-a35c-73f1e9cfde6f/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-30T18:45:53.128+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:53 INFO Executor: Fetching spark://767c6fe0d003:35283/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675104326858
[2023-01-30T18:45:53.153+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:53 INFO Utils: Fetching spark://767c6fe0d003:35283/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-b19cf80c-e1dd-4993-aa2d-40133f7ea118/userFiles-7b0838dd-b8dd-4868-a35c-73f1e9cfde6f/fetchFileTemp10333838383266567182.tmp
[2023-01-30T18:45:55.111+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:55 INFO Executor: Adding file:/tmp/spark-b19cf80c-e1dd-4993-aa2d-40133f7ea118/userFiles-7b0838dd-b8dd-4868-a35c-73f1e9cfde6f/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-30T18:45:55.417+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45173.
[2023-01-30T18:45:55.418+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:55 INFO NettyBlockTransferService: Server created on 767c6fe0d003:45173
[2023-01-30T18:45:55.487+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-30T18:45:55.821+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 767c6fe0d003, 45173, None)
[2023-01-30T18:45:55.968+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:55 INFO BlockManagerMasterEndpoint: Registering block manager 767c6fe0d003:45173 with 434.4 MiB RAM, BlockManagerId(driver, 767c6fe0d003, 45173, None)
[2023-01-30T18:45:56.038+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 767c6fe0d003, 45173, None)
[2023-01-30T18:45:56.069+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 767c6fe0d003, 45173, None)
[2023-01-30T18:46:01.559+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:01 INFO AsyncEventQueue: Process of event SparkListenerResourceProfileAdded(Profile: id = 0, executor resources: cores -> name: cores, amount: 1, script: , vendor: ,memory -> name: memory, amount: 1024, script: , vendor: ,offHeap -> name: offHeap, amount: 0, script: , vendor: , task resources: cpus -> name: cpus, amount: 1.0) by listener AppStatusListener took 1.084380589s.
[2023-01-30T18:46:15.619+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:15 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-30T18:46:15.803+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:15 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-30T18:46:50.799+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:50 INFO InMemoryFileIndex: It took 685 ms to list leaf files for 1 paths.
[2023-01-30T18:46:52.411+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:52 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-30T18:46:53.398+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-30T18:46:53.422+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 767c6fe0d003:45173 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-30T18:46:53.476+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:53 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-30T18:47:01.271+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:01 INFO FileInputFormat: Total input files to process : 1
[2023-01-30T18:47:01.378+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:01 INFO FileInputFormat: Total input files to process : 1
[2023-01-30T18:47:01.487+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:01 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-30T18:47:01.639+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:01 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-30T18:47:01.646+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:01 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-30T18:47:01.646+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:01 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:47:01.666+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:01 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:47:01.731+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:01 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-30T18:47:02.119+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:02 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-30T18:47:02.133+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:02 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-30T18:47:02.135+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:02 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 767c6fe0d003:45173 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-30T18:47:02.137+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:02 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:47:02.221+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:47:02.222+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:02 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-30T18:47:02.629+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:02 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-30T18:47:02.850+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:02 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-30T18:47:04.032+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:04 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010400__202101010500.json:0+9364
[2023-01-30T18:47:05.835+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:05 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-30T18:47:05.905+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3513 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:47:05.931+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:05 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-30T18:47:05.968+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:05 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 4.091 s
[2023-01-30T18:47:06.003+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:06 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:47:06.004+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-30T18:47:06.015+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:06 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 4.524569 s
[2023-01-30T18:47:08.895+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:08 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 767c6fe0d003:45173 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-30T18:47:12.746+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:12 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 767c6fe0d003:45173 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-30T18:47:53.667+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:53 INFO FileSourceStrategy: Pushed Filters:
[2023-01-30T18:47:53.683+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:53 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-30T18:47:53.695+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:53 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-30T18:48:00.493+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:00 INFO CodeGenerator: Code generated in 4303.455146 ms
[2023-01-30T18:48:00.815+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:00 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-30T18:48:01.726+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:01 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-30T18:48:01.805+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:01 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 767c6fe0d003:45173 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-30T18:48:01.857+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:01 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-30T18:48:02.458+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-30T18:48:03.701+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:03 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-30T18:48:03.707+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:03 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-30T18:48:03.708+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:03 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-30T18:48:03.709+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:03 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:48:03.711+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:03 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:48:03.727+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:03 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-30T18:48:03.795+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:03 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-30T18:48:03.815+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:03 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-30T18:48:03.846+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:03 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 767c6fe0d003:45173 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-30T18:48:03.895+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:03 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:48:03.963+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:48:03.971+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:03 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-30T18:48:04.319+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:04 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-30T18:48:04.326+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:04 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-30T18:48:06.885+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:06 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-30T18:48:11.940+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:11 INFO CodeGenerator: Code generated in 3695.59245 ms
[2023-01-30T18:48:14.923+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:14 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1671 bytes result sent to driver
[2023-01-30T18:48:15.168+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:15 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 11129 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:48:15.220+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:15 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 11.418 s
[2023-01-30T18:48:15.308+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:15 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:48:15.339+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:15 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-30T18:48:15.340+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-30T18:48:15.341+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:15 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 11.633066 s
[2023-01-30T18:48:17.832+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:17 INFO FileSourceStrategy: Pushed Filters:
[2023-01-30T18:48:17.847+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:17 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-30T18:48:17.850+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:17 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-30T18:48:18.950+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:18 INFO CodeGenerator: Code generated in 570.074534 ms
[2023-01-30T18:48:18.983+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:18 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-30T18:48:19.039+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:19 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-30T18:48:19.045+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:19 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 767c6fe0d003:45173 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-30T18:48:19.058+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:19 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-30T18:48:19.072+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-30T18:48:19.170+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:19 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-30T18:48:19.179+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:19 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-30T18:48:19.180+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:19 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-30T18:48:19.181+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:19 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:48:19.181+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:19 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:48:19.204+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:19 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-30T18:48:19.233+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:19 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-30T18:48:19.241+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:19 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-30T18:48:19.247+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:19 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 767c6fe0d003:45173 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-30T18:48:19.251+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:19 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:48:19.256+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:48:19.259+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:19 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-30T18:48:19.265+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:19 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-30T18:48:19.283+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:19 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-30T18:48:19.375+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:19 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-30T18:48:21.178+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:21 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-30T18:48:21.221+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:21 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1958 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:48:21.244+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:21 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-30T18:48:21.271+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:21 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 2.049 s
[2023-01-30T18:48:21.371+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:21 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:48:21.372+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-30T18:48:21.397+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:21 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 2.210071 s
[2023-01-30T18:48:24.603+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:24 INFO FileSourceStrategy: Pushed Filters:
[2023-01-30T18:48:24.619+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:24 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-30T18:48:24.641+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:24 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-30T18:48:26.167+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:26 INFO CodeGenerator: Code generated in 567.46698 ms
[2023-01-30T18:48:26.204+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:26 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-30T18:48:26.563+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:26 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-30T18:48:26.655+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:26 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 767c6fe0d003:45173 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-30T18:48:26.656+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:26 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-30T18:48:26.780+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-30T18:48:27.539+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:27 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-30T18:48:27.560+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:27 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-30T18:48:27.561+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:27 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-30T18:48:27.561+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:27 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:48:27.562+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:27 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:48:27.642+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:27 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-30T18:48:27.930+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:27 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-01-30T18:48:27.977+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:27 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-30T18:48:27.980+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:27 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 767c6fe0d003:45173 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-30T18:48:27.995+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:27 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:48:28.018+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:48:28.023+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:28 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-30T18:48:28.043+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:28 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-30T18:48:28.057+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:28 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-30T18:48:28.274+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:28 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-30T18:48:29.267+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:29 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 767c6fe0d003:45173 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-30T18:48:29.469+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:29 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 767c6fe0d003:45173 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-30T18:48:29.992+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:29 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1798 bytes result sent to driver
[2023-01-30T18:48:30.015+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:30 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1974 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:48:30.022+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:30 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 2.307 s
[2023-01-30T18:48:30.025+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:30 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-30T18:48:30.027+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:30 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:48:30.028+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-30T18:48:30.038+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:30 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 2.494837 s
[2023-01-30T18:48:31.335+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:31 INFO FileSourceStrategy: Pushed Filters:
[2023-01-30T18:48:31.344+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:31 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-30T18:48:31.345+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:31 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-30T18:48:32.858+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:32 INFO CodeGenerator: Code generated in 1251.525228 ms
[2023-01-30T18:48:33.519+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:33 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-30T18:48:34.369+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:34 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-30T18:48:34.377+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:34 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 767c6fe0d003:45173 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-30T18:48:34.506+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:34 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-30T18:48:34.844+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-30T18:48:35.667+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:35 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-30T18:48:35.742+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:35 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-30T18:48:35.743+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:35 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-30T18:48:35.744+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:35 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:48:35.750+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:35 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:48:35.871+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:35 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-30T18:48:36.070+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:36 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-30T18:48:36.169+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:36 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-30T18:48:36.255+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:36 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 767c6fe0d003:45173 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-30T18:48:36.273+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:36 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:48:36.353+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:48:36.419+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:36 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-30T18:48:36.520+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:36 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-30T18:48:36.537+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:36 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-30T18:48:37.725+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:37 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-30T18:48:39.535+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1585 bytes result sent to driver
[2023-01-30T18:48:39.583+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 3074 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:48:39.584+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-30T18:48:39.605+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 3.747 s
[2023-01-30T18:48:39.607+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:48:39.607+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-30T18:48:39.608+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 3.940160 s
[2023-01-30T18:48:46.778+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:46 INFO FileSourceStrategy: Pushed Filters:
[2023-01-30T18:48:46.787+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:46 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-30T18:48:46.791+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:46 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-30T18:48:48.271+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:48 INFO CodeGenerator: Code generated in 951.956256 ms
[2023-01-30T18:48:48.367+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:48 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.2 MiB)
[2023-01-30T18:48:48.549+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:48 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-30T18:48:48.551+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:48 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 767c6fe0d003:45173 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-30T18:48:48.579+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:48 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-30T18:48:48.609+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-30T18:48:48.991+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:48 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-30T18:48:48.991+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:48 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-30T18:48:48.992+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:48 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-30T18:48:48.992+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:48 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:48:48.992+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:48 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:48:48.998+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:48 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-30T18:48:49.093+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.1 MiB)
[2023-01-30T18:48:49.137+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.1 MiB)
[2023-01-30T18:48:49.146+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 767c6fe0d003:45173 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-30T18:48:49.154+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:48:49.154+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:48:49.166+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-30T18:48:49.176+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-30T18:48:49.177+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-30T18:48:49.298+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-30T18:48:49.895+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2537 bytes result sent to driver
[2023-01-30T18:48:49.900+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 730 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:48:49.901+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-30T18:48:49.913+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.894 s
[2023-01-30T18:48:49.918+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:48:49.919+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-30T18:48:49.919+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.926722 s
[2023-01-30T18:48:50.524+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:50 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 767c6fe0d003:45173 in memory (size: 12.7 KiB, free: 434.2 MiB)
[2023-01-30T18:48:50.649+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:50 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 767c6fe0d003:45173 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-30T18:48:50.819+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:50 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 767c6fe0d003:45173 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-30T18:48:51.132+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:51 INFO FileSourceStrategy: Pushed Filters:
[2023-01-30T18:48:51.134+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:51 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-30T18:48:51.175+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:51 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-30T18:48:55.795+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:55 INFO CodeGenerator: Code generated in 3513.513133 ms
[2023-01-30T18:48:55.821+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:55 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.1 MiB)
[2023-01-30T18:48:55.959+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:55 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-30T18:48:55.969+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:55 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 767c6fe0d003:45173 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-30T18:48:56.013+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:56 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-30T18:48:56.090+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-30T18:48:57.098+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:57 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-30T18:48:57.115+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:57 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-30T18:48:57.115+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:57 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-30T18:48:57.115+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:57 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:48:57.116+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:57 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:48:57.137+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:57 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-30T18:48:57.198+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:57 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 433.0 MiB)
[2023-01-30T18:48:57.270+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:57 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 433.0 MiB)
[2023-01-30T18:48:57.279+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:57 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 767c6fe0d003:45173 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-30T18:48:57.285+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:57 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:48:57.297+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:48:57.298+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:57 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-30T18:48:57.300+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:57 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-30T18:48:57.302+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:57 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-30T18:48:57.471+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:57 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-30T18:48:58.305+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:58 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-30T18:48:58.313+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:58 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 1008 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:48:58.313+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:58 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-30T18:48:58.314+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:58 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 1.173 s
[2023-01-30T18:48:58.321+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:58 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:48:58.322+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-30T18:48:58.353+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:58 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 1.242707 s
[2023-01-30T18:49:01.971+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:01 INFO FileSourceStrategy: Pushed Filters:
[2023-01-30T18:49:01.975+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:01 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-30T18:49:01.977+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:01 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-30T18:49:06.196+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:06 INFO CodeGenerator: Code generated in 2479.996191 ms
[2023-01-30T18:49:06.511+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:06 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.8 MiB)
[2023-01-30T18:49:07.551+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:07 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-30T18:49:07.557+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:07 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 767c6fe0d003:45173 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-30T18:49:07.590+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:07 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-30T18:49:07.638+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-30T18:49:09.061+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:09 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-30T18:49:09.091+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:09 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-30T18:49:09.092+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:09 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-30T18:49:09.094+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:09 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:49:09.095+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:09 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:49:09.105+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:09 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-30T18:49:09.325+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:09 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-30T18:49:09.361+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:09 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.7 MiB)
[2023-01-30T18:49:09.366+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:09 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 767c6fe0d003:45173 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-30T18:49:09.383+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:09 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:49:09.559+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:49:09.595+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:09 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-30T18:49:09.596+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:09 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-30T18:49:09.597+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:09 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-30T18:49:09.907+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:09 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-30T18:49:10.984+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:10 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1772 bytes result sent to driver
[2023-01-30T18:49:10.990+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:10 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 1421 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:49:10.991+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:10 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-30T18:49:10.997+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:10 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 1.727 s
[2023-01-30T18:49:10.999+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:10 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:49:11.000+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-30T18:49:11.020+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:11 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 1.958639 s
[2023-01-30T18:49:16.031+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:16 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 767c6fe0d003:45173 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-30T18:49:16.188+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:16 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 767c6fe0d003:45173 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-30T18:49:16.389+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:16 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 767c6fe0d003:45173 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-30T18:49:16.464+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:16 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 767c6fe0d003:45173 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-30T18:49:16.554+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:16 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 767c6fe0d003:45173 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-30T18:49:16.659+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:16 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 767c6fe0d003:45173 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-30T18:49:16.766+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:16 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 767c6fe0d003:45173 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-30T18:49:16.996+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:16 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 767c6fe0d003:45173 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-30T18:50:15.523+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:15 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-30T18:50:15.611+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-30T18:50:15.612+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-30T18:50:15.619+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-30T18:50:15.619+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-30T18:50:15.620+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-30T18:50:15.622+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-30T18:50:18.822+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:18 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-30T18:50:18.840+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:18 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-30T18:50:18.841+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:18 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-30T18:50:18.844+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:18 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:50:18.845+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:18 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:50:18.875+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:18 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-30T18:50:20.601+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:20 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-30T18:50:20.677+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:20 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-30T18:50:20.680+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:20 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 767c6fe0d003:45173 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-30T18:50:20.695+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:20 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:50:20.696+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:50:20.697+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:20 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-30T18:50:20.762+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:20 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-30T18:50:20.826+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:20 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-30T18:50:22.725+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-30T18:50:22.726+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-30T18:50:22.760+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-30T18:50:22.763+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-30T18:50:22.772+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-30T18:50:22.789+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-30T18:50:23.035+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:23 INFO CodecConfig: Compression: SNAPPY
[2023-01-30T18:50:23.118+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:23 INFO CodecConfig: Compression: SNAPPY
[2023-01-30T18:50:23.573+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:23 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-30T18:50:23.579+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:23 INFO ParquetOutputFormat: Validation is off
[2023-01-30T18:50:23.582+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:23 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-30T18:50:23.586+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:23 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-30T18:50:23.587+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-30T18:50:23.587+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-30T18:50:23.588+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-30T18:50:23.588+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-30T18:50:23.588+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-30T18:50:23.589+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-30T18:50:23.589+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-30T18:50:23.589+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-30T18:50:23.589+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-30T18:50:23.590+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-30T18:50:23.590+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-30T18:50:23.590+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-30T18:50:23.596+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-30T18:50:23.596+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-30T18:50:24.295+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:24 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-30T18:50:24.296+0000] {spark_submit.py:495} INFO - {
[2023-01-30T18:50:24.296+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-30T18:50:24.296+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-30T18:50:24.296+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-30T18:50:24.296+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-30T18:50:24.297+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-30T18:50:24.297+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:24.297+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:24.297+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-30T18:50:24.297+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-30T18:50:24.298+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-30T18:50:24.298+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:24.298+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:24.298+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-30T18:50:24.299+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:24.306+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:24.307+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:24.315+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:24.316+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-30T18:50:24.316+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:24.316+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:24.317+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:24.317+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:24.317+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-30T18:50:24.317+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:24.318+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:24.318+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:24.318+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:24.318+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-30T18:50:24.318+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:24.319+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:24.370+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:24.371+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:24.371+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-30T18:50:24.371+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:24.371+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:24.371+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:24.371+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:24.372+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-30T18:50:24.372+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:24.372+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:24.372+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:24.372+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:24.372+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-30T18:50:24.373+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:24.373+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:24.373+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:24.373+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:24.410+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-30T18:50:24.411+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:24.411+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:24.416+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:24.417+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:24.421+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-30T18:50:24.427+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:24.443+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:24.457+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:24.477+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:24.478+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-30T18:50:24.478+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:24.479+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:24.479+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:24.479+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:24.480+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-30T18:50:24.480+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:24.493+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:24.494+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:24.494+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:24.494+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-30T18:50:24.494+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:24.494+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:24.494+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:24.495+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:24.495+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-30T18:50:24.495+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:24.495+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:24.495+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:24.495+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:24.495+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-30T18:50:24.496+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:24.496+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:24.496+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:24.496+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:24.496+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-30T18:50:24.497+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:24.497+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:24.497+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:24.497+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:24.497+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-30T18:50:24.497+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:24.498+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:24.498+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:24.499+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:24.500+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-30T18:50:24.521+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:24.524+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:24.528+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:24.535+0000] {spark_submit.py:495} INFO - } ]
[2023-01-30T18:50:24.536+0000] {spark_submit.py:495} INFO - }
[2023-01-30T18:50:24.537+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-30T18:50:24.537+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-30T18:50:24.538+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-30T18:50:24.538+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-30T18:50:24.538+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-30T18:50:24.539+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-30T18:50:24.539+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-30T18:50:24.540+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-30T18:50:24.541+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-30T18:50:24.543+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-30T18:50:24.547+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-30T18:50:24.574+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-30T18:50:24.576+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-30T18:50:24.576+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-30T18:50:24.577+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-30T18:50:24.640+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-30T18:50:24.640+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-30T18:50:24.640+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-30T18:50:24.640+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-30T18:50:24.641+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-30T18:50:24.641+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-30T18:50:24.641+0000] {spark_submit.py:495} INFO - }
[2023-01-30T18:50:24.641+0000] {spark_submit.py:495} INFO - 
[2023-01-30T18:50:24.641+0000] {spark_submit.py:495} INFO - 
[2023-01-30T18:50:26.293+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:26 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-30T18:50:31.438+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:31 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675104345004-20231754-92a7-470a-beda-55bf827e84b9/_temporary/0/_temporary/' directory.
[2023-01-30T18:50:31.439+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:31 INFO FileOutputCommitter: Saved output of task 'attempt_20230130185016944938999459205867_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675104345004-20231754-92a7-470a-beda-55bf827e84b9/_temporary/0/task_20230130185016944938999459205867_0008_m_000000
[2023-01-30T18:50:31.442+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:31 INFO SparkHadoopMapRedUtil: attempt_20230130185016944938999459205867_0008_m_000000_8: Committed. Elapsed time: 1684 ms.
[2023-01-30T18:50:31.533+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:31 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-30T18:50:31.536+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:31 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 10827 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:50:31.537+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:31 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-30T18:50:31.541+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:31 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 12.598 s
[2023-01-30T18:50:31.542+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:31 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:50:31.543+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-30T18:50:31.544+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:31 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 12.721455 s
[2023-01-30T18:50:31.554+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:31 INFO FileFormatWriter: Start to commit write Job b7ba4bd4-4e35-4297-924a-c9e64c8336e3.
[2023-01-30T18:50:32.524+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:32 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675104345004-20231754-92a7-470a-beda-55bf827e84b9/_temporary/0/task_20230130185016944938999459205867_0008_m_000000/' directory.
[2023-01-30T18:50:32.939+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:32 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675104345004-20231754-92a7-470a-beda-55bf827e84b9/' directory.
[2023-01-30T18:50:33.467+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:33 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 767c6fe0d003:45173 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-30T18:50:33.769+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:33 INFO FileFormatWriter: Write Job b7ba4bd4-4e35-4297-924a-c9e64c8336e3 committed. Elapsed time: 2215 ms.
[2023-01-30T18:50:33.781+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:33 INFO FileFormatWriter: Finished processing stats for write job b7ba4bd4-4e35-4297-924a-c9e64c8336e3.
[2023-01-30T18:50:35.845+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:35 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675104345004-20231754-92a7-470a-beda-55bf827e84b9/part-00000-20292f4f-235e-4b29-818f-8f82e2eab157-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=4d94247e-b11f-4e25-b7a2-b3e8717f0d73, location=US}
[2023-01-30T18:50:41.514+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:41 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=4d94247e-b11f-4e25-b7a2-b3e8717f0d73, location=US}
[2023-01-30T18:50:42.065+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-30T18:50:42.350+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:42 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-30T18:50:42.384+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:42 INFO SparkUI: Stopped Spark web UI at http://767c6fe0d003:4045
[2023-01-30T18:50:42.425+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:42 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-30T18:50:42.463+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:42 INFO MemoryStore: MemoryStore cleared
[2023-01-30T18:50:42.464+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:42 INFO BlockManager: BlockManager stopped
[2023-01-30T18:50:42.472+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:42 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-30T18:50:42.481+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:42 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-30T18:50:42.504+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:42 INFO SparkContext: Successfully stopped SparkContext
[2023-01-30T18:50:42.505+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:42 INFO ShutdownHookManager: Shutdown hook called
[2023-01-30T18:50:42.507+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-b19cf80c-e1dd-4993-aa2d-40133f7ea118/pyspark-ef8dc3a6-66f7-492c-8ecf-845623c5d308
[2023-01-30T18:50:42.520+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-b19cf80c-e1dd-4993-aa2d-40133f7ea118
[2023-01-30T18:50:42.537+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-01b6fdf2-0557-422a-a445-bff4224c949a
[2023-01-30T18:50:42.895+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata, task_id=stage_total_generation, execution_date=20210101T040000, start_date=20230130T184443, end_date=20230130T185042
[2023-01-30T18:50:42.959+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-30T18:50:43.008+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T01:10:46.723+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-31T01:10:46.751+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-31T01:10:46.751+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T01:10:46.752+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T01:10:46.752+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T01:10:46.793+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 04:00:00+00:00
[2023-01-31T01:10:46.828+0000] {standard_task_runner.py:55} INFO - Started process 1767 to run task
[2023-01-31T01:10:46.845+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T04:00:00+00:00', '--job-id', '682', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpccti2zig']
[2023-01-31T01:10:46.855+0000] {standard_task_runner.py:83} INFO - Job 682: Subtask stage_total_generation
[2023-01-31T01:10:47.075+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [running]> on host 767c6fe0d003
[2023-01-31T01:10:47.485+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T04:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T04:00:00+00:00
[2023-01-31T01:10:47.552+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T01:10:47.563+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET
[2023-01-31T01:11:34.070+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:34 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T01:11:34.793+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T01:11:36.222+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:36 INFO ResourceUtils: ==============================================================
[2023-01-31T01:11:36.223+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:36 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T01:11:36.223+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:36 INFO ResourceUtils: ==============================================================
[2023-01-31T01:11:36.242+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:36 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T01:11:36.554+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:36 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T01:11:36.581+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:36 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T01:11:36.604+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:36 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T01:11:36.868+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:36 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T01:11:36.868+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:36 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T01:11:36.869+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:36 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T01:11:36.874+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:36 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T01:11:36.884+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T01:11:38.037+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:38 INFO Utils: Successfully started service 'sparkDriver' on port 41039.
[2023-01-31T01:11:38.325+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:38 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T01:11:38.830+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:38 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T01:11:39.319+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:39 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T01:11:39.359+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:39 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T01:11:39.397+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:39 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T01:11:39.578+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:39 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-af39a4e5-3f61-46d1-a47e-24b00b477f1f
[2023-01-31T01:11:39.740+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:39 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T01:11:39.970+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:39 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T01:11:42.577+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T01:11:42.604+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:42 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-31T01:11:42.764+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:42 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://767c6fe0d003:41039/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675127494016
[2023-01-31T01:11:42.764+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:42 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://767c6fe0d003:41039/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675127494016
[2023-01-31T01:11:43.574+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:43 INFO Executor: Starting executor ID driver on host 767c6fe0d003
[2023-01-31T01:11:43.844+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:43 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T01:11:44.101+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:44 INFO Executor: Fetching spark://767c6fe0d003:41039/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675127494016
[2023-01-31T01:11:44.575+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:44 INFO TransportClientFactory: Successfully created connection to 767c6fe0d003/172.19.0.6:41039 after 241 ms (0 ms spent in bootstraps)
[2023-01-31T01:11:44.637+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:44 INFO Utils: Fetching spark://767c6fe0d003:41039/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-94e9da9d-3eae-428e-aa93-7e52cf733f6f/userFiles-4ab5d2d6-0197-4aa8-9a48-684a80aa93fe/fetchFileTemp2350289657269459804.tmp
[2023-01-31T01:11:46.387+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:46 INFO Executor: Adding file:/tmp/spark-94e9da9d-3eae-428e-aa93-7e52cf733f6f/userFiles-4ab5d2d6-0197-4aa8-9a48-684a80aa93fe/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T01:11:46.388+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:46 INFO Executor: Fetching spark://767c6fe0d003:41039/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675127494016
[2023-01-31T01:11:46.388+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:46 INFO Utils: Fetching spark://767c6fe0d003:41039/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-94e9da9d-3eae-428e-aa93-7e52cf733f6f/userFiles-4ab5d2d6-0197-4aa8-9a48-684a80aa93fe/fetchFileTemp17647187418318203124.tmp
[2023-01-31T01:11:47.054+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:47 INFO Executor: Adding file:/tmp/spark-94e9da9d-3eae-428e-aa93-7e52cf733f6f/userFiles-4ab5d2d6-0197-4aa8-9a48-684a80aa93fe/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T01:11:47.086+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43683.
[2023-01-31T01:11:47.087+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:47 INFO NettyBlockTransferService: Server created on 767c6fe0d003:43683
[2023-01-31T01:11:47.087+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:47 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T01:11:47.153+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:47 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 767c6fe0d003, 43683, None)
[2023-01-31T01:11:47.186+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:47 INFO BlockManagerMasterEndpoint: Registering block manager 767c6fe0d003:43683 with 434.4 MiB RAM, BlockManagerId(driver, 767c6fe0d003, 43683, None)
[2023-01-31T01:11:47.225+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:47 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 767c6fe0d003, 43683, None)
[2023-01-31T01:11:47.233+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:47 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 767c6fe0d003, 43683, None)
[2023-01-31T01:11:50.637+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:50 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T01:11:50.674+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:50 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T01:12:01.260+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:01 INFO InMemoryFileIndex: It took 430 ms to list leaf files for 1 paths.
[2023-01-31T01:12:01.997+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:01 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T01:12:02.408+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T01:12:02.419+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 767c6fe0d003:43683 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T01:12:02.439+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:02 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T01:12:05.035+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:05 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T01:12:05.108+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:05 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T01:12:05.351+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:05 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T01:12:05.600+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:05 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T01:12:05.627+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:05 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T01:12:05.629+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:05 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:12:05.696+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:05 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:12:05.730+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:05 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T01:12:06.340+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T01:12:06.428+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T01:12:06.428+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 767c6fe0d003:43683 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T01:12:06.449+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:06 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:12:06.687+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:12:06.701+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T01:12:07.351+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:07 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T01:12:07.437+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:07 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T01:12:08.322+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:08 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010400__202101010500.json:0+9364
[2023-01-31T01:12:10.812+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:10 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T01:12:10.981+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:10 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3698 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:12:11.005+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T01:12:11.347+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:11 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 5.425 s
[2023-01-31T01:12:11.581+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:11 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:12:11.582+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T01:12:11.660+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:11 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 6.298765 s
[2023-01-31T01:12:13.818+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:13 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 767c6fe0d003:43683 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T01:12:13.975+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:13 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 767c6fe0d003:43683 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T01:13:06.586+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:06 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T01:13:06.589+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:06 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T01:13:06.593+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:06 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T01:13:09.461+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:09 INFO CodeGenerator: Code generated in 1441.413076 ms
[2023-01-31T01:13:10.124+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:10 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T01:13:10.628+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:10 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T01:13:10.643+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:10 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 767c6fe0d003:43683 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T01:13:10.680+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:10 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T01:13:10.850+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T01:13:11.287+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T01:13:11.291+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T01:13:11.291+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T01:13:11.291+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:13:11.291+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:13:11.295+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T01:13:11.452+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T01:13:11.578+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T01:13:11.581+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 767c6fe0d003:43683 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T01:13:11.675+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:13:11.675+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:13:11.675+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T01:13:11.760+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T01:13:11.779+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T01:13:12.977+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:12 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T01:13:13.309+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:13 INFO CodeGenerator: Code generated in 266.081355 ms
[2023-01-31T01:13:13.956+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:13 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1671 bytes result sent to driver
[2023-01-31T01:13:13.987+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:13 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2310 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:13:13.989+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T01:13:13.995+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:13 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 2.640 s
[2023-01-31T01:13:13.996+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:13 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:13:13.996+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T01:13:13.998+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:13 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 2.709317 s
[2023-01-31T01:13:15.012+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T01:13:15.014+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T01:13:15.014+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T01:13:15.085+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO CodeGenerator: Code generated in 34.843552 ms
[2023-01-31T01:13:15.099+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T01:13:15.158+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T01:13:15.160+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 767c6fe0d003:43683 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T01:13:15.162+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T01:13:15.166+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T01:13:15.281+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T01:13:15.281+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T01:13:15.282+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T01:13:15.282+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:13:15.283+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:13:15.335+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T01:13:15.379+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T01:13:15.550+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T01:13:15.600+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 767c6fe0d003:43683 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T01:13:15.606+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:13:15.614+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:13:15.619+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T01:13:15.627+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T01:13:15.631+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T01:13:15.750+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T01:13:16.286+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:16 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T01:13:16.365+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:16 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 733 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:13:16.372+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:16 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T01:13:16.384+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:16 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 1.034 s
[2023-01-31T01:13:16.385+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:16 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:13:16.385+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T01:13:16.386+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:16 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 1.100338 s
[2023-01-31T01:13:17.402+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:17 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 767c6fe0d003:43683 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T01:13:17.547+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:17 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 767c6fe0d003:43683 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T01:13:17.784+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:17 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T01:13:17.787+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:17 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T01:13:17.787+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:17 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T01:13:21.718+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:21 INFO CodeGenerator: Code generated in 3509.658227 ms
[2023-01-31T01:13:21.895+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:21 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T01:13:22.050+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:22 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T01:13:22.057+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:22 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 767c6fe0d003:43683 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T01:13:22.076+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:22 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T01:13:22.103+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T01:13:22.353+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:22 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T01:13:22.374+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:22 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T01:13:22.375+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:22 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T01:13:22.376+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:22 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:13:22.376+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:22 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:13:22.382+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:22 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T01:13:22.401+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:22 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T01:13:22.413+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:22 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-31T01:13:22.420+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:22 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 767c6fe0d003:43683 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T01:13:22.423+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:22 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:13:22.427+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:13:22.430+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:22 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T01:13:22.442+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:22 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T01:13:22.449+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:22 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T01:13:22.563+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:22 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T01:13:23.396+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:23 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T01:13:23.411+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:23 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 970 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:13:23.422+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:23 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 1.028 s
[2023-01-31T01:13:23.423+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:23 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:13:23.426+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:23 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T01:13:23.429+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T01:13:23.469+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:23 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 1.079948 s
[2023-01-31T01:13:23.722+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:23 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T01:13:23.725+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:23 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T01:13:23.726+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:23 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T01:13:23.834+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:23 INFO CodeGenerator: Code generated in 75.783721 ms
[2023-01-31T01:13:23.863+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:23 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T01:13:23.929+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:23 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T01:13:23.944+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:23 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 767c6fe0d003:43683 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T01:13:23.958+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:23 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T01:13:23.971+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T01:13:24.070+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:24 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T01:13:24.116+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:24 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T01:13:24.117+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:24 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T01:13:24.118+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:24 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:13:24.118+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:24 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:13:24.164+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:24 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T01:13:24.164+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:24 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T01:13:24.189+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:24 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T01:13:24.194+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:24 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 767c6fe0d003:43683 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T01:13:24.197+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:24 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:13:24.205+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:13:24.210+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:24 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T01:13:24.216+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:24 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T01:13:24.239+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:24 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T01:13:24.280+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:24 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T01:13:24.474+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:24 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1585 bytes result sent to driver
[2023-01-31T01:13:24.484+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:24 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 269 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:13:24.485+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:24 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T01:13:24.485+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:24 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.349 s
[2023-01-31T01:13:24.486+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:24 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:13:24.486+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T01:13:24.487+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:24 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.417189 s
[2023-01-31T01:13:24.862+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:24 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 767c6fe0d003:43683 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T01:13:24.874+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:24 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 767c6fe0d003:43683 in memory (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T01:13:25.005+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-31T01:13:25.005+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 266, in <module>
[2023-01-31T01:13:25.005+0000] {spark_submit.py:495} INFO - main(
[2023-01-31T01:13:25.006+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 243, in main
[2023-01-31T01:13:25.006+0000] {spark_submit.py:495} INFO - run_method = method(
[2023-01-31T01:13:25.007+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 196, in transform_generation
[2023-01-31T01:13:25.007+0000] {spark_submit.py:495} INFO - all_df = parse_datetimeindex(self.spark, df_ts, df_nonts, tz=None)
[2023-01-31T01:13:25.008+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/helpers/parsers.py", line 61, in parse_datetimeindex
[2023-01-31T01:13:25.008+0000] {spark_submit.py:495} INFO - date_index = spark.sql(
[2023-01-31T01:13:25.009+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 1034, in sql
[2023-01-31T01:13:25.009+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
[2023-01-31T01:13:25.010+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 196, in deco
[2023-01-31T01:13:25.026+0000] {spark_submit.py:495} INFO - pyspark.sql.utils.ParseException:
[2023-01-31T01:13:25.026+0000] {spark_submit.py:495} INFO - Syntax error at or near ':': extra input ':'(line 1, pos 59)
[2023-01-31T01:13:25.027+0000] {spark_submit.py:495} INFO - 
[2023-01-31T01:13:25.028+0000] {spark_submit.py:495} INFO - == SQL ==
[2023-01-31T01:13:25.029+0000] {spark_submit.py:495} INFO - SELECT sequence(to_utc_timestamp('2021-01-01 04:00:00', +07:00), to_utc_timestamp('2021-01-01 05:00:00', +07:00), INTERVAL 15 MINUTES) as measured_at
[2023-01-31T01:13:25.029+0000] {spark_submit.py:495} INFO - -----------------------------------------------------------^^^
[2023-01-31T01:13:25.030+0000] {spark_submit.py:495} INFO - 
[2023-01-31T01:13:25.261+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:25 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T01:13:25.288+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:25 INFO SparkUI: Stopped Spark web UI at http://767c6fe0d003:4041
[2023-01-31T01:13:25.332+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:25 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T01:13:25.373+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:25 INFO MemoryStore: MemoryStore cleared
[2023-01-31T01:13:25.374+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:25 INFO BlockManager: BlockManager stopped
[2023-01-31T01:13:25.383+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:25 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T01:13:25.391+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:25 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T01:13:25.404+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:25 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T01:13:25.437+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:25 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T01:13:25.440+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-c802f062-ffd0-4ee7-aa7c-ec48e42122d7
[2023-01-31T01:13:25.450+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-94e9da9d-3eae-428e-aa93-7e52cf733f6f/pyspark-27aa0b15-f01d-4a55-8bec-06860c4b58b7
[2023-01-31T01:13:25.463+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-94e9da9d-3eae-428e-aa93-7e52cf733f6f
[2023-01-31T01:13:25.781+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET. Error code is: 1.
[2023-01-31T01:13:25.787+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata, task_id=stage_total_generation, execution_date=20210101T040000, start_date=20230131T011046, end_date=20230131T011325
[2023-01-31T01:13:25.816+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 682 for task stage_total_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET. Error code is: 1.; 1767)
[2023-01-31T01:13:25.872+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-31T01:13:25.905+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T01:22:23.303+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-31T01:22:23.331+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-31T01:22:23.332+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T01:22:23.332+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T01:22:23.333+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T01:22:23.366+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 04:00:00+00:00
[2023-01-31T01:22:23.379+0000] {standard_task_runner.py:55} INFO - Started process 4887 to run task
[2023-01-31T01:22:23.390+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T04:00:00+00:00', '--job-id', '715', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp8stn2nht']
[2023-01-31T01:22:23.395+0000] {standard_task_runner.py:83} INFO - Job 715: Subtask stage_total_generation
[2023-01-31T01:22:23.543+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [running]> on host 767c6fe0d003
[2023-01-31T01:22:23.686+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T04:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T04:00:00+00:00
[2023-01-31T01:22:23.721+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T01:22:23.723+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET
[2023-01-31T01:22:46.498+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:46 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T01:22:47.021+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T01:22:48.105+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:48 INFO ResourceUtils: ==============================================================
[2023-01-31T01:22:48.139+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:48 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T01:22:48.156+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:48 INFO ResourceUtils: ==============================================================
[2023-01-31T01:22:48.168+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:48 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T01:22:48.345+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:48 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T01:22:48.374+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:48 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T01:22:48.382+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:48 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T01:22:48.961+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:48 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T01:22:48.970+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:48 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T01:22:48.973+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:48 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T01:22:48.980+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:48 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T01:22:48.992+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T01:22:52.674+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:52 INFO Utils: Successfully started service 'sparkDriver' on port 42229.
[2023-01-31T01:22:53.172+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:53 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T01:22:53.834+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:53 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T01:22:54.112+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T01:22:54.135+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T01:22:54.229+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:54 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T01:22:54.613+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b0f79c02-19e8-4100-862d-5af1957c3e31
[2023-01-31T01:22:54.751+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:54 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T01:22:55.238+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:55 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T01:22:58.049+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T01:22:58.158+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:58 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-31T01:22:58.692+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:58 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://767c6fe0d003:42229/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675128166421
[2023-01-31T01:22:58.707+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:58 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://767c6fe0d003:42229/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675128166421
[2023-01-31T01:22:59.240+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:59 INFO Executor: Starting executor ID driver on host 767c6fe0d003
[2023-01-31T01:22:59.370+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:59 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T01:22:59.645+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:59 INFO Executor: Fetching spark://767c6fe0d003:42229/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675128166421
[2023-01-31T01:23:00.287+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:00 INFO TransportClientFactory: Successfully created connection to 767c6fe0d003/172.19.0.6:42229 after 432 ms (0 ms spent in bootstraps)
[2023-01-31T01:23:00.508+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:00 INFO Utils: Fetching spark://767c6fe0d003:42229/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-c565d7e7-5f7c-48fa-a8b3-569e21692d74/userFiles-d6f8ae92-b0e6-4f59-99fc-63ceba3e7276/fetchFileTemp16030950939113244253.tmp
[2023-01-31T01:23:03.102+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:03 INFO Executor: Adding file:/tmp/spark-c565d7e7-5f7c-48fa-a8b3-569e21692d74/userFiles-d6f8ae92-b0e6-4f59-99fc-63ceba3e7276/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T01:23:03.103+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:03 INFO Executor: Fetching spark://767c6fe0d003:42229/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675128166421
[2023-01-31T01:23:03.114+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:03 INFO Utils: Fetching spark://767c6fe0d003:42229/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-c565d7e7-5f7c-48fa-a8b3-569e21692d74/userFiles-d6f8ae92-b0e6-4f59-99fc-63ceba3e7276/fetchFileTemp11317457005477402963.tmp
[2023-01-31T01:23:04.615+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:04 INFO Executor: Adding file:/tmp/spark-c565d7e7-5f7c-48fa-a8b3-569e21692d74/userFiles-d6f8ae92-b0e6-4f59-99fc-63ceba3e7276/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T01:23:04.766+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35633.
[2023-01-31T01:23:04.767+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:04 INFO NettyBlockTransferService: Server created on 767c6fe0d003:35633
[2023-01-31T01:23:04.780+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:04 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T01:23:04.918+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 767c6fe0d003, 35633, None)
[2023-01-31T01:23:04.939+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:04 INFO BlockManagerMasterEndpoint: Registering block manager 767c6fe0d003:35633 with 434.4 MiB RAM, BlockManagerId(driver, 767c6fe0d003, 35633, None)
[2023-01-31T01:23:04.991+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 767c6fe0d003, 35633, None)
[2023-01-31T01:23:05.010+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 767c6fe0d003, 35633, None)
[2023-01-31T01:23:12.150+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:12 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T01:23:12.206+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:12 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T01:23:23.858+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:23 INFO InMemoryFileIndex: It took 303 ms to list leaf files for 1 paths.
[2023-01-31T01:23:24.454+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:24 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T01:23:24.681+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T01:23:24.738+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 767c6fe0d003:35633 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T01:23:24.770+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:24 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T01:23:26.930+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:26 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T01:23:27.030+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:27 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T01:23:27.219+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:27 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T01:23:27.414+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:27 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T01:23:27.430+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:27 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T01:23:27.431+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:27 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:23:27.470+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:27 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:23:27.482+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:27 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T01:23:27.802+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:27 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T01:23:27.818+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:27 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T01:23:27.822+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 767c6fe0d003:35633 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T01:23:27.837+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:27 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:23:27.969+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:23:27.980+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:27 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T01:23:28.238+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:28 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T01:23:28.376+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:28 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T01:23:29.475+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:29 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010400__202101010500.json:0+9367
[2023-01-31T01:23:30.366+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:30 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T01:23:30.434+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:30 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2302 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:23:30.448+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:30 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T01:23:30.487+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:30 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 2.962 s
[2023-01-31T01:23:30.523+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:30 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:23:30.526+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T01:23:30.531+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:30 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 3.309597 s
[2023-01-31T01:23:33.194+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:33 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 767c6fe0d003:35633 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T01:23:33.246+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:33 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 767c6fe0d003:35633 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T01:23:57.794+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:57 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T01:23:57.806+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:57 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T01:23:57.826+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:57 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T01:24:00.861+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:00 INFO CodeGenerator: Code generated in 2188.659857 ms
[2023-01-31T01:24:00.962+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:00 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T01:24:01.193+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:01 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T01:24:01.205+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:01 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 767c6fe0d003:35633 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T01:24:01.234+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:01 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T01:24:01.414+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T01:24:01.795+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:01 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T01:24:01.795+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:01 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T01:24:01.796+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:01 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T01:24:01.796+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:01 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:24:01.805+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:01 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:24:01.818+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:01 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T01:24:01.858+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:01 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T01:24:01.914+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:01 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T01:24:01.919+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:01 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 767c6fe0d003:35633 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T01:24:01.920+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:01 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:24:01.955+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:24:01.956+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:01 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T01:24:01.990+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:01 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T01:24:01.997+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:01 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T01:24:02.571+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:02 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9367, partition values: [empty row]
[2023-01-31T01:24:03.271+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:03 INFO CodeGenerator: Code generated in 509.915689 ms
[2023-01-31T01:24:04.136+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:04 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1671 bytes result sent to driver
[2023-01-31T01:24:04.161+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:04 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2229 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:24:04.164+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:04 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T01:24:04.173+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:04 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 2.328 s
[2023-01-31T01:24:04.173+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:04 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:24:04.174+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T01:24:04.176+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:04 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 2.383584 s
[2023-01-31T01:24:04.745+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:04 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T01:24:04.746+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:04 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T01:24:04.761+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:04 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T01:24:05.004+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:05 INFO CodeGenerator: Code generated in 118.431376 ms
[2023-01-31T01:24:05.080+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:05 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T01:24:05.199+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:05 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T01:24:05.203+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:05 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 767c6fe0d003:35633 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T01:24:05.215+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:05 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T01:24:05.235+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T01:24:05.430+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:05 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T01:24:05.434+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:05 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T01:24:05.435+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:05 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T01:24:05.436+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:05 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:24:05.436+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:05 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:24:05.451+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:05 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T01:24:05.474+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:05 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T01:24:05.489+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:05 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T01:24:05.494+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:05 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 767c6fe0d003:35633 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T01:24:05.507+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:05 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:24:05.515+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:24:05.531+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:05 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T01:24:05.547+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:05 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T01:24:05.548+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:05 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T01:24:05.654+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:05 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9367, partition values: [empty row]
[2023-01-31T01:24:06.271+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:06 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T01:24:06.295+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:06 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 740 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:24:06.301+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:06 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.844 s
[2023-01-31T01:24:06.302+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:06 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:24:06.305+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:06 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T01:24:06.312+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T01:24:06.371+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:06 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.936736 s
[2023-01-31T01:24:07.757+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:07 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T01:24:07.760+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:07 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T01:24:07.763+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:07 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T01:24:10.049+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:10 INFO CodeGenerator: Code generated in 1978.83373 ms
[2023-01-31T01:24:10.062+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:10 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T01:24:10.215+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:10 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T01:24:10.226+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:10 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 767c6fe0d003:35633 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T01:24:10.278+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:10 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T01:24:10.282+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T01:24:10.338+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:10 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T01:24:10.341+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:10 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T01:24:10.343+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:10 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T01:24:10.344+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:10 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:24:10.346+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:10 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:24:10.351+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:10 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T01:24:10.381+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:10 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-01-31T01:24:10.922+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:10 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T01:24:10.981+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:10 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 767c6fe0d003:35633 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T01:24:10.990+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:10 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:24:10.991+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:24:10.999+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:10 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T01:24:11.004+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:11 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T01:24:11.007+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:11 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T01:24:11.598+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:11 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9367, partition values: [empty row]
[2023-01-31T01:24:12.740+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:12 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T01:24:12.802+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:12 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1797 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:24:12.810+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:12 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 2.444 s
[2023-01-31T01:24:12.811+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:12 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:24:12.813+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:12 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T01:24:12.813+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T01:24:12.815+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:12 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 2.475650 s
[2023-01-31T01:24:12.926+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:12 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T01:24:12.927+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:12 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T01:24:12.927+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:12 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T01:24:13.028+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO CodeGenerator: Code generated in 62.785408 ms
[2023-01-31T01:24:13.087+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T01:24:13.244+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T01:24:13.247+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 767c6fe0d003:35633 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T01:24:13.256+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T01:24:13.265+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T01:24:13.458+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T01:24:13.470+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T01:24:13.471+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T01:24:13.471+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:24:13.471+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:24:13.476+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T01:24:13.529+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T01:24:13.581+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T01:24:13.591+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 767c6fe0d003:35633 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T01:24:13.598+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:24:13.609+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:24:13.613+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T01:24:13.622+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T01:24:13.623+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T01:24:13.714+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9367, partition values: [empty row]
[2023-01-31T01:24:14.183+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:14 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T01:24:14.191+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:14 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 571 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:24:14.192+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:14 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T01:24:14.193+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:14 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.715 s
[2023-01-31T01:24:14.196+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:14 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:24:14.196+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T01:24:14.197+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:14 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.729038 s
[2023-01-31T01:24:15.914+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:15 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 767c6fe0d003:35633 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T01:24:16.091+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:16 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 767c6fe0d003:35633 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T01:24:16.541+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:16 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 767c6fe0d003:35633 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T01:24:16.728+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:16 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 767c6fe0d003:35633 in memory (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T01:24:17.075+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-31T01:24:17.075+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 266, in <module>
[2023-01-31T01:24:17.076+0000] {spark_submit.py:495} INFO - main(
[2023-01-31T01:24:17.076+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 243, in main
[2023-01-31T01:24:17.077+0000] {spark_submit.py:495} INFO - run_method = method(
[2023-01-31T01:24:17.077+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 196, in transform_generation
[2023-01-31T01:24:17.077+0000] {spark_submit.py:495} INFO - all_df = parse_datetimeindex(self.spark, df_ts, df_nonts, tz=None)
[2023-01-31T01:24:17.078+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/helpers/parsers.py", line 61, in parse_datetimeindex
[2023-01-31T01:24:17.078+0000] {spark_submit.py:495} INFO - date_index = spark.sql(
[2023-01-31T01:24:17.096+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 1034, in sql
[2023-01-31T01:24:17.104+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
[2023-01-31T01:24:17.105+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 196, in deco
[2023-01-31T01:24:17.105+0000] {spark_submit.py:495} INFO - pyspark.sql.utils.ParseException:
[2023-01-31T01:24:17.106+0000] {spark_submit.py:495} INFO - Syntax error at or near ':': extra input ':'(line 1, pos 59)
[2023-01-31T01:24:17.106+0000] {spark_submit.py:495} INFO - 
[2023-01-31T01:24:17.106+0000] {spark_submit.py:495} INFO - == SQL ==
[2023-01-31T01:24:17.107+0000] {spark_submit.py:495} INFO - SELECT sequence(to_utc_timestamp('2021-01-01 05:00:00', +07:00), to_utc_timestamp('2021-01-01 06:00:00', +07:00), INTERVAL 15 MINUTES) as measured_at
[2023-01-31T01:24:17.107+0000] {spark_submit.py:495} INFO - -----------------------------------------------------------^^^
[2023-01-31T01:24:17.108+0000] {spark_submit.py:495} INFO - 
[2023-01-31T01:24:18.283+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:18 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T01:24:18.880+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:18 INFO SparkUI: Stopped Spark web UI at http://767c6fe0d003:4041
[2023-01-31T01:24:19.047+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:19 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T01:24:19.257+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:19 INFO MemoryStore: MemoryStore cleared
[2023-01-31T01:24:19.269+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:19 INFO BlockManager: BlockManager stopped
[2023-01-31T01:24:19.286+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:19 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T01:24:19.303+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:19 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T01:24:19.420+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:19 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T01:24:19.420+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:19 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T01:24:19.421+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-c565d7e7-5f7c-48fa-a8b3-569e21692d74
[2023-01-31T01:24:19.485+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-15980975-ba0b-445e-b903-9d925eb3085a
[2023-01-31T01:24:19.590+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-c565d7e7-5f7c-48fa-a8b3-569e21692d74/pyspark-afae661e-e45c-4cb2-a4f2-96abb2fb73ab
[2023-01-31T01:24:20.785+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET. Error code is: 1.
[2023-01-31T01:24:20.833+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata, task_id=stage_total_generation, execution_date=20210101T040000, start_date=20230131T012223, end_date=20230131T012420
[2023-01-31T01:24:20.947+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 715 for task stage_total_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET. Error code is: 1.; 4887)
[2023-01-31T01:24:21.155+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-31T01:24:21.253+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T01:26:24.555+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-31T01:26:24.644+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-31T01:26:24.644+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T01:26:24.645+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T01:26:24.646+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T01:26:24.701+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 04:00:00+00:00
[2023-01-31T01:26:24.722+0000] {standard_task_runner.py:55} INFO - Started process 6506 to run task
[2023-01-31T01:26:24.738+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T04:00:00+00:00', '--job-id', '735', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpdaoajiqa']
[2023-01-31T01:26:24.745+0000] {standard_task_runner.py:83} INFO - Job 735: Subtask stage_total_generation
[2023-01-31T01:26:24.981+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [running]> on host 767c6fe0d003
[2023-01-31T01:26:25.231+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T04:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T04:00:00+00:00
[2023-01-31T01:26:25.269+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T01:26:25.273+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET
[2023-01-31T01:26:50.667+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:50 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T01:26:51.182+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T01:26:51.638+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:51 INFO ResourceUtils: ==============================================================
[2023-01-31T01:26:51.645+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:51 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T01:26:51.645+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:51 INFO ResourceUtils: ==============================================================
[2023-01-31T01:26:51.646+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:51 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T01:26:51.729+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:51 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T01:26:51.745+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:51 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T01:26:51.755+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:51 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T01:26:52.009+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:52 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T01:26:52.024+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:52 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T01:26:52.032+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:52 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T01:26:52.037+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:52 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T01:26:52.047+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T01:26:53.734+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:53 INFO Utils: Successfully started service 'sparkDriver' on port 34941.
[2023-01-31T01:26:53.950+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:53 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T01:26:54.671+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:54 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T01:26:54.771+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T01:26:54.773+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T01:26:54.821+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:54 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T01:26:54.946+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8a096c6d-f257-45e9-be08-acca39bc5cd4
[2023-01-31T01:26:55.004+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:55 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T01:26:55.105+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:55 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T01:26:55.881+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T01:26:55.882+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:55 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T01:26:55.913+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:55 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T01:26:55.913+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:55 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T01:26:55.914+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:55 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-31T01:26:55.929+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:55 INFO Utils: Successfully started service 'SparkUI' on port 4045.
[2023-01-31T01:26:56.006+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:56 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://767c6fe0d003:34941/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675128410624
[2023-01-31T01:26:56.009+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:56 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://767c6fe0d003:34941/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675128410624
[2023-01-31T01:26:56.359+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:56 INFO Executor: Starting executor ID driver on host 767c6fe0d003
[2023-01-31T01:26:56.376+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:56 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T01:26:56.402+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:56 INFO Executor: Fetching spark://767c6fe0d003:34941/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675128410624
[2023-01-31T01:26:56.654+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:56 INFO TransportClientFactory: Successfully created connection to 767c6fe0d003/172.19.0.6:34941 after 191 ms (0 ms spent in bootstraps)
[2023-01-31T01:26:56.706+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:56 INFO Utils: Fetching spark://767c6fe0d003:34941/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-a768692d-cfee-4355-9edb-9ad637f07520/userFiles-7abfe33e-32bb-4243-a4f0-51631db152b4/fetchFileTemp15158665381869285712.tmp
[2023-01-31T01:26:58.150+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:58 INFO Executor: Adding file:/tmp/spark-a768692d-cfee-4355-9edb-9ad637f07520/userFiles-7abfe33e-32bb-4243-a4f0-51631db152b4/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T01:26:58.151+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:58 INFO Executor: Fetching spark://767c6fe0d003:34941/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675128410624
[2023-01-31T01:26:58.210+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:58 INFO Utils: Fetching spark://767c6fe0d003:34941/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-a768692d-cfee-4355-9edb-9ad637f07520/userFiles-7abfe33e-32bb-4243-a4f0-51631db152b4/fetchFileTemp15317888138389822270.tmp
[2023-01-31T01:26:59.471+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:59 INFO Executor: Adding file:/tmp/spark-a768692d-cfee-4355-9edb-9ad637f07520/userFiles-7abfe33e-32bb-4243-a4f0-51631db152b4/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T01:26:59.515+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34221.
[2023-01-31T01:26:59.516+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:59 INFO NettyBlockTransferService: Server created on 767c6fe0d003:34221
[2023-01-31T01:26:59.519+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T01:26:59.543+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 767c6fe0d003, 34221, None)
[2023-01-31T01:26:59.666+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:59 INFO BlockManagerMasterEndpoint: Registering block manager 767c6fe0d003:34221 with 434.4 MiB RAM, BlockManagerId(driver, 767c6fe0d003, 34221, None)
[2023-01-31T01:26:59.714+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 767c6fe0d003, 34221, None)
[2023-01-31T01:26:59.719+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 767c6fe0d003, 34221, None)
[2023-01-31T01:27:05.906+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:05 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T01:27:06.022+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:06 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T01:27:22.191+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:22 INFO InMemoryFileIndex: It took 334 ms to list leaf files for 1 paths.
[2023-01-31T01:27:23.855+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T01:27:24.461+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T01:27:24.471+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 767c6fe0d003:34221 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T01:27:24.482+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:24 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T01:27:27.878+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:27 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T01:27:28.018+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:28 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T01:27:28.150+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:28 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T01:27:28.487+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:28 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T01:27:28.612+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:28 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T01:27:28.613+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:27:28.619+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:27:28.739+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:28 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T01:27:29.884+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:29 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1675128448620,ArraySeq(org.apache.spark.scheduler.StageInfo@1ad264c1),{spark.master=local, spark.driver.port=34941, spark.submit.pyFiles=, spark.app.startTime=1675128410624, spark.rdd.compress=True, spark.executor.id=driver, spark.app.name=gcp_playground, spark.submit.deployMode=client, spark.hadoop.google.cloud.auth.service.account.json.keyfile=/.google/credentials/google_credentials.json, spark.driver.host=767c6fe0d003, spark.app.id=local-1675128416197, spark.app.submitTime=1675128403972, spark.hadoop.fs.AbstractFileSystem.gs.impl=google.cloud.hadoop.fs.gcs.GoogleHadoopFS, spark.sql.session.timeZone=UTC, spark.hadoop.google.cloud.auth.service.account.enable=true, spark.app.initial.jar.urls=spark://767c6fe0d003:34941/jars/gcs-connector-hadoop3-latest.jar,spark://767c6fe0d003:34941/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED, spark.jars=/opt/spark/jars/gcs-connector-hadoop3-latest.jar, /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.sql.warehouse.dir=file:/opt/***/spark-warehouse, spark.serializer.objectStreamReset=100, spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED}) by listener AppStatusListener took 1.139162949s.
[2023-01-31T01:27:31.068+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:31 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T01:27:31.168+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:31 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T01:27:31.175+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 767c6fe0d003:34221 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T01:27:31.186+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:31 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:27:31.506+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:27:31.521+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T01:27:32.330+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:32 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T01:27:32.677+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:32 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T01:27:34.774+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:34 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010400__202101010500.json:0+9364
[2023-01-31T01:27:37.327+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:37 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T01:27:37.534+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:37 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 5434 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:27:37.542+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:37 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T01:27:37.669+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:37 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 7.980 s
[2023-01-31T01:27:37.721+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:37 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:27:37.723+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T01:27:37.747+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:37 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 9.595809 s
[2023-01-31T01:27:45.070+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:45 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 767c6fe0d003:34221 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T01:27:45.279+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:45 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 767c6fe0d003:34221 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T01:28:06.384+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:06 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T01:28:06.387+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:06 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T01:28:06.393+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:06 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T01:28:08.229+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:08 INFO CodeGenerator: Code generated in 966.122695 ms
[2023-01-31T01:28:08.258+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:08 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T01:28:08.294+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:08 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T01:28:08.295+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:08 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 767c6fe0d003:34221 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T01:28:08.303+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:08 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T01:28:08.331+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T01:28:08.489+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:08 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T01:28:08.492+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:08 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T01:28:08.492+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:08 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T01:28:08.493+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:08 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:28:08.494+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:08 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:28:08.499+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:08 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T01:28:08.524+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:08 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T01:28:08.539+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:08 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T01:28:08.542+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:08 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 767c6fe0d003:34221 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T01:28:08.545+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:08 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:28:08.552+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:28:08.554+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:08 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T01:28:08.566+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:08 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T01:28:08.569+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:08 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T01:28:08.906+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:08 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T01:28:09.600+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:09 INFO CodeGenerator: Code generated in 545.565634 ms
[2023-01-31T01:28:09.977+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:09 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T01:28:09.991+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:09 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1429 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:28:09.994+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:09 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 1.495 s
[2023-01-31T01:28:09.994+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:09 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:28:09.999+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:09 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T01:28:10.016+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T01:28:10.016+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 1.512232 s
[2023-01-31T01:28:10.420+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 767c6fe0d003:34221 in memory (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T01:28:10.456+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T01:28:10.457+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T01:28:10.458+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T01:28:10.497+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO CodeGenerator: Code generated in 18.556884 ms
[2023-01-31T01:28:10.512+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T01:28:10.591+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T01:28:10.596+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 767c6fe0d003:34221 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T01:28:10.598+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T01:28:10.609+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T01:28:10.790+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T01:28:10.792+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T01:28:10.793+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T01:28:10.793+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:28:10.796+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:28:10.800+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T01:28:10.805+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T01:28:10.836+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T01:28:10.843+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 767c6fe0d003:34221 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T01:28:10.846+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:28:10.847+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:28:10.847+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T01:28:10.852+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T01:28:10.853+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T01:28:10.894+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T01:28:11.068+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T01:28:11.084+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 235 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:28:11.089+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T01:28:11.089+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.284 s
[2023-01-31T01:28:11.089+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:28:11.089+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T01:28:11.090+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.295849 s
[2023-01-31T01:28:11.506+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T01:28:11.510+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T01:28:11.513+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T01:28:12.041+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO CodeGenerator: Code generated in 307.352785 ms
[2023-01-31T01:28:12.058+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T01:28:12.097+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T01:28:12.098+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 767c6fe0d003:34221 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T01:28:12.100+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T01:28:12.106+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T01:28:12.254+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T01:28:12.257+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T01:28:12.258+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T01:28:12.258+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:28:12.258+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:28:12.266+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T01:28:12.276+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T01:28:12.296+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T01:28:12.300+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 767c6fe0d003:34221 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T01:28:12.302+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:28:12.302+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:28:12.303+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T01:28:12.305+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T01:28:12.305+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T01:28:12.344+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T01:28:12.514+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T01:28:12.522+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 218 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:28:12.529+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.265 s
[2023-01-31T01:28:12.529+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:28:12.530+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T01:28:12.531+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T01:28:12.532+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.278255 s
[2023-01-31T01:28:12.716+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T01:28:12.717+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T01:28:12.718+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T01:28:12.770+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO CodeGenerator: Code generated in 39.763313 ms
[2023-01-31T01:28:12.780+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T01:28:12.806+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T01:28:12.810+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 767c6fe0d003:34221 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T01:28:12.812+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T01:28:12.814+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T01:28:12.852+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T01:28:12.854+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T01:28:12.854+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T01:28:12.855+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:28:12.855+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:28:12.855+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T01:28:12.876+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T01:28:12.988+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T01:28:12.989+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 767c6fe0d003:34221 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T01:28:12.990+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:28:12.993+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:28:12.993+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T01:28:12.998+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T01:28:12.998+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T01:28:13.026+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 767c6fe0d003:34221 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T01:28:13.038+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T01:28:13.263+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1585 bytes result sent to driver
[2023-01-31T01:28:13.266+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 767c6fe0d003:34221 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T01:28:13.266+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 270 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:28:13.266+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T01:28:13.267+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.411 s
[2023-01-31T01:28:13.268+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:28:13.269+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T01:28:13.279+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.426248 s
[2023-01-31T01:28:13.794+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-31T01:28:13.795+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 266, in <module>
[2023-01-31T01:28:13.795+0000] {spark_submit.py:495} INFO - main(
[2023-01-31T01:28:13.796+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 243, in main
[2023-01-31T01:28:13.796+0000] {spark_submit.py:495} INFO - run_method = method(
[2023-01-31T01:28:13.796+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 196, in transform_generation
[2023-01-31T01:28:13.796+0000] {spark_submit.py:495} INFO - all_df = parse_datetimeindex(self.spark, df_ts, df_nonts, tz=None)
[2023-01-31T01:28:13.797+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/helpers/parsers.py", line 61, in parse_datetimeindex
[2023-01-31T01:28:13.797+0000] {spark_submit.py:495} INFO - date_index = spark.sql(
[2023-01-31T01:28:13.797+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 1034, in sql
[2023-01-31T01:28:13.797+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
[2023-01-31T01:28:13.797+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 196, in deco
[2023-01-31T01:28:13.816+0000] {spark_submit.py:495} INFO - pyspark.sql.utils.ParseException:
[2023-01-31T01:28:13.817+0000] {spark_submit.py:495} INFO - Syntax error at or near ':': extra input ':'(line 1, pos 59)
[2023-01-31T01:28:13.817+0000] {spark_submit.py:495} INFO - 
[2023-01-31T01:28:13.817+0000] {spark_submit.py:495} INFO - == SQL ==
[2023-01-31T01:28:13.818+0000] {spark_submit.py:495} INFO - SELECT sequence(to_utc_timestamp('2021-01-01 04:00:00', +07:00), to_utc_timestamp('2021-01-01 05:00:00', +07:00), INTERVAL 15 MINUTES) as measured_at
[2023-01-31T01:28:13.818+0000] {spark_submit.py:495} INFO - -----------------------------------------------------------^^^
[2023-01-31T01:28:13.818+0000] {spark_submit.py:495} INFO - 
[2023-01-31T01:28:14.067+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:14 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T01:28:14.098+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:14 INFO SparkUI: Stopped Spark web UI at http://767c6fe0d003:4045
[2023-01-31T01:28:14.142+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:14 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T01:28:14.204+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:14 INFO MemoryStore: MemoryStore cleared
[2023-01-31T01:28:14.204+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:14 INFO BlockManager: BlockManager stopped
[2023-01-31T01:28:14.225+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:14 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T01:28:14.235+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:14 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T01:28:14.271+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:14 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T01:28:14.272+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:14 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T01:28:14.278+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-a768692d-cfee-4355-9edb-9ad637f07520
[2023-01-31T01:28:14.302+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-84771ad3-82dd-4ebf-b82b-5876d376045b
[2023-01-31T01:28:14.340+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-a768692d-cfee-4355-9edb-9ad637f07520/pyspark-6bcc0faa-4c43-4f93-8d8f-149e882b8bbc
[2023-01-31T01:28:14.914+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET. Error code is: 1.
[2023-01-31T01:28:14.920+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata, task_id=stage_total_generation, execution_date=20210101T040000, start_date=20230131T012624, end_date=20230131T012814
[2023-01-31T01:28:14.942+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 735 for task stage_total_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET. Error code is: 1.; 6506)
[2023-01-31T01:28:14.992+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-31T01:28:15.032+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T03:42:46.363+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-31T03:42:46.405+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-31T03:42:46.406+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T03:42:46.406+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T03:42:46.406+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T03:42:46.461+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 04:00:00+00:00
[2023-01-31T03:42:46.521+0000] {standard_task_runner.py:55} INFO - Started process 410 to run task
[2023-01-31T03:42:46.538+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T04:00:00+00:00', '--job-id', '806', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp9ea4p7d5']
[2023-01-31T03:42:46.544+0000] {standard_task_runner.py:83} INFO - Job 806: Subtask stage_total_generation
[2023-01-31T03:42:46.849+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [running]> on host 2f101bc15e7a
[2023-01-31T03:42:47.241+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T04:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T04:00:00+00:00
[2023-01-31T03:42:47.306+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T03:42:47.316+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET
[2023-01-31T03:44:12.329+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:12 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T03:44:14.793+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T03:44:17.220+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:17 INFO ResourceUtils: ==============================================================
[2023-01-31T03:44:17.229+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:17 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T03:44:17.260+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:17 INFO ResourceUtils: ==============================================================
[2023-01-31T03:44:17.308+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:17 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T03:44:19.045+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:19 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T03:44:19.278+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:19 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T03:44:19.352+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:19 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T03:44:22.587+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:22 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T03:44:22.591+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:22 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T03:44:22.599+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:22 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T03:44:22.603+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:22 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T03:44:22.609+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T03:44:33.403+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:33 INFO Utils: Successfully started service 'sparkDriver' on port 40893.
[2023-01-31T03:44:34.479+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:34 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T03:44:36.598+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:36 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T03:44:37.205+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:37 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T03:44:37.221+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:37 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T03:44:37.310+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:37 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T03:44:37.575+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:37 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fa3928a9-0eb7-49af-85ff-1b6877590b44
[2023-01-31T03:44:37.750+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:37 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T03:44:38.249+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:38 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T03:44:44.891+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T03:44:44.971+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:44 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T03:44:44.974+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:44 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T03:44:45.006+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:44 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T03:44:45.020+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:45 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-31T03:44:45.024+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:45 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[2023-01-31T03:44:45.172+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:45 INFO Utils: Successfully started service 'SparkUI' on port 4046.
[2023-01-31T03:44:45.995+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:45 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://2f101bc15e7a:40893/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675136652058
[2023-01-31T03:44:45.997+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:45 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://2f101bc15e7a:40893/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675136652058
[2023-01-31T03:44:47.067+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:47 INFO Executor: Starting executor ID driver on host 2f101bc15e7a
[2023-01-31T03:44:47.344+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:47 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T03:44:47.671+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:47 INFO Executor: Fetching spark://2f101bc15e7a:40893/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675136652058
[2023-01-31T03:44:47.991+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:47 INFO TransportClientFactory: Successfully created connection to 2f101bc15e7a/172.19.0.4:40893 after 183 ms (0 ms spent in bootstraps)
[2023-01-31T03:44:48.060+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:48 INFO Utils: Fetching spark://2f101bc15e7a:40893/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-f0da6cf6-191a-4da8-b587-2aaf1c8a7e8d/userFiles-57331e63-6516-4ac0-940b-1262ef2dca55/fetchFileTemp10788529456920773093.tmp
[2023-01-31T03:44:49.075+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:49 INFO Executor: Adding file:/tmp/spark-f0da6cf6-191a-4da8-b587-2aaf1c8a7e8d/userFiles-57331e63-6516-4ac0-940b-1262ef2dca55/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T03:44:49.078+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:49 INFO Executor: Fetching spark://2f101bc15e7a:40893/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675136652058
[2023-01-31T03:44:49.092+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:49 INFO Utils: Fetching spark://2f101bc15e7a:40893/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-f0da6cf6-191a-4da8-b587-2aaf1c8a7e8d/userFiles-57331e63-6516-4ac0-940b-1262ef2dca55/fetchFileTemp7174879989141605590.tmp
[2023-01-31T03:44:50.160+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:50 INFO Executor: Adding file:/tmp/spark-f0da6cf6-191a-4da8-b587-2aaf1c8a7e8d/userFiles-57331e63-6516-4ac0-940b-1262ef2dca55/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T03:44:50.226+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34137.
[2023-01-31T03:44:50.227+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:50 INFO NettyBlockTransferService: Server created on 2f101bc15e7a:34137
[2023-01-31T03:44:50.237+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T03:44:50.337+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 2f101bc15e7a, 34137, None)
[2023-01-31T03:44:50.454+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:50 INFO BlockManagerMasterEndpoint: Registering block manager 2f101bc15e7a:34137 with 434.4 MiB RAM, BlockManagerId(driver, 2f101bc15e7a, 34137, None)
[2023-01-31T03:44:50.508+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 2f101bc15e7a, 34137, None)
[2023-01-31T03:44:50.525+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 2f101bc15e7a, 34137, None)
[2023-01-31T03:44:57.157+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:57 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T03:44:57.383+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:57 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T03:45:46.753+0000] {spark_submit.py:495} INFO - 23/01/31 03:45:46 INFO InMemoryFileIndex: It took 1624 ms to list leaf files for 1 paths.
[2023-01-31T03:45:50.712+0000] {spark_submit.py:495} INFO - 23/01/31 03:45:50 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T03:45:51.776+0000] {spark_submit.py:495} INFO - 23/01/31 03:45:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T03:45:51.946+0000] {spark_submit.py:495} INFO - 23/01/31 03:45:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 2f101bc15e7a:34137 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T03:45:52.106+0000] {spark_submit.py:495} INFO - 23/01/31 03:45:52 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T03:46:11.224+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:11 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T03:46:12.074+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:12 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T03:46:13.511+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:13 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T03:46:14.154+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:14 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T03:46:14.160+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:14 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T03:46:14.178+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:14 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T03:46:14.196+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:14 INFO DAGScheduler: Missing parents: List()
[2023-01-31T03:46:14.488+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:14 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T03:46:17.726+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:17 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T03:46:17.759+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T03:46:17.766+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 2f101bc15e7a:34137 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T03:46:17.789+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:17 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T03:46:18.249+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T03:46:18.256+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:18 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T03:46:19.790+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:19 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (2f101bc15e7a, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T03:46:20.396+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:20 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T03:46:24.646+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:24 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010400__202101010500.json:0+9364
[2023-01-31T03:46:34.305+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:34 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T03:46:34.891+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:34 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 15189 ms on 2f101bc15e7a (executor driver) (1/1)
[2023-01-31T03:46:35.030+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:35 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T03:46:35.803+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:35 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 19.461 s
[2023-01-31T03:46:36.609+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:36 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T03:46:36.624+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T03:46:36.679+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:36 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 23.139970 s
[2023-01-31T03:46:36.781+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:36 INFO AsyncEventQueue: Process of event SparkListenerTaskEnd(0,0,ResultTask,Success,org.apache.spark.scheduler.TaskInfo@798324ac,org.apache.spark.executor.ExecutorMetrics@37031bc8,org.apache.spark.executor.TaskMetrics@3666af47) by listener AppStatusListener took 1.172112215s.
[2023-01-31T03:47:15.981+0000] {spark_submit.py:495} INFO - 23/01/31 03:47:15 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 2f101bc15e7a:34137 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T03:47:16.141+0000] {spark_submit.py:495} INFO - 23/01/31 03:47:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 2f101bc15e7a:34137 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T03:48:27.399+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:27 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T03:48:27.419+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:27 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T03:48:27.459+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:27 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T03:48:36.022+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:36 INFO CodeGenerator: Code generated in 3364.796053 ms
[2023-01-31T03:48:36.052+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:36 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T03:48:36.234+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:36 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T03:48:36.247+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:36 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 2f101bc15e7a:34137 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T03:48:36.257+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:36 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T03:48:36.329+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T03:48:38.196+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:38 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T03:48:38.199+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:38 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T03:48:38.200+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:38 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T03:48:38.200+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:38 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T03:48:38.201+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:38 INFO DAGScheduler: Missing parents: List()
[2023-01-31T03:48:38.231+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:38 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T03:48:38.362+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:38 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T03:48:38.483+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:38 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T03:48:38.523+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:38 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 2f101bc15e7a:34137 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T03:48:38.524+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:38 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T03:48:38.529+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T03:48:38.536+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:38 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T03:48:38.677+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:38 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (2f101bc15e7a, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T03:48:38.728+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:38 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T03:48:43.455+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:43 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T03:48:46.879+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:46 INFO CodeGenerator: Code generated in 2611.8237 ms
[2023-01-31T03:48:49.644+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:49 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1671 bytes result sent to driver
[2023-01-31T03:48:49.734+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:49 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 11160 ms on 2f101bc15e7a (executor driver) (1/1)
[2023-01-31T03:48:49.753+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:49 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T03:48:49.772+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:49 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 11.453 s
[2023-01-31T03:48:49.811+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:49 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T03:48:49.854+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T03:48:49.861+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:49 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 11.662885 s
[2023-01-31T03:48:53.330+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:53 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T03:48:53.331+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:53 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T03:48:53.414+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:53 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T03:48:54.896+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:54 INFO CodeGenerator: Code generated in 666.262002 ms
[2023-01-31T03:48:55.141+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:55 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T03:48:55.218+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:55 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T03:48:55.222+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:55 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 2f101bc15e7a:34137 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T03:48:55.243+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:55 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T03:48:55.346+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T03:48:55.787+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:55 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T03:48:55.794+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:55 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T03:48:55.795+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:55 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T03:48:55.798+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:55 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T03:48:55.798+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:55 INFO DAGScheduler: Missing parents: List()
[2023-01-31T03:48:55.815+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:55 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T03:48:55.864+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:55 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T03:48:55.945+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:55 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T03:48:55.961+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:55 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 2f101bc15e7a:34137 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T03:48:55.975+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:55 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T03:48:56.003+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T03:48:56.010+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:56 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T03:48:56.027+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:56 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (2f101bc15e7a, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T03:48:56.053+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:56 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T03:48:56.376+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:56 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T03:48:58.166+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:58 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T03:48:58.240+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:58 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2211 ms on 2f101bc15e7a (executor driver) (1/1)
[2023-01-31T03:48:58.267+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:58 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 2.431 s
[2023-01-31T03:48:58.277+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:58 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T03:48:58.319+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:58 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T03:48:58.323+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T03:48:58.333+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:58 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 2.543484 s
[2023-01-31T03:49:00.753+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:00 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 2f101bc15e7a:34137 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T03:49:01.495+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:01 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T03:49:01.500+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:01 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T03:49:01.510+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:01 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T03:49:09.333+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:09 INFO CodeGenerator: Code generated in 4206.112113 ms
[2023-01-31T03:49:09.717+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:09 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T03:49:10.816+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:10 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T03:49:10.880+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:10 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 2f101bc15e7a:34137 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T03:49:10.902+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:10 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T03:49:10.966+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T03:49:11.856+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:11 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T03:49:11.862+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:11 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T03:49:11.863+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:11 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T03:49:11.867+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:11 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T03:49:11.868+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:11 INFO DAGScheduler: Missing parents: List()
[2023-01-31T03:49:11.966+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:11 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T03:49:12.319+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:12 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T03:49:12.489+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:12 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T03:49:12.509+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 2f101bc15e7a:34137 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T03:49:12.528+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:12 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T03:49:12.746+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T03:49:12.747+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:12 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T03:49:12.799+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:12 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (2f101bc15e7a, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T03:49:12.804+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:12 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T03:49:13.307+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:13 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T03:49:15.099+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:15 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1755 bytes result sent to driver
[2023-01-31T03:49:15.135+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:15 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 2348 ms on 2f101bc15e7a (executor driver) (1/1)
[2023-01-31T03:49:15.137+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:15 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T03:49:15.172+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:15 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 3.149 s
[2023-01-31T03:49:15.179+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:15 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T03:49:15.185+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T03:49:15.202+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:15 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 3.345474 s
[2023-01-31T03:49:15.902+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:15 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T03:49:15.906+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:15 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T03:49:15.908+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:15 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T03:49:17.157+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:17 INFO CodeGenerator: Code generated in 1194.95171 ms
[2023-01-31T03:49:17.206+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:17 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T03:49:17.533+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:17 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T03:49:17.536+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:17 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 2f101bc15e7a:34137 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T03:49:17.544+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:17 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T03:49:17.563+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T03:49:18.955+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:18 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 2f101bc15e7a:34137 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T03:49:19.009+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:19 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T03:49:19.018+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:19 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T03:49:19.023+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:19 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T03:49:19.024+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:19 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T03:49:19.033+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:19 INFO DAGScheduler: Missing parents: List()
[2023-01-31T03:49:19.167+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:19 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T03:49:19.700+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:19 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T03:49:19.954+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:19 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T03:49:20.030+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:20 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 2f101bc15e7a:34137 (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T03:49:20.059+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:20 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T03:49:20.185+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T03:49:20.194+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:20 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T03:49:20.226+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:20 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (2f101bc15e7a, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T03:49:20.308+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:20 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T03:49:21.438+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:21 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T03:49:23.558+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:23 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T03:49:23.567+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:23 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 3354 ms on 2f101bc15e7a (executor driver) (1/1)
[2023-01-31T03:49:23.577+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:23 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 4.367 s
[2023-01-31T03:49:23.580+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:23 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T03:49:23.586+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:23 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T03:49:23.642+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T03:49:23.643+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:23 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 4.625725 s
[2023-01-31T03:49:26.300+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:26 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 2f101bc15e7a:34137 in memory (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T03:49:31.225+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-31T03:49:31.227+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 266, in <module>
[2023-01-31T03:49:31.231+0000] {spark_submit.py:495} INFO - main(
[2023-01-31T03:49:31.237+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 243, in main
[2023-01-31T03:49:31.248+0000] {spark_submit.py:495} INFO - run_method = method(
[2023-01-31T03:49:31.254+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 196, in transform_generation
[2023-01-31T03:49:31.262+0000] {spark_submit.py:495} INFO - all_df = parse_datetimeindex(self.spark, df_ts, df_nonts, tz=None)
[2023-01-31T03:49:31.271+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/helpers/parsers.py", line 70, in parse_datetimeindex
[2023-01-31T03:49:31.276+0000] {spark_submit.py:495} INFO - date_index = date_index.selectExpr(
[2023-01-31T03:49:31.283+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 2048, in selectExpr
[2023-01-31T03:49:31.298+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
[2023-01-31T03:49:31.304+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 196, in deco
[2023-01-31T03:49:31.461+0000] {spark_submit.py:495} INFO - pyspark.sql.utils.ParseException:
[2023-01-31T03:49:31.462+0000] {spark_submit.py:495} INFO - Syntax error at or near ':': extra input ':'(line 1, pos 43)
[2023-01-31T03:49:31.463+0000] {spark_submit.py:495} INFO - 
[2023-01-31T03:49:31.464+0000] {spark_submit.py:495} INFO - == SQL ==
[2023-01-31T03:49:31.464+0000] {spark_submit.py:495} INFO - to_utc_timestamp('2023-01-31 03:42:27', +07:00) as created_at
[2023-01-31T03:49:31.465+0000] {spark_submit.py:495} INFO - -------------------------------------------^^^
[2023-01-31T03:49:31.466+0000] {spark_submit.py:495} INFO - 
[2023-01-31T03:49:35.027+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:35 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T03:49:35.776+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:35 INFO SparkUI: Stopped Spark web UI at http://2f101bc15e7a:4046
[2023-01-31T03:49:36.859+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:36 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T03:49:37.599+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:37 INFO MemoryStore: MemoryStore cleared
[2023-01-31T03:49:37.599+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:37 INFO BlockManager: BlockManager stopped
[2023-01-31T03:49:37.761+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:37 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T03:49:37.925+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T03:49:38.679+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:38 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T03:49:38.679+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:38 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T03:49:38.765+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-f0da6cf6-191a-4da8-b587-2aaf1c8a7e8d
[2023-01-31T03:49:39.006+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-ee7c4de6-f43f-4822-8d26-964b5d3eb537
[2023-01-31T03:49:39.209+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-f0da6cf6-191a-4da8-b587-2aaf1c8a7e8d/pyspark-52812f1e-ced3-4d3d-8117-559c0de23599
[2023-01-31T03:49:41.918+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET. Error code is: 1.
[2023-01-31T03:49:42.027+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata, task_id=stage_total_generation, execution_date=20210101T040000, start_date=20230131T034246, end_date=20230131T034942
[2023-01-31T03:49:42.192+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 806 for task stage_total_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET. Error code is: 1.; 410)
[2023-01-31T03:49:42.662+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-31T03:49:42.908+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T03:54:19.559+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-31T03:54:19.781+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-31T03:54:19.785+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T03:54:19.786+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T03:54:19.823+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T03:54:20.411+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 04:00:00+00:00
[2023-01-31T03:54:20.809+0000] {standard_task_runner.py:55} INFO - Started process 2395 to run task
[2023-01-31T03:54:20.943+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T04:00:00+00:00', '--job-id', '818', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpt4ljfnsu']
[2023-01-31T03:54:20.980+0000] {standard_task_runner.py:83} INFO - Job 818: Subtask stage_total_generation
[2023-01-31T03:54:21.560+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [running]> on host 2f101bc15e7a
[2023-01-31T03:54:22.435+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T04:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T04:00:00+00:00
[2023-01-31T03:54:22.579+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T03:54:22.600+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET
[2023-01-31T03:55:48.018+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:47 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T03:55:50.531+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T03:55:55.878+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:55 INFO ResourceUtils: ==============================================================
[2023-01-31T03:55:55.978+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:55 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T03:55:56.031+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:56 INFO ResourceUtils: ==============================================================
[2023-01-31T03:55:56.043+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:56 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T03:55:57.379+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T03:55:57.641+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:57 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T03:55:57.648+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:57 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T03:55:58.128+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:58 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T03:55:58.137+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:58 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T03:55:58.183+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:58 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T03:55:58.186+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:58 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T03:55:58.227+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T03:56:10.479+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:10 INFO Utils: Successfully started service 'sparkDriver' on port 44235.
[2023-01-31T03:56:11.269+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:11 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T03:56:14.710+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:14 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T03:56:17.667+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:17 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T03:56:17.731+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:17 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T03:56:18.052+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:18 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T03:56:19.021+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:19 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f244e166-96eb-40c7-b59b-faf81a6a4596
[2023-01-31T03:56:19.398+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:19 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T03:56:20.778+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:20 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T03:56:32.118+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T03:56:32.251+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:32 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T03:56:32.333+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:32 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T03:56:32.493+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:32 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T03:56:32.509+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:32 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-31T03:56:32.517+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:32 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[2023-01-31T03:56:33.446+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:33 INFO Utils: Successfully started service 'SparkUI' on port 4046.
[2023-01-31T03:56:35.486+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:35 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://2f101bc15e7a:44235/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675137347603
[2023-01-31T03:56:35.490+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:35 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://2f101bc15e7a:44235/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675137347603
[2023-01-31T03:56:40.100+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:40 INFO Executor: Starting executor ID driver on host 2f101bc15e7a
[2023-01-31T03:56:40.235+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:40 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T03:56:40.934+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:40 INFO Executor: Fetching spark://2f101bc15e7a:44235/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675137347603
[2023-01-31T03:56:43.410+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:43 INFO TransportClientFactory: Successfully created connection to 2f101bc15e7a/172.19.0.4:44235 after 947 ms (0 ms spent in bootstraps)
[2023-01-31T03:56:44.176+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:44 INFO Utils: Fetching spark://2f101bc15e7a:44235/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-bbbc06d7-dd62-4325-a992-8fdf1e979b8f/userFiles-368be3ba-b705-4caf-b6f0-714318af517e/fetchFileTemp6375306772838776118.tmp
[2023-01-31T03:56:54.926+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:54 INFO Executor: Adding file:/tmp/spark-bbbc06d7-dd62-4325-a992-8fdf1e979b8f/userFiles-368be3ba-b705-4caf-b6f0-714318af517e/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T03:56:54.927+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:54 INFO Executor: Fetching spark://2f101bc15e7a:44235/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675137347603
[2023-01-31T03:56:54.957+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:54 INFO Utils: Fetching spark://2f101bc15e7a:44235/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-bbbc06d7-dd62-4325-a992-8fdf1e979b8f/userFiles-368be3ba-b705-4caf-b6f0-714318af517e/fetchFileTemp10389180832665270720.tmp
[2023-01-31T03:56:59.791+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:59 INFO Executor: Adding file:/tmp/spark-bbbc06d7-dd62-4325-a992-8fdf1e979b8f/userFiles-368be3ba-b705-4caf-b6f0-714318af517e/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T03:57:00.047+0000] {spark_submit.py:495} INFO - 23/01/31 03:57:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37991.
[2023-01-31T03:57:00.048+0000] {spark_submit.py:495} INFO - 23/01/31 03:57:00 INFO NettyBlockTransferService: Server created on 2f101bc15e7a:37991
[2023-01-31T03:57:00.106+0000] {spark_submit.py:495} INFO - 23/01/31 03:57:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T03:57:00.229+0000] {spark_submit.py:495} INFO - 23/01/31 03:57:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 2f101bc15e7a, 37991, None)
[2023-01-31T03:57:00.871+0000] {spark_submit.py:495} INFO - 23/01/31 03:57:00 INFO BlockManagerMasterEndpoint: Registering block manager 2f101bc15e7a:37991 with 434.4 MiB RAM, BlockManagerId(driver, 2f101bc15e7a, 37991, None)
[2023-01-31T03:57:00.965+0000] {spark_submit.py:495} INFO - 23/01/31 03:57:00 INFO Executor: Told to re-register on heartbeat
[2023-01-31T03:57:01.014+0000] {spark_submit.py:495} INFO - 23/01/31 03:57:00 INFO BlockManager: BlockManager null re-registering with master
[2023-01-31T03:57:01.015+0000] {spark_submit.py:495} INFO - 23/01/31 03:57:01 INFO BlockManagerMaster: Registering BlockManager null
[2023-01-31T03:57:01.406+0000] {spark_submit.py:495} INFO - 23/01/31 03:57:01 ERROR Inbox: Ignoring error
[2023-01-31T03:57:01.407+0000] {spark_submit.py:495} INFO - java.lang.NullPointerException
[2023-01-31T03:57:01.408+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T03:57:01.409+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T03:57:01.550+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T03:57:01.551+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T03:57:01.551+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T03:57:01.552+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T03:57:01.552+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T03:57:01.553+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T03:57:01.623+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T03:57:01.624+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T03:57:01.624+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T03:57:01.625+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T03:57:01.625+0000] {spark_submit.py:495} INFO - 23/01/31 03:57:01 WARN Executor: Issue communicating with driver in heartbeater
[2023-01-31T03:57:01.626+0000] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-01-31T03:57:01.626+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-01-31T03:57:01.751+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-01-31T03:57:01.751+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-01-31T03:57:01.752+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-01-31T03:57:01.753+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2023-01-31T03:57:01.753+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-01-31T03:57:01.754+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-01-31T03:57:01.754+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-01-31T03:57:01.799+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
[2023-01-31T03:57:01.799+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-01-31T03:57:01.800+0000] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-01-31T03:57:01.801+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T03:57:01.801+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-01-31T03:57:01.802+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-01-31T03:57:01.802+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T03:57:01.879+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T03:57:01.880+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T03:57:01.880+0000] {spark_submit.py:495} INFO - Caused by: java.lang.NullPointerException
[2023-01-31T03:57:01.880+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T03:57:01.881+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T03:57:01.881+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T03:57:01.938+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T03:57:01.939+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T03:57:01.939+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T03:57:01.939+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T03:57:01.940+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T03:57:01.940+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T03:57:01.941+0000] {spark_submit.py:495} INFO - ... 3 more
[2023-01-31T03:57:01.941+0000] {spark_submit.py:495} INFO - 23/01/31 03:57:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 2f101bc15e7a, 37991, None)
[2023-01-31T03:57:01.941+0000] {spark_submit.py:495} INFO - 23/01/31 03:57:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 2f101bc15e7a, 37991, None)
[2023-01-31T03:57:30.158+0000] {spark_submit.py:495} INFO - 23/01/31 03:57:30 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T03:57:30.535+0000] {spark_submit.py:495} INFO - 23/01/31 03:57:30 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T03:58:44.059+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:44 INFO InMemoryFileIndex: It took 4266 ms to list leaf files for 1 paths.
[2023-01-31T03:58:48.201+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T03:58:49.833+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:49 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T03:58:49.945+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 2f101bc15e7a:37991 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T03:58:50.392+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:50 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T03:58:57.001+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:57 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T03:58:57.200+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:57 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T03:58:57.439+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:57 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T03:58:57.719+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:57 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T03:58:57.747+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:57 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T03:58:57.749+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:57 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T03:58:57.799+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:57 INFO DAGScheduler: Missing parents: List()
[2023-01-31T03:58:58.003+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:58 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T03:58:59.019+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:59 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T03:58:59.115+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:59 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T03:58:59.118+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:59 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 2f101bc15e7a:37991 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T03:58:59.148+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:59 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T03:58:59.759+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T03:58:59.782+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:59 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T03:59:00.646+0000] {spark_submit.py:495} INFO - 23/01/31 03:59:00 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (2f101bc15e7a, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T03:59:00.896+0000] {spark_submit.py:495} INFO - 23/01/31 03:59:00 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T03:59:02.329+0000] {spark_submit.py:495} INFO - 23/01/31 03:59:02 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010400__202101010500.json:0+9364
[2023-01-31T03:59:05.200+0000] {spark_submit.py:495} INFO - 23/01/31 03:59:05 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T03:59:05.265+0000] {spark_submit.py:495} INFO - 23/01/31 03:59:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4841 ms on 2f101bc15e7a (executor driver) (1/1)
[2023-01-31T03:59:05.276+0000] {spark_submit.py:495} INFO - 23/01/31 03:59:05 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T03:59:05.319+0000] {spark_submit.py:495} INFO - 23/01/31 03:59:05 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 7.080 s
[2023-01-31T03:59:05.347+0000] {spark_submit.py:495} INFO - 23/01/31 03:59:05 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T03:59:05.350+0000] {spark_submit.py:495} INFO - 23/01/31 03:59:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T03:59:05.370+0000] {spark_submit.py:495} INFO - 23/01/31 03:59:05 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 7.926000 s
[2023-01-31T03:59:30.776+0000] {spark_submit.py:495} INFO - 23/01/31 03:59:30 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 2f101bc15e7a:37991 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T03:59:30.896+0000] {spark_submit.py:495} INFO - 23/01/31 03:59:30 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 2f101bc15e7a:37991 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T04:00:22.484+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:22 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:00:22.623+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:22 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T04:00:22.791+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:22 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:00:33.459+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:33 INFO CodeGenerator: Code generated in 8721.045181 ms
[2023-01-31T04:00:34.148+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:34 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T04:00:34.774+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:34 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T04:00:34.802+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:34 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 2f101bc15e7a:37991 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T04:00:34.829+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:34 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T04:00:35.204+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:00:37.754+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:37 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T04:00:37.771+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:37 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T04:00:37.771+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:37 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T04:00:37.777+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:37 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:00:37.822+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:37 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:00:37.841+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:37 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T04:00:38.532+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:38 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T04:00:38.769+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:38 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T04:00:38.867+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:38 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 2f101bc15e7a:37991 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T04:00:38.924+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:38 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:00:39.063+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:00:39.065+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:39 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T04:00:39.430+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:39 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (2f101bc15e7a, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:00:39.504+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:39 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T04:00:45.303+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:45 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T04:00:48.927+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:48 INFO CodeGenerator: Code generated in 2724.75048 ms
[2023-01-31T04:00:51.662+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:51 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T04:00:51.818+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:51 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 12525 ms on 2f101bc15e7a (executor driver) (1/1)
[2023-01-31T04:00:51.836+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:51 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 13.689 s
[2023-01-31T04:00:51.839+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:51 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:00:51.886+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:51 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T04:00:51.896+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T04:00:51.915+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:51 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 14.158330 s
[2023-01-31T04:00:55.767+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:55 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:00:55.781+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:55 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T04:00:55.790+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:55 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:00:57.657+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:57 INFO CodeGenerator: Code generated in 1680.25637 ms
[2023-01-31T04:00:58.532+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:58 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 2f101bc15e7a:37991 in memory (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T04:00:58.784+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:58 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T04:00:59.750+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:59 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T04:00:59.809+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:59 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 2f101bc15e7a:37991 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:00:59.864+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:59 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T04:01:00.265+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:01:02.307+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:02 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T04:01:02.314+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:02 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T04:01:02.322+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:02 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T04:01:02.326+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:02 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:01:02.352+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:02 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:01:02.415+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:02 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T04:01:02.715+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:02 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T04:01:02.995+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:02 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T04:01:03.001+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:02 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 2f101bc15e7a:37991 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T04:01:03.057+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:03 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:01:03.104+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:01:03.139+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:03 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T04:01:03.156+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:03 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (2f101bc15e7a, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:01:03.210+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:03 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T04:01:03.779+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:03 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T04:01:08.857+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:08 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T04:01:08.976+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:08 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 5819 ms on 2f101bc15e7a (executor driver) (1/1)
[2023-01-31T04:01:08.978+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:08 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T04:01:09.143+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:08 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 6.505 s
[2023-01-31T04:01:09.181+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:09 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:01:09.288+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T04:01:09.319+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:09 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 7.007511 s
[2023-01-31T04:01:12.522+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:12 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:01:12.618+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:12 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 2f101bc15e7a:37991 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T04:01:12.750+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:12 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T04:01:12.751+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:12 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:01:16.869+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:16 INFO CodeGenerator: Code generated in 1895.47058 ms
[2023-01-31T04:01:16.870+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:16 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T04:01:17.517+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:17 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T04:01:17.521+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:17 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 2f101bc15e7a:37991 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:01:17.712+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:17 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T04:01:17.721+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:01:18.840+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:18 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T04:01:18.846+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:18 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T04:01:18.847+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:18 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T04:01:18.851+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:18 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:01:18.852+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:18 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:01:18.859+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:18 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T04:01:18.877+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:18 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T04:01:18.957+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:18 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-31T04:01:18.968+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:18 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 2f101bc15e7a:37991 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T04:01:19.013+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:19 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:01:19.103+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:01:19.120+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:19 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T04:01:19.195+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:19 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (2f101bc15e7a, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:01:19.202+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:19 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T04:01:20.782+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:20 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T04:01:22.478+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:22 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T04:01:22.558+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:22 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 3381 ms on 2f101bc15e7a (executor driver) (1/1)
[2023-01-31T04:01:22.610+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:22 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T04:01:22.618+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:22 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 3.743 s
[2023-01-31T04:01:22.630+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:22 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:01:22.632+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T04:01:22.715+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:22 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 3.817023 s
[2023-01-31T04:01:24.659+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:24 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:01:24.711+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:24 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T04:01:24.731+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:24 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:01:26.125+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:26 INFO CodeGenerator: Code generated in 1423.031196 ms
[2023-01-31T04:01:26.180+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:26 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T04:01:28.028+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:28 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T04:01:28.041+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:28 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 2f101bc15e7a:37991 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:01:28.057+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:28 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T04:01:28.176+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:01:29.560+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:29 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 2f101bc15e7a:37991 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T04:01:29.918+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:29 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T04:01:29.933+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:29 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T04:01:29.938+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:29 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T04:01:29.942+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:29 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:01:29.956+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:29 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:01:29.988+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:29 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T04:01:30.151+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:30 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.5 MiB)
[2023-01-31T04:01:30.196+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:30 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.5 MiB)
[2023-01-31T04:01:30.224+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:30 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 2f101bc15e7a:37991 (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T04:01:30.230+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:30 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:01:30.249+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:01:30.261+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:30 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T04:01:30.268+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:30 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (2f101bc15e7a, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:01:30.330+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:30 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T04:01:30.648+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:30 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T04:01:32.714+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:32 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1585 bytes result sent to driver
[2023-01-31T04:01:32.798+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:32 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 2518 ms on 2f101bc15e7a (executor driver) (1/1)
[2023-01-31T04:01:32.856+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:32 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T04:01:32.877+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:32 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 2.840 s
[2023-01-31T04:01:32.878+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:32 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:01:32.895+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T04:01:33.075+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:33 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 3.152062 s
[2023-01-31T04:01:38.245+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:38 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 2f101bc15e7a:37991 in memory (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T04:01:40.974+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:40 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:01:40.975+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:40 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T04:01:40.976+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:40 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:01:47.515+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:47 INFO CodeGenerator: Code generated in 6271.954649 ms
[2023-01-31T04:01:47.540+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:47 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T04:01:48.266+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:48 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T04:01:48.503+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:48 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 2f101bc15e7a:37991 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:01:48.515+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:48 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T04:01:48.702+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:01:49.376+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:49 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T04:01:49.380+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:49 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T04:01:49.381+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:49 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T04:01:49.381+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:49 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:01:49.382+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:49 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:01:49.431+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:49 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T04:01:49.934+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:49 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T04:01:50.622+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:50 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T04:01:50.628+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:50 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 2f101bc15e7a:37991 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T04:01:50.848+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:50 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:01:51.145+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:01:51.186+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:51 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T04:01:51.610+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:51 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (2f101bc15e7a, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:01:51.628+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:51 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T04:01:51.907+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:51 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T04:01:56.120+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:56 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2580 bytes result sent to driver
[2023-01-31T04:01:56.569+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:56 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 4958 ms on 2f101bc15e7a (executor driver) (1/1)
[2023-01-31T04:01:56.603+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:56 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 7.097 s
[2023-01-31T04:01:56.604+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:56 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T04:01:56.605+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:56 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:01:56.678+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T04:01:56.679+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:56 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 7.196620 s
[2023-01-31T04:01:59.094+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:59 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 2f101bc15e7a:37991 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T04:02:00.391+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:00 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:02:00.402+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:00 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T04:02:00.403+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:00 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:02:06.037+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:06 INFO CodeGenerator: Code generated in 3769.923966 ms
[2023-01-31T04:02:06.793+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:06 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.1 MiB)
[2023-01-31T04:02:08.217+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:08 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T04:02:09.352+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:09 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 2f101bc15e7a:37991 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:02:10.570+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:10 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T04:02:10.799+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:02:13.301+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:13 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T04:02:13.323+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:13 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T04:02:13.327+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:13 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T04:02:13.343+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:13 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:02:13.343+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:13 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:02:13.352+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:13 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T04:02:13.597+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:13 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 433.0 MiB)
[2023-01-31T04:02:14.955+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:14 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 433.0 MiB)
[2023-01-31T04:02:15.498+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:15 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 2f101bc15e7a:37991 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T04:02:15.499+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:15 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:02:15.527+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:02:15.561+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:15 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T04:02:16.024+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:16 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (2f101bc15e7a, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:02:16.048+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:16 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T04:02:16.693+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:16 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T04:02:20.803+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:20 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T04:02:20.844+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:20 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 4873 ms on 2f101bc15e7a (executor driver) (1/1)
[2023-01-31T04:02:20.883+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:20 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 7.457 s
[2023-01-31T04:02:20.916+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:20 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:02:20.940+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:20 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T04:02:21.024+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T04:02:21.026+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:21 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 7.695996 s
[2023-01-31T04:02:24.259+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:24 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:02:24.265+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:24 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T04:02:24.267+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:24 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:02:24.710+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:24 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 2f101bc15e7a:37991 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T04:02:25.057+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:25 INFO CodeGenerator: Code generated in 541.882311 ms
[2023-01-31T04:02:25.099+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:25 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.8 MiB)
[2023-01-31T04:02:25.243+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:25 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.8 MiB)
[2023-01-31T04:02:25.244+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:25 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 2f101bc15e7a:37991 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:02:25.290+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:25 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T04:02:25.338+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:02:26.228+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:26 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T04:02:26.255+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:26 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T04:02:26.256+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:26 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T04:02:26.257+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:26 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:02:26.257+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:26 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:02:26.407+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:26 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T04:02:26.474+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:26 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.8 MiB)
[2023-01-31T04:02:26.499+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:26 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.7 MiB)
[2023-01-31T04:02:26.510+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:26 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 2f101bc15e7a:37991 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T04:02:26.591+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:26 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:02:26.592+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:02:26.593+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:26 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T04:02:26.594+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:26 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (2f101bc15e7a, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:02:26.634+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:26 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T04:02:26.930+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:26 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T04:02:29.218+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:29 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1772 bytes result sent to driver
[2023-01-31T04:02:29.236+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:29 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 2645 ms on 2f101bc15e7a (executor driver) (1/1)
[2023-01-31T04:02:29.246+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:29 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 2.809 s
[2023-01-31T04:02:29.247+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:29 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:02:29.267+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:29 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T04:02:29.284+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T04:02:29.292+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:29 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 3.070666 s
[2023-01-31T04:02:37.338+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:37 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 2f101bc15e7a:37991 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:02:37.949+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:37 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 2f101bc15e7a:37991 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T04:02:38.138+0000] {spark_submit.py:495} INFO - 23/01/31 04:02:38 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 2f101bc15e7a:37991 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:05:12.830+0000] {base_job.py:232} ERROR - LocalTaskJob heartbeat got an exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/base_job.py", line 222, in heartbeat
    session.merge(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in merge
    return self._merge(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3131, in _merge
    merged = self.get(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2848, in get
    return self._get_impl(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2970, in _get_impl
    return db_load_fn(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/loading.py", line 530, in load_on_pk_identity
    session.execute(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1552, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-01-31T04:05:23.951+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:23 WARN Executor: Issue communicating with driver in heartbeater
[2023-01-31T04:05:24.012+0000] {spark_submit.py:495} INFO - org.apache.spark.rpc.RpcTimeoutException: Future timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval
[2023-01-31T04:05:24.012+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
[2023-01-31T04:05:24.013+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
[2023-01-31T04:05:24.013+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
[2023-01-31T04:05:24.013+0000] {spark_submit.py:495} INFO - at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)
[2023-01-31T04:05:24.014+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
[2023-01-31T04:05:24.014+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-01-31T04:05:24.048+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1053)
[2023-01-31T04:05:24.048+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-01-31T04:05:24.177+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
[2023-01-31T04:05:24.178+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-01-31T04:05:24.191+0000] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-01-31T04:05:24.191+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T04:05:24.194+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-01-31T04:05:24.194+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-01-31T04:05:24.548+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T04:05:24.585+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T04:05:24.586+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T04:05:24.616+0000] {spark_submit.py:495} INFO - Caused by: java.util.concurrent.TimeoutException: Future timed out after [10000 milliseconds]
[2023-01-31T04:05:24.616+0000] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryAwait0(Promise.scala:248)
[2023-01-31T04:05:24.633+0000] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:261)
[2023-01-31T04:05:24.633+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:293)
[2023-01-31T04:05:24.641+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-01-31T04:05:24.644+0000] {spark_submit.py:495} INFO - ... 12 more
[2023-01-31T04:05:24.668+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:24 WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false)
[2023-01-31T04:05:46.485+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:46 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,ArraySeq(),HashMap((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@2a1832a5)) by listener AppStatusListener took 2.578219055s.
[2023-01-31T04:06:05.650+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:05 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:06:05.760+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:06:05.764+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:06:05.792+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:06:05.793+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:06:05.793+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:06:05.807+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:06:06.822+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:06 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T04:06:06.826+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:06 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:06:06.827+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:06 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:06:06.828+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:06 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:06:06.830+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:06 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:06:06.831+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:06 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:06:07.172+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:07 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 433.0 MiB)
[2023-01-31T04:06:07.214+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:07 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.0 MiB)
[2023-01-31T04:06:07.221+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:07 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 2f101bc15e7a:37991 (size: 74.3 KiB, free: 434.2 MiB)
[2023-01-31T04:06:07.222+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:07 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:06:07.245+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:06:07.245+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:07 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T04:06:07.255+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:07 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (2f101bc15e7a, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T04:06:07.274+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:07 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T04:06:07.729+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:06:07.731+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:06:07.768+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:06:07.769+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:06:07.770+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:06:07.775+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:06:07.830+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:07 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T04:06:07.864+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:07 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T04:06:08.177+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:08 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T04:06:08.178+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:08 INFO ParquetOutputFormat: Validation is off
[2023-01-31T04:06:08.179+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:08 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T04:06:08.180+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:08 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T04:06:08.180+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T04:06:08.180+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T04:06:08.180+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T04:06:08.180+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T04:06:08.181+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T04:06:08.181+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T04:06:08.181+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T04:06:08.181+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T04:06:08.181+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T04:06:08.181+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T04:06:08.182+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T04:06:08.182+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T04:06:08.182+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T04:06:08.182+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T04:06:08.516+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:08 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T04:06:08.517+0000] {spark_submit.py:495} INFO - {
[2023-01-31T04:06:08.517+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T04:06:08.517+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T04:06:08.517+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T04:06:08.517+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T04:06:08.518+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T04:06:08.518+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:06:08.518+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:06:08.518+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T04:06:08.519+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T04:06:08.519+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T04:06:08.519+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:06:08.519+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:06:08.519+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T04:06:08.520+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:06:08.520+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:06:08.520+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:06:08.520+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:06:08.521+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T04:06:08.521+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:06:08.526+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:06:08.527+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:06:08.527+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:06:08.527+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T04:06:08.534+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:06:08.538+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:06:08.542+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:06:08.543+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:06:08.543+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T04:06:08.544+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:06:08.544+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:06:08.544+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:06:08.544+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:06:08.544+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T04:06:08.545+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:06:08.550+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:06:08.585+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:06:08.590+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:06:08.594+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T04:06:08.594+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:06:08.594+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:06:08.595+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:06:08.595+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:06:08.595+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T04:06:08.595+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:06:08.596+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:06:08.596+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:06:08.596+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:06:08.601+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T04:06:08.602+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:06:08.602+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:06:08.603+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:06:08.604+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:06:08.605+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T04:06:08.610+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:06:08.619+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:06:08.620+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:06:08.629+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:06:08.631+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T04:06:08.631+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:06:08.632+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:06:08.632+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:06:08.633+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:06:08.634+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T04:06:08.636+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:06:08.639+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:06:08.639+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:06:08.639+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:06:08.639+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T04:06:08.640+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:06:08.642+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:06:08.642+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:06:08.642+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:06:08.643+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T04:06:08.643+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:06:08.653+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:06:08.653+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:06:08.653+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:06:08.653+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T04:06:08.658+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:06:08.659+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:06:08.659+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:06:08.659+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:06:08.659+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T04:06:08.659+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:06:08.659+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:06:08.660+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:06:08.660+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:06:08.660+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T04:06:08.660+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:06:08.660+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:06:08.660+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:06:08.661+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:06:08.661+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T04:06:08.661+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:06:08.661+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:06:08.661+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:06:08.661+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T04:06:08.661+0000] {spark_submit.py:495} INFO - }
[2023-01-31T04:06:08.662+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T04:06:08.662+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T04:06:08.662+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T04:06:08.662+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T04:06:08.662+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T04:06:08.677+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T04:06:08.677+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T04:06:08.677+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T04:06:08.683+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T04:06:08.683+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T04:06:08.690+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T04:06:08.691+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T04:06:08.692+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T04:06:08.692+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T04:06:08.693+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T04:06:08.699+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T04:06:08.709+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T04:06:08.711+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T04:06:08.711+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T04:06:08.712+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T04:06:08.714+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T04:06:08.716+0000] {spark_submit.py:495} INFO - }
[2023-01-31T04:06:08.717+0000] {spark_submit.py:495} INFO - 
[2023-01-31T04:06:08.728+0000] {spark_submit.py:495} INFO - 
[2023-01-31T04:06:09.054+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:09 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T04:06:09.668+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:09 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 2f101bc15e7a:37991 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:06:09.775+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:09 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 2f101bc15e7a:37991 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:06:09.800+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:09 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 2f101bc15e7a:37991 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:06:09.808+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:09 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 2f101bc15e7a:37991 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:06:12.001+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:12 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675137398033-c15043d5-d6c8-4b7f-81e3-56f5a933b171/_temporary/0/_temporary/' directory.
[2023-01-31T04:06:12.001+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:12 INFO FileOutputCommitter: Saved output of task 'attempt_202301310406064965749705308060818_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675137398033-c15043d5-d6c8-4b7f-81e3-56f5a933b171/_temporary/0/task_202301310406064965749705308060818_0008_m_000000
[2023-01-31T04:06:12.003+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:12 INFO SparkHadoopMapRedUtil: attempt_202301310406064965749705308060818_0008_m_000000_8: Committed. Elapsed time: 956 ms.
[2023-01-31T04:06:12.023+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:12 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T04:06:12.032+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:12 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 4782 ms on 2f101bc15e7a (executor driver) (1/1)
[2023-01-31T04:06:12.032+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:12 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T04:06:12.033+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:12 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 5.204 s
[2023-01-31T04:06:12.033+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:12 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:06:12.033+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T04:06:12.036+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:12 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 5.210075 s
[2023-01-31T04:06:12.043+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:12 INFO FileFormatWriter: Start to commit write Job d2f6d6db-6d47-4c3c-939e-f4d3c73c3ada.
[2023-01-31T04:06:12.923+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:12 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675137398033-c15043d5-d6c8-4b7f-81e3-56f5a933b171/_temporary/0/task_202301310406064965749705308060818_0008_m_000000/' directory.
[2023-01-31T04:06:13.273+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:13 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675137398033-c15043d5-d6c8-4b7f-81e3-56f5a933b171/' directory.
[2023-01-31T04:06:13.717+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:13 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 2f101bc15e7a:37991 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T04:06:13.902+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:13 INFO FileFormatWriter: Write Job d2f6d6db-6d47-4c3c-939e-f4d3c73c3ada committed. Elapsed time: 1861 ms.
[2023-01-31T04:06:13.910+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:13 INFO FileFormatWriter: Finished processing stats for write job d2f6d6db-6d47-4c3c-939e-f4d3c73c3ada.
[2023-01-31T04:06:15.876+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:15 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675137398033-c15043d5-d6c8-4b7f-81e3-56f5a933b171/part-00000-1aa89c95-9599-430a-a563-bb6311adaa02-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=a6045963-307a-4893-889e-2f07addadfe2, location=US}
[2023-01-31T04:06:21.151+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:21 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=a6045963-307a-4893-889e-2f07addadfe2, location=US}
[2023-01-31T04:06:22.659+0000] {base_job.py:232} ERROR - LocalTaskJob heartbeat got an exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.19.0.3), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/base_job.py", line 204, in heartbeat
    session.merge(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in merge
    return self._merge(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3131, in _merge
    merged = self.get(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2848, in get
    return self._get_impl(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2970, in _get_impl
    return db_load_fn(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/loading.py", line 530, in load_on_pk_identity
    session.execute(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1552, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.19.0.3), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-01-31T04:06:23.840+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T04:06:24.746+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:24 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T04:06:24.822+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:24 INFO SparkUI: Stopped Spark web UI at http://2f101bc15e7a:4046
[2023-01-31T04:06:24.944+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:24 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T04:06:25.079+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:25 INFO MemoryStore: MemoryStore cleared
[2023-01-31T04:06:25.099+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:25 INFO BlockManager: BlockManager stopped
[2023-01-31T04:06:25.114+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:25 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T04:06:25.121+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:25 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T04:06:25.222+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:25 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T04:06:25.223+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:25 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T04:06:25.223+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-bbbc06d7-dd62-4325-a992-8fdf1e979b8f
[2023-01-31T04:06:25.251+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-bbbc06d7-dd62-4325-a992-8fdf1e979b8f/pyspark-8dc48e00-36d3-4ea0-b03d-f8213fcfecf5
[2023-01-31T04:06:25.299+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-29450faa-7d21-4a6b-b673-8cde31b4150a
[2023-01-31T04:06:27.136+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 1380, in _run_raw_task
    self.refresh_from_db(lock_for_update=True, session=session)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 791, in refresh_from_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.10/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.10/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.10/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 793, in refresh_from_db
    ti: TaskInstance | None = qry.with_for_update().one_or_none()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2849, in one_or_none
    return self._iter().one_or_none()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2915, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1552, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-01-31T04:06:27.151+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 818 for task stage_total_generation ((psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8); 2395)
[2023-01-31T04:06:27.198+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-31T04:06:27.201+0000] {taskinstance.py:2588} INFO - Skipping mini scheduling run due to exception: None
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 2553, in schedule_downstream_tasks
    ).one()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2869, in one
    return self._iter().one()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2915, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1552, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-01-31T04:24:15.484+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-31T04:24:15.526+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-31T04:24:15.526+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T04:24:15.527+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T04:24:15.527+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T04:24:15.571+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 04:00:00+00:00
[2023-01-31T04:24:15.595+0000] {standard_task_runner.py:55} INFO - Started process 257 to run task
[2023-01-31T04:24:15.618+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T04:00:00+00:00', '--job-id', '839', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpgxepqbky']
[2023-01-31T04:24:15.630+0000] {standard_task_runner.py:83} INFO - Job 839: Subtask stage_total_generation
[2023-01-31T04:24:15.878+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T04:24:16.090+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T04:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T04:00:00+00:00
[2023-01-31T04:24:16.118+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T04:24:16.121+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET
[2023-01-31T04:24:38.944+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:38 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T04:24:39.372+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T04:24:39.920+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:39 INFO ResourceUtils: ==============================================================
[2023-01-31T04:24:39.921+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:39 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T04:24:39.922+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:39 INFO ResourceUtils: ==============================================================
[2023-01-31T04:24:39.922+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:39 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T04:24:40.202+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:40 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T04:24:40.219+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:40 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T04:24:40.289+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:40 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T04:24:40.761+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:40 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T04:24:40.776+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:40 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T04:24:40.777+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:40 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T04:24:40.780+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:40 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T04:24:40.780+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T04:24:43.154+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:43 INFO Utils: Successfully started service 'sparkDriver' on port 46819.
[2023-01-31T04:24:43.530+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:43 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T04:24:44.200+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:44 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T04:24:44.727+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T04:24:44.746+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T04:24:44.821+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T04:24:45.252+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5be822bc-f7e8-4999-af41-797a8061063f
[2023-01-31T04:24:45.732+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:45 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T04:24:45.930+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:45 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T04:24:47.933+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:47 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-31T04:24:48.423+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:48 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:46819/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675139078890
[2023-01-31T04:24:48.432+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:48 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:46819/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675139078890
[2023-01-31T04:24:49.068+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:49 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T04:24:49.092+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:49 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T04:24:49.228+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:49 INFO Executor: Fetching spark://81d5fcd0285b:46819/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675139078890
[2023-01-31T04:24:49.761+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:49 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:46819 after 274 ms (0 ms spent in bootstraps)
[2023-01-31T04:24:49.852+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:49 INFO Utils: Fetching spark://81d5fcd0285b:46819/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-e5f66836-3e7b-4513-a744-42f510f5d5d7/userFiles-1efccac4-f0a7-4fdb-8737-241167057099/fetchFileTemp5757221050571813896.tmp
[2023-01-31T04:24:51.083+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:51 INFO Executor: Adding file:/tmp/spark-e5f66836-3e7b-4513-a744-42f510f5d5d7/userFiles-1efccac4-f0a7-4fdb-8737-241167057099/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T04:24:51.085+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:51 INFO Executor: Fetching spark://81d5fcd0285b:46819/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675139078890
[2023-01-31T04:24:51.095+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:51 INFO Utils: Fetching spark://81d5fcd0285b:46819/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-e5f66836-3e7b-4513-a744-42f510f5d5d7/userFiles-1efccac4-f0a7-4fdb-8737-241167057099/fetchFileTemp16870067803658023917.tmp
[2023-01-31T04:24:51.856+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:51 INFO Executor: Adding file:/tmp/spark-e5f66836-3e7b-4513-a744-42f510f5d5d7/userFiles-1efccac4-f0a7-4fdb-8737-241167057099/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T04:24:51.889+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41145.
[2023-01-31T04:24:51.890+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:51 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:41145
[2023-01-31T04:24:51.896+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T04:24:51.928+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 41145, None)
[2023-01-31T04:24:51.959+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:51 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:41145 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 41145, None)
[2023-01-31T04:24:51.974+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 41145, None)
[2023-01-31T04:24:51.978+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 41145, None)
[2023-01-31T04:24:56.524+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T04:24:56.588+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:56 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T04:25:11.742+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:11 INFO InMemoryFileIndex: It took 381 ms to list leaf files for 1 paths.
[2023-01-31T04:25:12.888+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:12 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T04:25:13.225+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T04:25:13.231+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:41145 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T04:25:13.244+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:13 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T04:25:14.593+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:14 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T04:25:14.652+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:14 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T04:25:14.709+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:14 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T04:25:14.771+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:14 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T04:25:14.773+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:14 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T04:25:14.776+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:14 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:25:14.785+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:14 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:25:14.812+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:14 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T04:25:15.044+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T04:25:15.051+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T04:25:15.053+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:41145 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T04:25:15.061+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:15 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:25:15.098+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:25:15.104+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T04:25:15.359+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T04:25:15.440+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:15 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T04:25:15.946+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:15 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010400__202101010500.json:0+9364
[2023-01-31T04:25:17.168+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:17 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T04:25:17.253+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1961 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:25:17.263+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:17 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T04:25:17.317+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:17 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 2.412 s
[2023-01-31T04:25:17.340+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:17 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:25:17.362+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T04:25:17.369+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:17 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 2.654528 s
[2023-01-31T04:25:19.699+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:19 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:41145 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T04:25:19.782+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:19 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:41145 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T04:25:28.532+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:28 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:25:28.536+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:28 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T04:25:28.540+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:28 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:25:29.891+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:29 INFO CodeGenerator: Code generated in 655.816613 ms
[2023-01-31T04:25:29.927+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:29 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T04:25:29.988+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:29 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T04:25:29.991+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:29 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:41145 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T04:25:29.996+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:29 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T04:25:30.030+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:25:30.222+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:30 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T04:25:30.228+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:30 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T04:25:30.229+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:30 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T04:25:30.229+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:25:30.230+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:25:30.230+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:30 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T04:25:30.255+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:30 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T04:25:30.272+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:30 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T04:25:30.273+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:30 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:41145 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T04:25:30.278+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:30 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:25:30.284+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:25:30.285+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:30 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T04:25:30.300+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:30 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:25:30.314+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:30 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T04:25:30.641+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:30 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T04:25:31.121+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO CodeGenerator: Code generated in 348.62046 ms
[2023-01-31T04:25:31.528+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T04:25:31.543+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1246 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:25:31.545+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T04:25:31.548+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 1.305 s
[2023-01-31T04:25:31.549+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:25:31.549+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T04:25:31.549+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 1.322690 s
[2023-01-31T04:25:31.711+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:25:31.711+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T04:25:31.712+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:25:31.771+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO CodeGenerator: Code generated in 38.205491 ms
[2023-01-31T04:25:31.785+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T04:25:31.819+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T04:25:31.822+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:41145 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:25:31.823+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T04:25:31.829+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:25:31.864+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T04:25:31.865+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T04:25:31.865+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T04:25:31.865+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:25:31.865+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:25:31.873+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T04:25:31.873+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T04:25:31.904+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T04:25:31.909+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:41145 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T04:25:31.911+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:25:31.914+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:25:31.915+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T04:25:31.934+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:25:31.934+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T04:25:31.964+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T04:25:32.149+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T04:25:32.159+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 226 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:25:32.160+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T04:25:32.162+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.292 s
[2023-01-31T04:25:32.163+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:25:32.163+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T04:25:32.163+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.299123 s
[2023-01-31T04:25:32.598+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:25:32.612+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T04:25:32.613+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:25:33.107+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO CodeGenerator: Code generated in 323.359639 ms
[2023-01-31T04:25:33.138+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T04:25:33.210+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T04:25:33.212+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:41145 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:25:33.216+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T04:25:33.226+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:25:33.346+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T04:25:33.348+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T04:25:33.349+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T04:25:33.349+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:25:33.351+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:25:33.352+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T04:25:33.362+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-01-31T04:25:33.389+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T04:25:33.396+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:41145 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T04:25:33.398+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:25:33.400+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:25:33.402+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T04:25:33.403+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:25:33.409+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T04:25:33.463+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T04:25:33.718+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T04:25:33.772+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 370 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:25:33.776+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T04:25:33.777+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.422 s
[2023-01-31T04:25:33.777+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:25:33.777+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T04:25:33.777+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.430532 s
[2023-01-31T04:25:33.856+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:25:33.857+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T04:25:33.857+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:25:33.960+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO CodeGenerator: Code generated in 64.380417 ms
[2023-01-31T04:25:34.028+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T04:25:34.109+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T04:25:34.117+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:41145 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:25:34.117+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T04:25:34.132+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:25:34.405+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T04:25:34.406+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:41145 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T04:25:34.406+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T04:25:34.406+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T04:25:34.406+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:25:34.406+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:25:34.413+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T04:25:34.413+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T04:25:34.431+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T04:25:34.433+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:41145 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T04:25:34.437+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:25:34.437+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:25:34.438+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T04:25:34.438+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:25:34.439+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T04:25:34.489+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T04:25:34.500+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:41145 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T04:25:34.567+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:41145 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T04:25:34.691+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T04:25:34.693+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 256 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:25:34.693+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T04:25:34.699+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.286 s
[2023-01-31T04:25:34.699+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:25:34.699+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T04:25:34.699+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.293939 s
[2023-01-31T04:25:35.653+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:35 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:25:35.658+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:35 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T04:25:35.660+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:35 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:25:36.214+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO CodeGenerator: Code generated in 401.416353 ms
[2023-01-31T04:25:36.248+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T04:25:36.344+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T04:25:36.356+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:41145 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:25:36.360+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T04:25:36.363+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:25:36.518+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T04:25:36.520+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T04:25:36.521+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T04:25:36.521+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:25:36.521+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:25:36.530+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T04:25:36.534+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T04:25:36.537+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T04:25:36.539+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:41145 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T04:25:36.541+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:25:36.542+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:25:36.542+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T04:25:36.553+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:25:36.560+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T04:25:36.583+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T04:25:36.711+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2537 bytes result sent to driver
[2023-01-31T04:25:36.716+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 161 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:25:36.716+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T04:25:36.718+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.191 s
[2023-01-31T04:25:36.719+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:25:36.719+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T04:25:36.721+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.200485 s
[2023-01-31T04:25:36.935+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:25:36.940+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T04:25:36.941+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:25:37.405+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO CodeGenerator: Code generated in 346.09876 ms
[2023-01-31T04:25:37.442+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T04:25:37.553+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-01-31T04:25:37.560+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:41145 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:25:37.570+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T04:25:37.583+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:25:37.815+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T04:25:37.829+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T04:25:37.829+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T04:25:37.829+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:25:37.830+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:25:37.838+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T04:25:37.842+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T04:25:37.850+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T04:25:37.859+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:41145 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T04:25:37.860+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:25:37.872+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:25:37.873+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T04:25:37.878+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:25:37.879+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T04:25:37.926+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T04:25:38.087+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T04:25:38.095+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 217 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:25:38.098+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T04:25:38.099+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.262 s
[2023-01-31T04:25:38.099+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:25:38.099+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T04:25:38.100+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.273474 s
[2023-01-31T04:25:38.252+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:25:38.254+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T04:25:38.254+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:25:38.483+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO CodeGenerator: Code generated in 137.289473 ms
[2023-01-31T04:25:38.490+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T04:25:38.510+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T04:25:38.510+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:41145 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T04:25:38.510+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T04:25:38.513+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:25:38.556+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T04:25:38.557+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T04:25:38.558+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T04:25:38.559+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:25:38.560+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:25:38.565+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T04:25:38.569+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.6 MiB)
[2023-01-31T04:25:38.569+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 432.6 MiB)
[2023-01-31T04:25:38.570+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:41145 (size: 12.7 KiB, free: 434.1 MiB)
[2023-01-31T04:25:38.574+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:25:38.575+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:25:38.575+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T04:25:38.575+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:25:38.575+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T04:25:38.606+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T04:25:38.779+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T04:25:38.781+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 207 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:25:38.781+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T04:25:38.787+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.220 s
[2023-01-31T04:25:38.787+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:25:38.787+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T04:25:38.788+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.228770 s
[2023-01-31T04:25:40.345+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:40 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:41145 in memory (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T04:25:40.417+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:40 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:41145 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:25:40.487+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:40 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:41145 in memory (size: 12.7 KiB, free: 434.2 MiB)
[2023-01-31T04:25:40.549+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:40 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:41145 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T04:25:40.623+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:40 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:41145 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:25:40.764+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:40 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:41145 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T04:25:40.832+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:40 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:41145 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:25:40.913+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:40 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:41145 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:25:41.012+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:41 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:41145 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:25:43.639+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:43 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:41145 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T04:25:49.536+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:49 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:25:49.576+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:25:49.578+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:25:49.610+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:25:49.610+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:25:49.610+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:25:49.618+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:25:50.476+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T04:25:50.477+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:25:50.477+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:25:50.477+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:25:50.477+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:25:50.477+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:25:50.584+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-31T04:25:50.589+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-31T04:25:50.590+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:41145 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T04:25:50.592+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:25:50.592+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:25:50.593+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T04:25:50.598+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T04:25:50.604+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T04:25:50.832+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:25:50.833+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:25:50.842+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:25:50.843+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:25:50.846+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:25:50.846+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:25:50.892+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T04:25:50.892+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T04:25:51.068+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:51 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T04:25:51.069+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:51 INFO ParquetOutputFormat: Validation is off
[2023-01-31T04:25:51.069+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:51 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T04:25:51.069+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:51 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T04:25:51.069+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T04:25:51.070+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T04:25:51.070+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T04:25:51.070+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T04:25:51.070+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T04:25:51.071+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T04:25:51.071+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T04:25:51.071+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T04:25:51.071+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T04:25:51.071+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T04:25:51.072+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T04:25:51.072+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T04:25:51.072+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T04:25:51.072+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T04:25:51.305+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T04:25:51.306+0000] {spark_submit.py:495} INFO - {
[2023-01-31T04:25:51.306+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T04:25:51.306+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T04:25:51.307+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T04:25:51.307+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T04:25:51.307+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T04:25:51.307+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:51.307+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:51.308+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T04:25:51.308+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T04:25:51.308+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T04:25:51.308+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:51.308+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:51.309+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T04:25:51.309+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:51.309+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:51.309+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:51.309+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:51.310+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T04:25:51.310+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:51.310+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:51.310+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:51.310+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:51.311+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T04:25:51.311+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:51.311+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:51.311+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:51.316+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:51.317+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T04:25:51.317+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:51.317+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:51.318+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:51.318+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:51.318+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T04:25:51.318+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:51.318+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:51.318+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:51.319+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:51.319+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T04:25:51.319+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:51.319+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:51.320+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:51.320+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:51.320+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T04:25:51.320+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:51.320+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:51.321+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:51.321+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:51.321+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T04:25:51.321+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:51.322+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:51.322+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:51.322+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:51.322+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T04:25:51.323+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:51.324+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:51.324+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:51.324+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:51.325+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T04:25:51.326+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:51.326+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:51.326+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:51.327+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:51.328+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T04:25:51.328+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:51.328+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:51.328+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:51.330+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:51.330+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T04:25:51.330+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:51.331+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:51.331+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:51.332+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:51.332+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T04:25:51.333+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:51.333+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:51.336+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:51.337+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:51.337+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T04:25:51.337+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:51.337+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:51.338+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:51.338+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:51.338+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T04:25:51.338+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:51.338+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:51.339+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:51.339+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:51.339+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T04:25:51.339+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:51.339+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:51.340+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:51.340+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:51.340+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T04:25:51.340+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:51.346+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:51.346+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:51.346+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T04:25:51.346+0000] {spark_submit.py:495} INFO - }
[2023-01-31T04:25:51.347+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T04:25:51.347+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T04:25:51.347+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T04:25:51.347+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T04:25:51.347+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T04:25:51.348+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T04:25:51.348+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T04:25:51.348+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T04:25:51.348+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T04:25:51.349+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T04:25:51.349+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T04:25:51.349+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T04:25:51.349+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T04:25:51.349+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T04:25:51.350+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T04:25:51.350+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T04:25:51.350+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T04:25:51.350+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T04:25:51.351+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T04:25:51.351+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T04:25:51.351+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T04:25:51.351+0000] {spark_submit.py:495} INFO - }
[2023-01-31T04:25:51.351+0000] {spark_submit.py:495} INFO - 
[2023-01-31T04:25:51.358+0000] {spark_submit.py:495} INFO - 
[2023-01-31T04:25:51.837+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:51 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T04:25:55.923+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:55 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675139088815-133e3a8d-5735-4252-a984-3d9f16da517e/_temporary/0/_temporary/' directory.
[2023-01-31T04:25:55.945+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:55 INFO FileOutputCommitter: Saved output of task 'attempt_202301310425508553113619504324284_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675139088815-133e3a8d-5735-4252-a984-3d9f16da517e/_temporary/0/task_202301310425508553113619504324284_0008_m_000000
[2023-01-31T04:25:55.966+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:55 INFO SparkHadoopMapRedUtil: attempt_202301310425508553113619504324284_0008_m_000000_8: Committed. Elapsed time: 1157 ms.
[2023-01-31T04:25:56.016+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:56 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T04:25:56.020+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:56 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 5426 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:25:56.021+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:56 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T04:25:56.022+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:56 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 5.546 s
[2023-01-31T04:25:56.023+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:56 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:25:56.023+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T04:25:56.025+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:56 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 5.552715 s
[2023-01-31T04:25:56.027+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:56 INFO FileFormatWriter: Start to commit write Job 79a0b02d-e298-453c-a07a-d0356108fe8f.
[2023-01-31T04:25:56.929+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:56 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675139088815-133e3a8d-5735-4252-a984-3d9f16da517e/_temporary/0/task_202301310425508553113619504324284_0008_m_000000/' directory.
[2023-01-31T04:25:57.289+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:57 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675139088815-133e3a8d-5735-4252-a984-3d9f16da517e/' directory.
[2023-01-31T04:25:57.496+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:57 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:41145 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T04:25:58.010+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:58 INFO FileFormatWriter: Write Job 79a0b02d-e298-453c-a07a-d0356108fe8f committed. Elapsed time: 1980 ms.
[2023-01-31T04:25:58.017+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:58 INFO FileFormatWriter: Finished processing stats for write job 79a0b02d-e298-453c-a07a-d0356108fe8f.
[2023-01-31T04:25:59.357+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:59 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675139088815-133e3a8d-5735-4252-a984-3d9f16da517e/part-00000-dc05fdf9-30e3-4bc7-b624-cc0543a744b8-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=d128c5da-4064-4bca-8a44-ec7b45e199cf, location=US}
[2023-01-31T04:26:03.028+0000] {spark_submit.py:495} INFO - 23/01/31 04:26:03 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=d128c5da-4064-4bca-8a44-ec7b45e199cf, location=US}
[2023-01-31T04:26:03.425+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T04:26:03.554+0000] {spark_submit.py:495} INFO - 23/01/31 04:26:03 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T04:26:03.577+0000] {spark_submit.py:495} INFO - 23/01/31 04:26:03 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4040
[2023-01-31T04:26:03.601+0000] {spark_submit.py:495} INFO - 23/01/31 04:26:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T04:26:03.621+0000] {spark_submit.py:495} INFO - 23/01/31 04:26:03 INFO MemoryStore: MemoryStore cleared
[2023-01-31T04:26:03.622+0000] {spark_submit.py:495} INFO - 23/01/31 04:26:03 INFO BlockManager: BlockManager stopped
[2023-01-31T04:26:03.626+0000] {spark_submit.py:495} INFO - 23/01/31 04:26:03 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T04:26:03.629+0000] {spark_submit.py:495} INFO - 23/01/31 04:26:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T04:26:03.637+0000] {spark_submit.py:495} INFO - 23/01/31 04:26:03 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T04:26:03.637+0000] {spark_submit.py:495} INFO - 23/01/31 04:26:03 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T04:26:03.637+0000] {spark_submit.py:495} INFO - 23/01/31 04:26:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-e5f66836-3e7b-4513-a744-42f510f5d5d7
[2023-01-31T04:26:03.643+0000] {spark_submit.py:495} INFO - 23/01/31 04:26:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-e5f66836-3e7b-4513-a744-42f510f5d5d7/pyspark-fad78d58-f283-4aa0-886d-a6fa437d7011
[2023-01-31T04:26:03.648+0000] {spark_submit.py:495} INFO - 23/01/31 04:26:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-b2315fec-a02e-4867-8b15-5d7c6633e950
[2023-01-31T04:26:03.776+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata, task_id=stage_total_generation, execution_date=20210101T040000, start_date=20230131T042415, end_date=20230131T042603
[2023-01-31T04:26:03.831+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T04:26:03.847+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T04:30:53.467+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-31T04:30:53.508+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-31T04:30:53.509+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T04:30:53.509+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T04:30:53.510+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T04:30:53.580+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 04:00:00+00:00
[2023-01-31T04:30:53.605+0000] {standard_task_runner.py:55} INFO - Started process 2701 to run task
[2023-01-31T04:30:53.629+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T04:00:00+00:00', '--job-id', '856', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpwgvjeq9f']
[2023-01-31T04:30:53.639+0000] {standard_task_runner.py:83} INFO - Job 856: Subtask stage_total_generation
[2023-01-31T04:30:53.951+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T04:30:54.316+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T04:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T04:00:00+00:00
[2023-01-31T04:30:54.373+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T04:30:54.376+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET
[2023-01-31T04:31:27.215+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:27 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T04:31:28.236+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T04:31:29.070+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:29 INFO ResourceUtils: ==============================================================
[2023-01-31T04:31:29.076+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:29 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T04:31:29.077+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:29 INFO ResourceUtils: ==============================================================
[2023-01-31T04:31:29.095+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:29 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T04:31:29.377+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:29 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T04:31:29.442+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:29 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T04:31:29.467+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:29 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T04:31:30.024+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:30 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T04:31:30.025+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:30 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T04:31:30.030+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:30 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T04:31:30.030+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:30 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T04:31:30.031+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T04:31:32.487+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:32 INFO Utils: Successfully started service 'sparkDriver' on port 46785.
[2023-01-31T04:31:33.068+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:33 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T04:31:33.380+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:33 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T04:31:33.593+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:33 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T04:31:33.596+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:33 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T04:31:33.645+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:33 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T04:31:33.765+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c801d58a-a3d6-49bf-b72b-de7c72f949a3
[2023-01-31T04:31:33.848+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:33 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T04:31:33.973+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:33 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T04:31:35.917+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T04:31:36.040+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:36 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-31T04:31:36.235+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:36 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:46785/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675139487107
[2023-01-31T04:31:36.242+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:36 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:46785/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675139487107
[2023-01-31T04:31:36.733+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:36 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T04:31:36.796+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:36 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T04:31:36.863+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:36 INFO Executor: Fetching spark://81d5fcd0285b:46785/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675139487107
[2023-01-31T04:31:37.333+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:37 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:46785 after 262 ms (0 ms spent in bootstraps)
[2023-01-31T04:31:37.389+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:37 INFO Utils: Fetching spark://81d5fcd0285b:46785/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-6fba33a3-aa11-4a53-92a7-90fd4f7400ba/userFiles-ad68b4ae-6698-4229-ae6a-eacd05b9d9d8/fetchFileTemp17285563602755870281.tmp
[2023-01-31T04:31:38.915+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:38 INFO Executor: Adding file:/tmp/spark-6fba33a3-aa11-4a53-92a7-90fd4f7400ba/userFiles-ad68b4ae-6698-4229-ae6a-eacd05b9d9d8/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T04:31:38.918+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:38 INFO Executor: Fetching spark://81d5fcd0285b:46785/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675139487107
[2023-01-31T04:31:38.920+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:38 INFO Utils: Fetching spark://81d5fcd0285b:46785/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-6fba33a3-aa11-4a53-92a7-90fd4f7400ba/userFiles-ad68b4ae-6698-4229-ae6a-eacd05b9d9d8/fetchFileTemp12822224977740956975.tmp
[2023-01-31T04:31:39.816+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:39 INFO Executor: Adding file:/tmp/spark-6fba33a3-aa11-4a53-92a7-90fd4f7400ba/userFiles-ad68b4ae-6698-4229-ae6a-eacd05b9d9d8/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T04:31:39.877+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35387.
[2023-01-31T04:31:39.878+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:39 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:35387
[2023-01-31T04:31:39.890+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:39 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T04:31:39.949+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 35387, None)
[2023-01-31T04:31:39.988+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:39 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:35387 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 35387, None)
[2023-01-31T04:31:40.002+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 35387, None)
[2023-01-31T04:31:40.020+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:40 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 35387, None)
[2023-01-31T04:31:44.336+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:44 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T04:31:44.390+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:44 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T04:32:03.228+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:03 INFO InMemoryFileIndex: It took 951 ms to list leaf files for 1 paths.
[2023-01-31T04:32:04.069+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T04:32:05.245+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T04:32:05.257+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:35387 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T04:32:05.304+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:05 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T04:32:10.256+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:10 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T04:32:10.837+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:10 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T04:32:10.942+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:10 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T04:32:11.049+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:11 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T04:32:11.065+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:11 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T04:32:11.069+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:11 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:32:11.113+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:11 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:32:11.239+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:11 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T04:32:12.126+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:12 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T04:32:12.176+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:12 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T04:32:12.186+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:12 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:35387 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T04:32:12.200+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:12 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:32:12.443+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:32:12.445+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:12 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T04:32:12.980+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:12 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T04:32:13.241+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:13 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T04:32:14.276+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:14 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010400__202101010500.json:0+9364
[2023-01-31T04:32:16.348+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:16 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T04:32:16.671+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3713 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:32:16.704+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:16 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T04:32:16.743+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:16 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 5.172 s
[2023-01-31T04:32:16.840+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:16 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:32:16.913+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T04:32:16.991+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:16 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 6.032487 s
[2023-01-31T04:32:25.947+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:25 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:35387 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T04:32:26.041+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:26 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:35387 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T04:32:48.509+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:48 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:32:48.527+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:48 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T04:32:48.546+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:48 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:32:50.657+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:50 INFO CodeGenerator: Code generated in 714.950536 ms
[2023-01-31T04:32:50.682+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:50 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T04:32:50.726+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:50 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T04:32:50.732+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:50 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:35387 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T04:32:50.743+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:50 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T04:32:50.781+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:32:51.323+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:51 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T04:32:51.343+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:51 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T04:32:51.344+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:51 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T04:32:51.344+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:51 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:32:51.344+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:51 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:32:51.348+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:51 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T04:32:51.519+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:51 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T04:32:51.548+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:51 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T04:32:51.560+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:51 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:35387 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T04:32:51.563+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:51 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:32:51.564+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:32:51.565+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:51 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T04:32:51.583+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:51 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:32:51.585+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:51 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T04:32:51.829+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:51 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T04:32:52.351+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:52 INFO CodeGenerator: Code generated in 433.670007 ms
[2023-01-31T04:32:53.623+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:53 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T04:32:53.731+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:53 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2122 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:32:53.777+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:53 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T04:32:53.782+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:53 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 2.406 s
[2023-01-31T04:32:53.784+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:53 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:32:53.785+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T04:32:53.790+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:53 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 2.447521 s
[2023-01-31T04:32:54.778+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:54 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:32:54.786+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:54 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T04:32:54.799+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:54 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:32:55.067+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:55 INFO CodeGenerator: Code generated in 178.329924 ms
[2023-01-31T04:32:55.175+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:55 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T04:32:55.518+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:55 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T04:32:55.539+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:55 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:35387 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:32:55.545+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:55 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T04:32:55.592+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:32:56.062+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T04:32:56.065+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T04:32:56.065+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T04:32:56.067+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:32:56.067+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:32:56.080+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T04:32:56.107+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T04:32:56.164+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T04:32:56.172+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:35387 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T04:32:56.177+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:32:56.177+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:32:56.178+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T04:32:56.195+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:32:56.196+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T04:32:56.327+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T04:32:56.741+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T04:32:56.787+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 594 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:32:56.788+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T04:32:56.848+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.753 s
[2023-01-31T04:32:56.849+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:32:56.849+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T04:32:56.849+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.762213 s
[2023-01-31T04:32:57.925+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:57 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:32:57.965+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:57 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T04:32:57.966+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:57 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:32:59.277+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:59 INFO CodeGenerator: Code generated in 1040.654181 ms
[2023-01-31T04:32:59.325+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:59 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T04:32:59.831+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:59 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T04:32:59.832+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:59 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:35387 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:32:59.861+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:59 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T04:32:59.922+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:33:00.836+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:00 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T04:33:00.842+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:00 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T04:33:00.844+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:00 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T04:33:00.857+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:00 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:33:00.859+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:00 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:33:00.860+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:00 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T04:33:01.007+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:00 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-01-31T04:33:01.052+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:01 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T04:33:01.077+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:01 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:35387 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T04:33:01.079+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:01 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:33:01.119+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:33:01.130+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:01 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T04:33:01.135+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:01 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:33:01.145+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:01 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T04:33:01.598+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:01 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T04:33:02.347+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:02 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T04:33:02.385+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:02 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1251 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:33:02.386+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:02 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T04:33:02.403+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:02 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 1.496 s
[2023-01-31T04:33:02.403+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:02 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:33:02.404+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T04:33:02.413+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:02 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 1.568178 s
[2023-01-31T04:33:02.833+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:02 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:33:02.841+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:02 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T04:33:02.847+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:02 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:33:03.445+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:03 INFO CodeGenerator: Code generated in 346.758603 ms
[2023-01-31T04:33:03.896+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:03 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T04:33:05.067+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:05 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T04:33:05.070+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:05 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:35387 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:33:05.076+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:05 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T04:33:05.218+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:33:06.043+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T04:33:06.049+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T04:33:06.050+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T04:33:06.055+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:33:06.069+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:33:06.078+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T04:33:06.158+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T04:33:06.181+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T04:33:06.184+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:35387 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T04:33:06.211+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:33:06.223+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:33:06.233+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T04:33:06.244+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:33:06.250+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T04:33:06.330+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T04:33:06.759+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T04:33:06.791+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 551 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:33:06.810+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.701 s
[2023-01-31T04:33:06.812+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:33:06.828+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T04:33:06.829+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T04:33:06.829+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.781330 s
[2023-01-31T04:33:08.644+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:08 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:33:08.646+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:08 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T04:33:08.647+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:08 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:33:09.144+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:09 INFO CodeGenerator: Code generated in 258.396882 ms
[2023-01-31T04:33:09.153+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:09 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.2 MiB)
[2023-01-31T04:33:09.155+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:09 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.1 MiB)
[2023-01-31T04:33:09.168+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:09 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:35387 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:33:09.169+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:09 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T04:33:09.169+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:33:09.212+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:09 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T04:33:09.217+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:09 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T04:33:09.219+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:09 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T04:33:09.222+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:09 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:33:09.223+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:09 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:33:09.231+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:09 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T04:33:09.248+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:09 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.1 MiB)
[2023-01-31T04:33:09.252+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:09 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.1 MiB)
[2023-01-31T04:33:09.258+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:09 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:35387 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T04:33:09.266+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:09 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:33:09.273+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:33:09.273+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:09 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T04:33:09.289+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:09 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:33:09.297+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:09 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T04:33:09.324+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:09 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T04:33:09.620+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:09 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2537 bytes result sent to driver
[2023-01-31T04:33:09.630+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:09 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 346 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:33:09.631+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:09 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T04:33:09.637+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:09 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.402 s
[2023-01-31T04:33:09.638+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:09 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:33:09.639+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T04:33:09.640+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:09 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.426006 s
[2023-01-31T04:33:10.505+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:10 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:33:10.518+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:10 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T04:33:10.545+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:10 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:33:13.550+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:13 INFO CodeGenerator: Code generated in 2043.176585 ms
[2023-01-31T04:33:13.725+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:13 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 432.9 MiB)
[2023-01-31T04:33:14.478+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-01-31T04:33:14.478+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:35387 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:33:14.513+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T04:33:14.518+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:33:14.542+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:35387 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T04:33:14.682+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:35387 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T04:33:14.689+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T04:33:14.690+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T04:33:14.690+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T04:33:14.690+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:33:14.690+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:33:14.785+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T04:33:14.803+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T04:33:14.807+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T04:33:14.824+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:35387 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T04:33:14.830+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:33:14.837+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:33:14.842+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T04:33:14.849+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:33:14.849+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T04:33:14.892+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:35387 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T04:33:15.007+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:15 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T04:33:15.108+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:15 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:35387 in memory (size: 12.7 KiB, free: 434.2 MiB)
[2023-01-31T04:33:15.533+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:15 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:35387 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T04:33:16.072+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:16 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T04:33:16.082+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:16 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 1236 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:33:16.095+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:16 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 1.287 s
[2023-01-31T04:33:16.096+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:16 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T04:33:16.097+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:16 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:33:16.098+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T04:33:16.099+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:16 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 1.405395 s
[2023-01-31T04:33:18.019+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:18 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:33:18.026+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:18 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T04:33:18.027+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:18 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:33:20.032+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:20 INFO CodeGenerator: Code generated in 1262.178759 ms
[2023-01-31T04:33:20.082+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:20 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.8 MiB)
[2023-01-31T04:33:20.280+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:20 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T04:33:20.286+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:20 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:35387 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:33:20.297+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:20 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T04:33:20.408+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:33:20.817+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:20 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T04:33:20.821+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:20 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T04:33:20.826+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:20 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T04:33:20.826+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:20 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:33:20.826+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:20 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:33:20.830+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:20 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T04:33:20.860+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:20 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T04:33:20.863+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:20 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.7 MiB)
[2023-01-31T04:33:20.870+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:20 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:35387 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T04:33:20.904+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:20 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:33:20.932+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:33:20.959+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:20 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T04:33:20.970+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:20 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:33:20.970+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:20 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T04:33:21.133+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:21 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T04:33:23.029+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:23 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T04:33:23.117+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:23 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 2156 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:33:23.138+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:23 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 2.293 s
[2023-01-31T04:33:23.139+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:23 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:33:23.212+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:23 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T04:33:23.228+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T04:33:23.229+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:23 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 2.396192 s
[2023-01-31T04:33:29.642+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:29 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:35387 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:33:29.807+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:29 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:35387 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:33:30.031+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:30 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:35387 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:33:30.119+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:30 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:35387 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:33:30.250+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:30 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:35387 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T04:33:30.422+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:30 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:35387 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:33:30.519+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:30 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:35387 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T04:33:30.724+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:30 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:35387 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T04:34:08.474+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:08 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:34:08.591+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:34:08.593+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:34:08.600+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:34:08.611+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:34:08.614+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:34:08.624+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:34:11.395+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:11 INFO CodeGenerator: Code generated in 86.725195 ms
[2023-01-31T04:34:11.790+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:11 INFO DAGScheduler: Registering RDD 118 (save at BigQueryWriteHelper.java:105) as input to shuffle 0
[2023-01-31T04:34:11.861+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:11 INFO DAGScheduler: Got map stage job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:11.863+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:11 INFO DAGScheduler: Final stage: ShuffleMapStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:11.870+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:11 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:11.874+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:11 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:11.939+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:11 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:12.218+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:12 INFO CodeGenerator: Code generated in 265.057568 ms
[2023-01-31T04:34:12.544+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:12 INFO CodeGenerator: Code generated in 198.186425 ms
[2023-01-31T04:34:12.592+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:12 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 11.1 KiB, free 434.2 MiB)
[2023-01-31T04:34:12.603+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:12 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 434.2 MiB)
[2023-01-31T04:34:12.606+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:12 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:35387 (size: 5.7 KiB, free: 434.4 MiB)
[2023-01-31T04:34:12.607+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:12 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:12.621+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:12.622+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:12 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T04:34:12.628+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:12 INFO DAGScheduler: Registering RDD 120 (save at BigQueryWriteHelper.java:105) as input to shuffle 1
[2023-01-31T04:34:12.628+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:12 INFO DAGScheduler: Got map stage job 9 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:12.628+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:12 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:12.633+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:12 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:12.654+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:12 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:12.654+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:12 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[120] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:12.671+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:12 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7660 bytes) taskResourceAssignments Map()
[2023-01-31T04:34:12.674+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:12 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T04:34:12.711+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:12 INFO CodeGenerator: Code generated in 115.92874 ms
[2023-01-31T04:34:13.172+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:13 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 15.6 KiB, free 434.1 MiB)
[2023-01-31T04:34:13.176+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:13 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 434.1 MiB)
[2023-01-31T04:34:13.183+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:13 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 81d5fcd0285b:35387 (size: 8.1 KiB, free: 434.4 MiB)
[2023-01-31T04:34:13.205+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:13 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:13.206+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[120] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:13.207+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:13 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2023-01-31T04:34:13.224+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:13 INFO DAGScheduler: Registering RDD 122 (save at BigQueryWriteHelper.java:105) as input to shuffle 2
[2023-01-31T04:34:13.229+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:13 INFO DAGScheduler: Got map stage job 10 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:13.242+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:13 INFO DAGScheduler: Final stage: ShuffleMapStage 10 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:13.243+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:13 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:13.277+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:13 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:13.360+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:13 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[122] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:13.466+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:13 INFO CodeGenerator: Code generated in 415.340262 ms
[2023-01-31T04:34:13.720+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:13 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 15.7 KiB, free 434.1 MiB)
[2023-01-31T04:34:13.744+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:13 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 434.1 MiB)
[2023-01-31T04:34:13.746+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:13 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 81d5fcd0285b:35387 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:34:13.748+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:13 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:13.783+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:13 INFO CodeGenerator: Code generated in 551.037841 ms
[2023-01-31T04:34:13.806+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[122] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:13.808+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:13 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2023-01-31T04:34:13.820+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:13 INFO CodeGenerator: Code generated in 313.563507 ms
[2023-01-31T04:34:13.896+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:13 INFO DAGScheduler: Registering RDD 124 (save at BigQueryWriteHelper.java:105) as input to shuffle 3
[2023-01-31T04:34:13.905+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:13 INFO DAGScheduler: Got map stage job 11 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:13.906+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:13 INFO DAGScheduler: Final stage: ShuffleMapStage 11 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:13.908+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:13 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:13.980+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:13 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:14.039+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[124] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:14.106+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 15.7 KiB, free 434.1 MiB)
[2023-01-31T04:34:14.145+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 434.1 MiB)
[2023-01-31T04:34:14.148+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 81d5fcd0285b:35387 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:34:14.161+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:14.216+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO CodeGenerator: Code generated in 154.431583 ms
[2023-01-31T04:34:14.231+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[124] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:14.232+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2023-01-31T04:34:14.249+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Registering RDD 126 (save at BigQueryWriteHelper.java:105) as input to shuffle 4
[2023-01-31T04:34:14.249+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Got map stage job 12 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:14.249+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:14.249+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:14.249+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:14.263+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[126] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:14.309+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 15.7 KiB, free 434.1 MiB)
[2023-01-31T04:34:14.340+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 434.1 MiB)
[2023-01-31T04:34:14.341+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO CodeGenerator: Code generated in 98.026494 ms
[2023-01-31T04:34:14.361+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 81d5fcd0285b:35387 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:34:14.365+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:14.366+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[126] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:14.366+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2023-01-31T04:34:14.366+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Registering RDD 128 (save at BigQueryWriteHelper.java:105) as input to shuffle 5
[2023-01-31T04:34:14.366+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Got map stage job 13 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:14.376+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Final stage: ShuffleMapStage 13 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:14.378+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:14.379+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:14.384+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[128] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:14.418+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 15.7 KiB, free 434.0 MiB)
[2023-01-31T04:34:14.418+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 434.0 MiB)
[2023-01-31T04:34:14.420+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 81d5fcd0285b:35387 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:34:14.424+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO CodeGenerator: Code generated in 56.124973 ms
[2023-01-31T04:34:14.432+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:14.433+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[128] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:14.433+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2023-01-31T04:34:14.444+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Registering RDD 130 (save at BigQueryWriteHelper.java:105) as input to shuffle 6
[2023-01-31T04:34:14.445+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Got map stage job 14 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:14.445+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Final stage: ShuffleMapStage 14 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:14.445+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:14.445+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:14.465+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[130] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:14.492+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 15.7 KiB, free 434.0 MiB)
[2023-01-31T04:34:14.493+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 434.0 MiB)
[2023-01-31T04:34:14.497+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 81d5fcd0285b:35387 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:34:14.528+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:14.531+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[130] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:14.546+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
[2023-01-31T04:34:14.554+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Registering RDD 132 (save at BigQueryWriteHelper.java:105) as input to shuffle 7
[2023-01-31T04:34:14.558+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Got map stage job 15 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:14.570+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Final stage: ShuffleMapStage 15 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:14.582+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:14.582+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO CodeGenerator: Code generated in 54.029043 ms
[2023-01-31T04:34:14.621+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:14.663+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[132] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:14.857+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO CodeGenerator: Code generated in 63.331775 ms
[2023-01-31T04:34:14.869+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:14 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 15.7 KiB, free 434.0 MiB)
[2023-01-31T04:34:15.025+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 434.0 MiB)
[2023-01-31T04:34:15.025+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2130 bytes result sent to driver
[2023-01-31T04:34:15.025+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 81d5fcd0285b:35387 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:34:15.029+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:34:15.029+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
[2023-01-31T04:34:15.071+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 2444 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:34:15.071+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T04:34:15.101+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:15.102+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[132] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:15.121+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
[2023-01-31T04:34:15.150+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO DAGScheduler: Registering RDD 134 (save at BigQueryWriteHelper.java:105) as input to shuffle 8
[2023-01-31T04:34:15.150+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO DAGScheduler: Got map stage job 16 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:15.150+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO DAGScheduler: Final stage: ShuffleMapStage 16 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:15.151+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:15.151+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:15.178+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[134] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:15.453+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 15.7 KiB, free 434.0 MiB)
[2023-01-31T04:34:15.454+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 434.0 MiB)
[2023-01-31T04:34:15.469+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 81d5fcd0285b:35387 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:34:15.484+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:15.486+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[134] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:15.487+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
[2023-01-31T04:34:15.566+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO DAGScheduler: Registering RDD 136 (save at BigQueryWriteHelper.java:105) as input to shuffle 9
[2023-01-31T04:34:15.573+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO CodeGenerator: Code generated in 253.489977 ms
[2023-01-31T04:34:15.575+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO DAGScheduler: Got map stage job 17 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:15.575+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO DAGScheduler: Final stage: ShuffleMapStage 17 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:15.575+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:15.576+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:15.598+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO DAGScheduler: Submitting ShuffleMapStage 17 (MapPartitionsRDD[136] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:15.719+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 15.7 KiB, free 434.0 MiB)
[2023-01-31T04:34:15.793+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.9 MiB)
[2023-01-31T04:34:15.797+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 81d5fcd0285b:35387 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:34:15.809+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:15 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:16.045+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[136] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:16.047+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:16 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
[2023-01-31T04:34:16.123+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:16 INFO DAGScheduler: ShuffleMapStage 8 (save at BigQueryWriteHelper.java:105) finished in 4.041 s
[2023-01-31T04:34:16.147+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:16 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:34:16.148+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:16 INFO CodeGenerator: Code generated in 188.309953 ms
[2023-01-31T04:34:16.155+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:16 INFO DAGScheduler: running: HashSet(ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 9, ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15)
[2023-01-31T04:34:16.181+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:16 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:34:16.201+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:16 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:34:16.780+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:16 INFO DAGScheduler: Registering RDD 138 (save at BigQueryWriteHelper.java:105) as input to shuffle 10
[2023-01-31T04:34:16.786+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:16 INFO DAGScheduler: Got map stage job 18 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:16.803+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:16 INFO DAGScheduler: Final stage: ShuffleMapStage 18 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:16.803+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:16 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:16.805+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:16 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:16.842+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:16 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[138] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:16.948+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:16 INFO CodeGenerator: Code generated in 155.24831 ms
[2023-01-31T04:34:17.011+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:17 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 15.7 KiB, free 433.9 MiB)
[2023-01-31T04:34:17.047+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:17 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.9 MiB)
[2023-01-31T04:34:17.050+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:17 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 81d5fcd0285b:35387 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:34:17.094+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:17 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:17.118+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[138] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:17.130+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:17 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
[2023-01-31T04:34:17.182+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:17 INFO DAGScheduler: Registering RDD 140 (save at BigQueryWriteHelper.java:105) as input to shuffle 11
[2023-01-31T04:34:17.197+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:17 INFO DAGScheduler: Got map stage job 19 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:17.198+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:17 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:17.199+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:17 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:17.200+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:17 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:17.289+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:17 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[140] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:17.464+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:17 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 15.7 KiB, free 433.9 MiB)
[2023-01-31T04:34:17.560+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:17 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.9 MiB)
[2023-01-31T04:34:17.562+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:17 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 81d5fcd0285b:35387 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:34:17.563+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:17 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:17.564+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[140] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:17.593+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:17 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
[2023-01-31T04:34:17.699+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:17 INFO DAGScheduler: Registering RDD 142 (save at BigQueryWriteHelper.java:105) as input to shuffle 12
[2023-01-31T04:34:17.699+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:17 INFO DAGScheduler: Got map stage job 20 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:17.732+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:17 INFO DAGScheduler: Final stage: ShuffleMapStage 20 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:17.732+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:17 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:17.733+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:17 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:17.737+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:17 INFO CodeGenerator: Code generated in 598.556084 ms
[2023-01-31T04:34:17.793+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:17 INFO DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[142] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:17.967+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:17 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 15.7 KiB, free 433.9 MiB)
[2023-01-31T04:34:17.979+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:17 INFO CodeGenerator: Code generated in 19.413237 ms
[2023-01-31T04:34:18.025+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:18 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.9 MiB)
[2023-01-31T04:34:18.026+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:18 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 81d5fcd0285b:35387 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:34:18.029+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:18 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:18.102+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:18 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[142] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:18.106+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:18 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
[2023-01-31T04:34:18.179+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:18 INFO DAGScheduler: Registering RDD 144 (save at BigQueryWriteHelper.java:105) as input to shuffle 13
[2023-01-31T04:34:18.179+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:18 INFO DAGScheduler: Got map stage job 21 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:18.180+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:18 INFO DAGScheduler: Final stage: ShuffleMapStage 21 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:18.182+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:18 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:18.183+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:18 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:18.261+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:18 INFO DAGScheduler: Submitting ShuffleMapStage 21 (MapPartitionsRDD[144] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:18.329+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:18 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 15.7 KiB, free 433.9 MiB)
[2023-01-31T04:34:18.429+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:18 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.9 MiB)
[2023-01-31T04:34:18.434+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:18 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 81d5fcd0285b:35387 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:34:18.460+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:18 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:18.772+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:18 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[144] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:18.773+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:18 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
[2023-01-31T04:34:18.841+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:18 INFO DAGScheduler: Registering RDD 146 (save at BigQueryWriteHelper.java:105) as input to shuffle 14
[2023-01-31T04:34:18.843+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:18 INFO DAGScheduler: Got map stage job 22 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:18.844+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:18 INFO DAGScheduler: Final stage: ShuffleMapStage 22 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:18.885+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:18 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:18.886+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:18 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:18.886+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:18 INFO CodeGenerator: Code generated in 593.594052 ms
[2023-01-31T04:34:18.908+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:18 INFO DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[146] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:19.381+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:19 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 15.7 KiB, free 433.8 MiB)
[2023-01-31T04:34:19.889+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:19 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.8 MiB)
[2023-01-31T04:34:19.893+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:19 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 81d5fcd0285b:35387 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:34:19.893+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:19 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:19.894+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:19 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[146] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:19.894+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:19 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
[2023-01-31T04:34:19.894+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:19 INFO DAGScheduler: Registering RDD 148 (save at BigQueryWriteHelper.java:105) as input to shuffle 15
[2023-01-31T04:34:19.894+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:19 INFO DAGScheduler: Got map stage job 23 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:19.894+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:19 INFO DAGScheduler: Final stage: ShuffleMapStage 23 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:19.894+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:19 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:19.895+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:19 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:19.945+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:19 INFO DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[148] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:20.036+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:20 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 15.7 KiB, free 433.8 MiB)
[2023-01-31T04:34:20.049+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:20 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.8 MiB)
[2023-01-31T04:34:20.050+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:20 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 81d5fcd0285b:35387 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:34:20.051+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:20 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:20.058+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:20 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[148] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:20.059+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:20 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
[2023-01-31T04:34:20.059+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:20 INFO DAGScheduler: Registering RDD 150 (save at BigQueryWriteHelper.java:105) as input to shuffle 16
[2023-01-31T04:34:20.182+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:20 INFO DAGScheduler: Got map stage job 24 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:20.209+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:20 INFO DAGScheduler: Final stage: ShuffleMapStage 24 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:20.238+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:20 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:20.241+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:20 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:20.376+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:20 INFO DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[150] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:20.584+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:20 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 15.7 KiB, free 433.8 MiB)
[2023-01-31T04:34:20.601+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:20 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.8 MiB)
[2023-01-31T04:34:20.621+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:20 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 81d5fcd0285b:35387 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:34:20.668+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:20 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:20.669+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:20 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[150] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:20.670+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:20 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
[2023-01-31T04:34:21.690+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:21 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:35387 in memory (size: 5.7 KiB, free: 434.2 MiB)
[2023-01-31T04:34:22.539+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:22 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2023-01-31T04:34:25.512+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:25 INFO CodeGenerator: Code generated in 714.551318 ms
[2023-01-31T04:34:26.643+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:26 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2023-01-31T04:34:26.649+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:26 INFO DAGScheduler: Got job 25 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2023-01-31T04:34:26.649+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:26 INFO DAGScheduler: Final stage: ResultStage 26 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2023-01-31T04:34:26.650+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)
[2023-01-31T04:34:26.650+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:26 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:26.664+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:26 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[154] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2023-01-31T04:34:27.250+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 21.6 KiB, free 433.8 MiB)
[2023-01-31T04:34:27.251+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 433.8 MiB)
[2023-01-31T04:34:27.251+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 81d5fcd0285b:35387 (size: 10.2 KiB, free: 434.2 MiB)
[2023-01-31T04:34:27.251+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:27.251+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[154] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:27.252+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
[2023-01-31T04:34:32.097+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO CodeGenerator: Code generated in 150.686776 ms
[2023-01-31T04:34:32.848+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO PythonRunner: Times: total = 16670, boot = 15715, init = 954, finish = 1
[2023-01-31T04:34:32.861+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 2521 bytes result sent to driver
[2023-01-31T04:34:32.874+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:34:32.885+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
[2023-01-31T04:34:32.899+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 17856 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:34:32.900+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2023-01-31T04:34:32.900+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 56675
[2023-01-31T04:34:32.902+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO DAGScheduler: ShuffleMapStage 9 (save at BigQueryWriteHelper.java:105) finished in 20.258 s
[2023-01-31T04:34:32.904+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:34:32.905+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO DAGScheduler: running: HashSet(ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:34:32.908+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:34:32.927+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:34:36.388+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:36 INFO PythonRunner: Times: total = 3084, boot = -99, init = 3183, finish = 0
[2023-01-31T04:34:36.613+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:36 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 2478 bytes result sent to driver
[2023-01-31T04:34:36.631+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:36 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:34:36.691+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:36 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 3775 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:34:36.694+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:36 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2023-01-31T04:34:36.715+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:36 INFO DAGScheduler: ShuffleMapStage 10 (save at BigQueryWriteHelper.java:105) finished in 23.284 s
[2023-01-31T04:34:36.751+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:36 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:34:36.753+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:36 INFO DAGScheduler: running: HashSet(ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:34:36.754+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:36 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:34:36.755+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:36 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:34:36.791+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:36 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
[2023-01-31T04:34:37.858+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:37 INFO PythonRunner: Times: total = 690, boot = -798, init = 1488, finish = 0
[2023-01-31T04:34:37.917+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:37 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 2521 bytes result sent to driver
[2023-01-31T04:34:37.938+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:37 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:34:37.943+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:37 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
[2023-01-31T04:34:37.953+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:37 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 1326 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:34:37.953+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:37 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2023-01-31T04:34:37.953+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:37 INFO DAGScheduler: ShuffleMapStage 11 (save at BigQueryWriteHelper.java:105) finished in 23.914 s
[2023-01-31T04:34:37.953+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:37 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:34:37.954+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:37 INFO DAGScheduler: running: HashSet(ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:34:37.954+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:37 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:34:37.954+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:37 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:34:38.280+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:38 INFO PythonRunner: Times: total = 288, boot = -447, init = 735, finish = 0
[2023-01-31T04:34:38.288+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:38 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 2478 bytes result sent to driver
[2023-01-31T04:34:38.293+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:38 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:34:38.306+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:38 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 379 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:34:38.307+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:38 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2023-01-31T04:34:38.307+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:38 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)
[2023-01-31T04:34:38.307+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:38 INFO DAGScheduler: ShuffleMapStage 12 (save at BigQueryWriteHelper.java:105) finished in 24.031 s
[2023-01-31T04:34:38.357+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:38 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:34:38.357+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:38 INFO DAGScheduler: running: HashSet(ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:34:38.357+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:38 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:34:38.358+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:38 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:34:40.000+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:39 INFO PythonRunner: Times: total = 1254, boot = -96, init = 1293, finish = 57
[2023-01-31T04:34:40.268+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:40 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 2478 bytes result sent to driver
[2023-01-31T04:34:40.281+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:40 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:34:40.288+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:40 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 1994 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:34:40.288+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:40 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2023-01-31T04:34:40.288+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:40 INFO DAGScheduler: ShuffleMapStage 13 (save at BigQueryWriteHelper.java:105) finished in 25.902 s
[2023-01-31T04:34:40.288+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:40 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:34:40.288+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:40 INFO DAGScheduler: running: HashSet(ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:34:40.288+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:40 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:34:40.289+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:40 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:34:40.345+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:40 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)
[2023-01-31T04:34:43.003+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:42 INFO PythonRunner: Times: total = 2516, boot = -161, init = 2677, finish = 0
[2023-01-31T04:34:43.031+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:43 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 2478 bytes result sent to driver
[2023-01-31T04:34:43.048+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:43 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:34:43.049+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:43 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)
[2023-01-31T04:34:43.053+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:43 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 2777 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:34:43.055+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:43 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
[2023-01-31T04:34:43.064+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:43 INFO DAGScheduler: ShuffleMapStage 14 (save at BigQueryWriteHelper.java:105) finished in 28.596 s
[2023-01-31T04:34:43.065+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:43 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:34:43.066+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:43 INFO DAGScheduler: running: HashSet(ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:34:43.067+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:43 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:34:43.068+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:43 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:34:43.700+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:43 INFO PythonRunner: Times: total = 605, boot = -30, init = 635, finish = 0
[2023-01-31T04:34:43.720+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:43 INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 2478 bytes result sent to driver
[2023-01-31T04:34:43.721+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:43 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:34:43.753+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:43 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 686 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:34:43.755+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:43 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2023-01-31T04:34:43.770+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:43 INFO DAGScheduler: ShuffleMapStage 15 (save at BigQueryWriteHelper.java:105) finished in 29.062 s
[2023-01-31T04:34:43.771+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:43 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:34:43.772+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:43 INFO DAGScheduler: running: HashSet(ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:34:43.799+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:43 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:34:43.819+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:43 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:34:43.895+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:43 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)
[2023-01-31T04:34:44.614+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:44 INFO PythonRunner: Times: total = 534, boot = -339, init = 873, finish = 0
[2023-01-31T04:34:44.731+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:44 INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 2521 bytes result sent to driver
[2023-01-31T04:34:44.743+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:44 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:34:44.745+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:44 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 1017 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:34:44.746+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:44 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool
[2023-01-31T04:34:44.747+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:44 INFO Executor: Running task 0.0 in stage 17.0 (TID 17)
[2023-01-31T04:34:44.758+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:44 INFO DAGScheduler: ShuffleMapStage 16 (save at BigQueryWriteHelper.java:105) finished in 29.562 s
[2023-01-31T04:34:44.769+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:44 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:34:44.770+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:44 INFO DAGScheduler: running: HashSet(ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:34:44.770+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:44 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:34:44.770+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:44 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:34:47.728+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:47 INFO PythonRunner: Times: total = 2570, boot = -286, init = 2856, finish = 0
[2023-01-31T04:34:47.810+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:47 INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 2478 bytes result sent to driver
[2023-01-31T04:34:47.851+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:47 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:34:47.851+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:47 INFO Executor: Running task 0.0 in stage 18.0 (TID 18)
[2023-01-31T04:34:47.851+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:47 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 3097 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:34:47.852+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:47 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
[2023-01-31T04:34:47.906+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:47 INFO DAGScheduler: ShuffleMapStage 17 (save at BigQueryWriteHelper.java:105) finished in 32.282 s
[2023-01-31T04:34:47.907+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:47 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:34:47.907+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:47 INFO DAGScheduler: running: HashSet(ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:34:47.908+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:47 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:34:47.918+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:47 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:34:49.709+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:49 INFO PythonRunner: Times: total = 1559, boot = -320, init = 1879, finish = 0
[2023-01-31T04:34:49.805+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:49 INFO Executor: Finished task 0.0 in stage 18.0 (TID 18). 2478 bytes result sent to driver
[2023-01-31T04:34:49.816+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:49 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:34:49.818+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:49 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 1988 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:34:49.819+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:49 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
[2023-01-31T04:34:49.835+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:49 INFO DAGScheduler: ShuffleMapStage 18 (save at BigQueryWriteHelper.java:105) finished in 32.987 s
[2023-01-31T04:34:49.835+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:49 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:34:49.835+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:49 INFO DAGScheduler: running: HashSet(ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:34:49.836+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:49 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:34:49.836+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:49 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:34:49.836+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:49 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)
[2023-01-31T04:34:50.520+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:50 INFO PythonRunner: Times: total = 582, boot = -76, init = 658, finish = 0
[2023-01-31T04:34:50.577+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:50 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 2478 bytes result sent to driver
[2023-01-31T04:34:50.597+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:50 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:34:50.608+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:50 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 788 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:34:50.611+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:50 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
[2023-01-31T04:34:50.611+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:50 INFO DAGScheduler: ShuffleMapStage 19 (save at BigQueryWriteHelper.java:105) finished in 33.310 s
[2023-01-31T04:34:50.611+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:50 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:34:50.611+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:50 INFO DAGScheduler: running: HashSet(ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:34:50.612+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:50 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:34:50.616+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:50 INFO Executor: Running task 0.0 in stage 20.0 (TID 20)
[2023-01-31T04:34:50.619+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:50 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:34:51.758+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:51 INFO PythonRunner: Times: total = 1035, boot = 75, init = 960, finish = 0
[2023-01-31T04:34:51.777+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:51 INFO Executor: Finished task 0.0 in stage 20.0 (TID 20). 2478 bytes result sent to driver
[2023-01-31T04:34:51.783+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:51 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:34:51.795+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:51 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 1202 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:34:51.797+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:51 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool
[2023-01-31T04:34:51.798+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:51 INFO DAGScheduler: ShuffleMapStage 20 (save at BigQueryWriteHelper.java:105) finished in 33.979 s
[2023-01-31T04:34:51.799+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:51 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:34:51.800+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:51 INFO DAGScheduler: running: HashSet(ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:34:51.810+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:51 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:34:51.847+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:51 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:34:51.857+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:51 INFO Executor: Running task 0.0 in stage 21.0 (TID 21)
[2023-01-31T04:34:53.659+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:53 INFO PythonRunner: Times: total = 1631, boot = -137, init = 1768, finish = 0
[2023-01-31T04:34:53.807+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:53 INFO Executor: Finished task 0.0 in stage 21.0 (TID 21). 2521 bytes result sent to driver
[2023-01-31T04:34:53.817+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:53 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:34:53.823+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:53 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 21) in 2042 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:34:53.824+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:53 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
[2023-01-31T04:34:53.824+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:53 INFO Executor: Running task 0.0 in stage 22.0 (TID 22)
[2023-01-31T04:34:53.824+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:53 INFO DAGScheduler: ShuffleMapStage 21 (save at BigQueryWriteHelper.java:105) finished in 35.564 s
[2023-01-31T04:34:53.824+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:53 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:34:53.824+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:53 INFO DAGScheduler: running: HashSet(ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:34:53.824+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:53 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:34:53.824+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:53 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:34:56.331+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:56 INFO PythonRunner: Times: total = 2489, boot = 1, init = 2488, finish = 0
[2023-01-31T04:34:56.364+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:56 INFO Executor: Finished task 0.0 in stage 22.0 (TID 22). 2478 bytes result sent to driver
[2023-01-31T04:34:56.367+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:56 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 23) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:34:56.367+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:56 INFO Executor: Running task 0.0 in stage 23.0 (TID 23)
[2023-01-31T04:34:56.372+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:56 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 2561 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:34:56.374+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:56 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool
[2023-01-31T04:34:56.399+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:56 INFO DAGScheduler: ShuffleMapStage 22 (save at BigQueryWriteHelper.java:105) finished in 37.449 s
[2023-01-31T04:34:56.400+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:56 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:34:56.400+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:56 INFO DAGScheduler: running: HashSet(ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:34:56.400+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:56 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:34:56.400+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:56 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:35:00.779+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:00 INFO PythonRunner: Times: total = 3997, boot = -22, init = 4012, finish = 7
[2023-01-31T04:35:00.882+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:00 INFO Executor: Finished task 0.0 in stage 23.0 (TID 23). 2478 bytes result sent to driver
[2023-01-31T04:35:00.892+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:00 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 24) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:35:00.903+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:00 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 23) in 4532 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:35:00.916+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:00 INFO Executor: Running task 0.0 in stage 24.0 (TID 24)
[2023-01-31T04:35:00.917+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:00 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool
[2023-01-31T04:35:00.943+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:00 INFO DAGScheduler: ShuffleMapStage 23 (save at BigQueryWriteHelper.java:105) finished in 40.939 s
[2023-01-31T04:35:00.944+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:00 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:35:00.946+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:00 INFO DAGScheduler: running: HashSet(ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:35:00.969+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:00 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:35:01.032+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:00 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:35:03.749+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:03 INFO PythonRunner: Times: total = 2570, boot = 11, init = 2559, finish = 0
[2023-01-31T04:35:03.839+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:03 INFO Executor: Finished task 0.0 in stage 24.0 (TID 24). 2478 bytes result sent to driver
[2023-01-31T04:35:03.846+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:03 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 25) (81d5fcd0285b, executor driver, partition 0, NODE_LOCAL, 7399 bytes) taskResourceAssignments Map()
[2023-01-31T04:35:03.865+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:03 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 24) in 2978 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:35:03.889+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:03 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool
[2023-01-31T04:35:03.889+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:03 INFO DAGScheduler: ShuffleMapStage 24 (save at BigQueryWriteHelper.java:105) finished in 43.479 s
[2023-01-31T04:35:03.892+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:03 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:35:03.899+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:03 INFO DAGScheduler: running: HashSet(ResultStage 26)
[2023-01-31T04:35:03.945+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:03 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:35:03.946+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:03 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:35:03.947+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:03 INFO Executor: Running task 0.0 in stage 26.0 (TID 25)
[2023-01-31T04:35:05.551+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:05 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 81d5fcd0285b:35387 in memory (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:35:05.681+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:05 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 81d5fcd0285b:35387 in memory (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:35:05.694+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:05 INFO ShuffleBlockFetcherIterator: Getting 1 (97.0 B) non-empty blocks including 1 (97.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:35:05.706+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 208 ms
[2023-01-31T04:35:05.778+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:05 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 81d5fcd0285b:35387 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:35:05.945+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:05 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 81d5fcd0285b:35387 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:35:06.152+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:06 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 81d5fcd0285b:35387 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:35:06.557+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:06 INFO CodeGenerator: Code generated in 751.055385 ms
[2023-01-31T04:35:06.975+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:06 INFO CodeGenerator: Code generated in 208.679436 ms
[2023-01-31T04:35:07.590+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:07 INFO CodeGenerator: Code generated in 192.408663 ms
[2023-01-31T04:35:07.804+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:07 INFO CodeGenerator: Code generated in 200.129064 ms
[2023-01-31T04:35:08.888+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:08 INFO CodeGenerator: Code generated in 200.078491 ms
[2023-01-31T04:35:09.312+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:09 INFO CodeGenerator: Code generated in 206.687673 ms
[2023-01-31T04:35:09.361+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:09 INFO CodeGenerator: Code generated in 70.554005 ms
[2023-01-31T04:35:09.428+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:09 INFO Executor: Finished task 0.0 in stage 26.0 (TID 25). 3217 bytes result sent to driver
[2023-01-31T04:35:09.440+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:09 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 25) in 5598 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:35:09.440+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:09 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool
[2023-01-31T04:35:09.449+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:09 INFO DAGScheduler: ResultStage 26 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 42.278 s
[2023-01-31T04:35:09.469+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:09 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:35:09.471+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
[2023-01-31T04:35:09.477+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:09 INFO DAGScheduler: Job 25 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 42.828718 s
[2023-01-31T04:35:09.890+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:09 INFO CodeGenerator: Code generated in 231.621133 ms
[2023-01-31T04:35:09.927+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:09 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 1024.1 KiB, free 432.9 MiB)
[2023-01-31T04:35:09.954+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:09 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 278.0 B, free 432.9 MiB)
[2023-01-31T04:35:09.959+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:09 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 81d5fcd0285b:35387 (size: 278.0 B, free: 434.3 MiB)
[2023-01-31T04:35:10.013+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:09 INFO SparkContext: Created broadcast 34 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2023-01-31T04:35:12.835+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:12 INFO CodeGenerator: Code generated in 557.46745 ms
[2023-01-31T04:35:13.017+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:13 INFO DAGScheduler: Registering RDD 156 (save at BigQueryWriteHelper.java:105) as input to shuffle 17
[2023-01-31T04:35:13.018+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:13 INFO DAGScheduler: Got map stage job 26 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:35:13.020+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:13 INFO DAGScheduler: Final stage: ShuffleMapStage 27 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:35:13.021+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:13 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:35:13.023+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:13 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:35:13.075+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:13 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[156] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:35:13.450+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:13 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 18.1 KiB, free 432.9 MiB)
[2023-01-31T04:35:13.453+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:13 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 432.9 MiB)
[2023-01-31T04:35:13.458+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:13 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 81d5fcd0285b:35387 (size: 9.2 KiB, free: 434.3 MiB)
[2023-01-31T04:35:13.492+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:13 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:35:13.498+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[156] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:35:13.513+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:13 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
[2023-01-31T04:35:13.518+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:13 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 26) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:35:13.518+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:13 INFO Executor: Running task 0.0 in stage 27.0 (TID 26)
[2023-01-31T04:35:13.760+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:13 INFO CodeGenerator: Code generated in 71.02672 ms
[2023-01-31T04:35:14.741+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO PythonRunner: Times: total = 1152, boot = -9528, init = 10680, finish = 0
[2023-01-31T04:35:14.940+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO Executor: Finished task 0.0 in stage 27.0 (TID 26). 2577 bytes result sent to driver
[2023-01-31T04:35:14.950+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 26) in 1432 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:35:14.950+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
[2023-01-31T04:35:14.952+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO DAGScheduler: ShuffleMapStage 27 (save at BigQueryWriteHelper.java:105) finished in 1.873 s
[2023-01-31T04:35:14.953+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:35:14.953+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO DAGScheduler: running: HashSet()
[2023-01-31T04:35:14.953+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:35:14.953+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:35:15.382+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:15 INFO ShufflePartitionsUtil: For shuffle(17, 1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2023-01-31T04:35:17.742+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:17 INFO CodeGenerator: Code generated in 648.560488 ms
[2023-01-31T04:35:18.313+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:18 INFO CodeGenerator: Code generated in 405.726723 ms
[2023-01-31T04:35:19.862+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:19 INFO CodeGenerator: Code generated in 67.730626 ms
[2023-01-31T04:35:20.533+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:20 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2023-01-31T04:35:20.533+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:20 INFO DAGScheduler: Got job 27 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2023-01-31T04:35:20.533+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:20 INFO DAGScheduler: Final stage: ResultStage 30 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2023-01-31T04:35:20.533+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 28, ShuffleMapStage 29)
[2023-01-31T04:35:20.534+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:20 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:35:20.540+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:20 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[163] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2023-01-31T04:35:20.583+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:20 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 48.6 KiB, free 432.8 MiB)
[2023-01-31T04:35:20.719+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:20 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 22.6 KiB, free 432.8 MiB)
[2023-01-31T04:35:20.720+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:20 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 81d5fcd0285b:35387 (size: 22.6 KiB, free: 434.2 MiB)
[2023-01-31T04:35:20.735+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:20 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:35:20.749+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:20 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 81d5fcd0285b:35387 in memory (size: 9.2 KiB, free: 434.2 MiB)
[2023-01-31T04:35:20.878+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[163] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:35:20.878+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:20 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
[2023-01-31T04:35:20.881+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:20 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 27) (81d5fcd0285b, executor driver, partition 0, NODE_LOCAL, 7897 bytes) taskResourceAssignments Map()
[2023-01-31T04:35:20.885+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:20 INFO Executor: Running task 0.0 in stage 30.0 (TID 27)
[2023-01-31T04:35:20.936+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:20 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 81d5fcd0285b:35387 in memory (size: 10.2 KiB, free: 434.3 MiB)
[2023-01-31T04:35:21.202+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:21 INFO ShuffleBlockFetcherIterator: Getting 1 (288.0 B) non-empty blocks including 1 (288.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:35:21.214+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
[2023-01-31T04:35:21.537+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:21 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 81d5fcd0285b:35387 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:35:21.762+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:21 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 81d5fcd0285b:35387 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:35:21.893+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:21 INFO CodeGenerator: Code generated in 594.270373 ms
[2023-01-31T04:35:22.065+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:22 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 81d5fcd0285b:35387 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:35:22.254+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:22 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 81d5fcd0285b:35387 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:35:22.350+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:22 INFO CodeGenerator: Code generated in 177.372474 ms
[2023-01-31T04:35:22.410+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:22 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:35:22.446+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 39 ms
[2023-01-31T04:35:22.555+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:22 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 81d5fcd0285b:35387 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:35:22.621+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:22 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 81d5fcd0285b:35387 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:35:22.782+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:22 INFO CodeGenerator: Code generated in 219.170076 ms
[2023-01-31T04:35:23.250+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:23 INFO CodeGenerator: Code generated in 286.774859 ms
[2023-01-31T04:35:24.237+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:24 INFO Executor: Finished task 0.0 in stage 30.0 (TID 27). 6215 bytes result sent to driver
[2023-01-31T04:35:24.254+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:24 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 27) in 3374 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:35:24.256+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:24 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool
[2023-01-31T04:35:24.269+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:24 INFO DAGScheduler: ResultStage 30 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 3.716 s
[2023-01-31T04:35:24.329+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:24 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:35:24.376+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
[2023-01-31T04:35:24.378+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:24 INFO DAGScheduler: Job 27 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 3.776259 s
[2023-01-31T04:35:24.876+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:24 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 81d5fcd0285b:35387 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:35:25.062+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:25 INFO CodeGenerator: Code generated in 60.616337 ms
[2023-01-31T04:35:25.073+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:25 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 1024.1 KiB, free 432.0 MiB)
[2023-01-31T04:35:25.079+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:25 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 286.0 B, free 432.0 MiB)
[2023-01-31T04:35:25.082+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:25 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 81d5fcd0285b:35387 (size: 286.0 B, free: 434.3 MiB)
[2023-01-31T04:35:25.085+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:25 INFO SparkContext: Created broadcast 37 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2023-01-31T04:35:25.129+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:25 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 81d5fcd0285b:35387 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:35:25.396+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:25 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 81d5fcd0285b:35387 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:35:26.429+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:26 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 81d5fcd0285b:35387 in memory (size: 22.6 KiB, free: 434.3 MiB)
[2023-01-31T04:35:26.615+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:26 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 81d5fcd0285b:35387 in memory (size: 8.1 KiB, free: 434.4 MiB)
[2023-01-31T04:35:27.272+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:27 INFO ShufflePartitionsUtil: For shuffle(2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2023-01-31T04:35:29.265+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:29 INFO CodeGenerator: Code generated in 434.356215 ms
[2023-01-31T04:35:29.845+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:29 INFO CodeGenerator: Code generated in 499.977816 ms
[2023-01-31T04:35:30.534+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:30 INFO CodeGenerator: Code generated in 465.34324 ms
[2023-01-31T04:35:31.124+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:31 INFO CodeGenerator: Code generated in 334.416844 ms
[2023-01-31T04:35:31.535+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:31 INFO CodeGenerator: Code generated in 348.982216 ms
[2023-01-31T04:35:32.089+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO CodeGenerator: Code generated in 462.172416 ms
[2023-01-31T04:35:32.301+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO CodeGenerator: Code generated in 167.64398 ms
[2023-01-31T04:35:32.513+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO CodeGenerator: Code generated in 184.794192 ms
[2023-01-31T04:35:32.798+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO CodeGenerator: Code generated in 232.215353 ms
[2023-01-31T04:35:33.157+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:33 INFO CodeGenerator: Code generated in 306.235504 ms
[2023-01-31T04:35:33.929+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:33 INFO CodeGenerator: Code generated in 697.599205 ms
[2023-01-31T04:35:34.490+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:34 INFO CodeGenerator: Code generated in 307.234535 ms
[2023-01-31T04:35:34.795+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:34 INFO CodeGenerator: Code generated in 238.013314 ms
[2023-01-31T04:35:35.731+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:35 INFO CodeGenerator: Code generated in 877.96181 ms
[2023-01-31T04:35:36.331+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:36 INFO CodeGenerator: Code generated in 394.583268 ms
[2023-01-31T04:35:37.423+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:37 INFO CodeGenerator: Code generated in 403.947751 ms
[2023-01-31T04:35:38.403+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:38 INFO CodeGenerator: Code generated in 358.23798 ms
[2023-01-31T04:35:39.772+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:39 INFO CodeGenerator: Code generated in 518.191935 ms
[2023-01-31T04:35:43.125+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:43 INFO CodeGenerator: Code generated in 867.060072 ms
[2023-01-31T04:35:44.115+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:44 INFO CodeGenerator: Code generated in 86.887069 ms
[2023-01-31T04:35:44.283+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:44 INFO CodeGenerator: Code generated in 71.173729 ms
[2023-01-31T04:35:44.358+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:44 INFO CodeGenerator: Code generated in 30.428189 ms
[2023-01-31T04:35:44.450+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:44 INFO CodeGenerator: Code generated in 36.465612 ms
[2023-01-31T04:35:44.789+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:44 INFO CodeGenerator: Code generated in 290.592746 ms
[2023-01-31T04:35:45.251+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:45 INFO CodeGenerator: Code generated in 108.383109 ms
[2023-01-31T04:35:45.553+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:45 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 81d5fcd0285b:35387 in memory (size: 8.1 KiB, free: 434.4 MiB)
[2023-01-31T04:35:46.573+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:46 INFO CodeGenerator: Code generated in 254.318208 ms
[2023-01-31T04:35:47.639+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:47 INFO CodeGenerator: Code generated in 328.021842 ms
[2023-01-31T04:35:49.305+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:49 INFO CodeGenerator: Code generated in 449.449494 ms
[2023-01-31T04:35:50.053+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:50 INFO CodeGenerator: Code generated in 209.898774 ms
[2023-01-31T04:35:50.683+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:50 INFO CodeGenerator: Code generated in 128.385708 ms
[2023-01-31T04:35:51.915+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:51 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T04:35:52.177+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:52 INFO DAGScheduler: Got job 28 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:35:52.178+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:52 INFO DAGScheduler: Final stage: ResultStage 46 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:35:52.179+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 32, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 37, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 31)
[2023-01-31T04:35:52.199+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:52 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:35:52.219+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:52 INFO DAGScheduler: Submitting ResultStage 46 (MapPartitionsRDD[224] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:35:52.452+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:52 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 276.3 KiB, free 431.9 MiB)
[2023-01-31T04:35:52.710+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:52 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 84.6 KiB, free 431.8 MiB)
[2023-01-31T04:35:52.715+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:52 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 81d5fcd0285b:35387 (size: 84.6 KiB, free: 434.3 MiB)
[2023-01-31T04:35:52.717+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:52 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:35:52.718+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (MapPartitionsRDD[224] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:35:52.718+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:52 INFO TaskSchedulerImpl: Adding task set 46.0 with 1 tasks resource profile 0
[2023-01-31T04:35:52.720+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:52 INFO TaskSetManager: Starting task 0.0 in stage 46.0 (TID 28) (81d5fcd0285b, executor driver, partition 0, NODE_LOCAL, 8781 bytes) taskResourceAssignments Map()
[2023-01-31T04:35:52.724+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:52 INFO Executor: Running task 0.0 in stage 46.0 (TID 28)
[2023-01-31T04:35:54.798+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:54 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:35:54.799+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
[2023-01-31T04:35:55.016+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:35:55.017+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 58 ms
[2023-01-31T04:35:55.175+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:35:55.180+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
[2023-01-31T04:35:55.272+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:35:55.273+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
[2023-01-31T04:35:55.376+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:35:55.377+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
[2023-01-31T04:35:55.541+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:35:55.541+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2023-01-31T04:35:55.593+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:35:55.595+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
[2023-01-31T04:35:55.689+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:35:55.690+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2023-01-31T04:35:55.824+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:35:55.825+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2023-01-31T04:35:55.872+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:35:55.873+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2023-01-31T04:35:55.895+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:35:55.905+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
[2023-01-31T04:35:55.919+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:35:55.926+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
[2023-01-31T04:35:55.947+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:35:55.955+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
[2023-01-31T04:35:55.966+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:35:55.972+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2023-01-31T04:35:55.996+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:35:55.999+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2023-01-31T04:35:56.107+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:56 INFO CodeGenerator: Code generated in 14.399965 ms
[2023-01-31T04:35:57.633+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:57 INFO Executor: Finished task 0.0 in stage 46.0 (TID 28). 25165 bytes result sent to driver
[2023-01-31T04:35:57.707+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:57 INFO TaskSetManager: Finished task 0.0 in stage 46.0 (TID 28) in 4987 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:35:57.718+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:57 INFO TaskSchedulerImpl: Removed TaskSet 46.0, whose tasks have all completed, from pool
[2023-01-31T04:35:57.756+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:57 INFO DAGScheduler: ResultStage 46 (save at BigQueryWriteHelper.java:105) finished in 5.506 s
[2023-01-31T04:35:57.791+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:57 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:35:57.861+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 46: Stage finished
[2023-01-31T04:35:57.873+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:57 INFO DAGScheduler: Job 28 finished: save at BigQueryWriteHelper.java:105, took 5.851463 s
[2023-01-31T04:35:57.965+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:57 INFO DAGScheduler: Registering RDD 225 (save at BigQueryWriteHelper.java:105) as input to shuffle 18
[2023-01-31T04:35:57.966+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:57 INFO DAGScheduler: Got map stage job 29 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:35:57.966+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:57 INFO DAGScheduler: Final stage: ShuffleMapStage 62 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:35:57.966+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:57 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ShuffleMapStage 55, ShuffleMapStage 56, ShuffleMapStage 57, ShuffleMapStage 58, ShuffleMapStage 59, ShuffleMapStage 60, ShuffleMapStage 61)
[2023-01-31T04:35:57.969+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:57 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:35:58.133+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:58 INFO DAGScheduler: Submitting ShuffleMapStage 62 (MapPartitionsRDD[225] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:35:59.454+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:59 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 276.2 KiB, free 431.5 MiB)
[2023-01-31T04:35:59.498+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:59 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 84.6 KiB, free 431.5 MiB)
[2023-01-31T04:35:59.498+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:59 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 81d5fcd0285b:35387 (size: 84.6 KiB, free: 434.2 MiB)
[2023-01-31T04:35:59.501+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:59 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:35:59.507+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:59 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 62 (MapPartitionsRDD[225] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:35:59.514+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:59 INFO TaskSchedulerImpl: Adding task set 62.0 with 1 tasks resource profile 0
[2023-01-31T04:35:59.549+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:59 INFO TaskSetManager: Starting task 0.0 in stage 62.0 (TID 29) (81d5fcd0285b, executor driver, partition 0, NODE_LOCAL, 8770 bytes) taskResourceAssignments Map()
[2023-01-31T04:35:59.609+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:59 INFO Executor: Running task 0.0 in stage 62.0 (TID 29)
[2023-01-31T04:35:59.994+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:59 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:00.008+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 19 ms
[2023-01-31T04:36:00.324+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:00 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:00.329+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
[2023-01-31T04:36:00.630+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:00 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:00.632+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 81 ms
[2023-01-31T04:36:00.748+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:00 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:00.755+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
[2023-01-31T04:36:00.941+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:00 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:00.958+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 35 ms
[2023-01-31T04:36:01.058+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:01 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:01.060+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2023-01-31T04:36:01.121+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:01 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:01.127+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 26 ms
[2023-01-31T04:36:01.186+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:01 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:01.205+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 40 ms
[2023-01-31T04:36:01.283+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:01 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:01.293+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
[2023-01-31T04:36:01.308+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:01 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:01.310+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2023-01-31T04:36:01.802+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:01 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:01.804+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 60 ms
[2023-01-31T04:36:02.059+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:02 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:02.060+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2023-01-31T04:36:02.126+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:02 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:02.127+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 58 ms
[2023-01-31T04:36:02.223+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:02 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:02.241+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 19 ms
[2023-01-31T04:36:02.296+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:02 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:02.302+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
[2023-01-31T04:36:02.674+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:02 INFO Executor: Finished task 0.0 in stage 62.0 (TID 29). 24984 bytes result sent to driver
[2023-01-31T04:36:02.790+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:02 INFO TaskSetManager: Finished task 0.0 in stage 62.0 (TID 29) in 3261 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:36:02.850+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:02 INFO TaskSchedulerImpl: Removed TaskSet 62.0, whose tasks have all completed, from pool
[2023-01-31T04:36:02.855+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:02 INFO DAGScheduler: ShuffleMapStage 62 (save at BigQueryWriteHelper.java:105) finished in 4.287 s
[2023-01-31T04:36:02.855+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:02 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:36:02.855+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:02 INFO DAGScheduler: running: HashSet()
[2023-01-31T04:36:02.855+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:02 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:36:02.856+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:02 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:36:02.941+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:02 INFO ShufflePartitionsUtil: For shuffle(18), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2023-01-31T04:36:04.288+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:04 INFO CodeGenerator: Code generated in 1022.378228 ms
[2023-01-31T04:36:04.824+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:04 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 81d5fcd0285b:35387 in memory (size: 84.6 KiB, free: 434.3 MiB)
[2023-01-31T04:36:05.126+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:05 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 81d5fcd0285b:35387 in memory (size: 84.6 KiB, free: 434.4 MiB)
[2023-01-31T04:36:06.580+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:06 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T04:36:06.591+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:06 INFO DAGScheduler: Got job 30 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:36:06.594+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:06 INFO DAGScheduler: Final stage: ResultStage 79 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:36:06.596+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 78)
[2023-01-31T04:36:06.599+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:06 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:36:06.610+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:06 INFO DAGScheduler: Submitting ResultStage 79 (MapPartitionsRDD[227] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:36:07.555+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:07 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 354.4 KiB, free 431.8 MiB)
[2023-01-31T04:36:07.742+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:07 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 110.5 KiB, free 431.7 MiB)
[2023-01-31T04:36:07.772+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:07 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 81d5fcd0285b:35387 (size: 110.5 KiB, free: 434.3 MiB)
[2023-01-31T04:36:07.875+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:07 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:36:07.883+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 79 (MapPartitionsRDD[227] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:36:07.884+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:07 INFO TaskSchedulerImpl: Adding task set 79.0 with 1 tasks resource profile 0
[2023-01-31T04:36:07.891+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:07 INFO TaskSetManager: Starting task 0.0 in stage 79.0 (TID 30) (81d5fcd0285b, executor driver, partition 0, NODE_LOCAL, 7399 bytes) taskResourceAssignments Map()
[2023-01-31T04:36:07.891+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:07 INFO Executor: Running task 0.0 in stage 79.0 (TID 30)
[2023-01-31T04:36:11.653+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:11 INFO ShuffleBlockFetcherIterator: Getting 1 (568.0 B) non-empty blocks including 1 (568.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:11.655+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
[2023-01-31T04:36:12.017+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:12 INFO CodeGenerator: Code generated in 242.056762 ms
[2023-01-31T04:36:12.236+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:36:12.236+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:36:12.285+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:12 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:36:12.286+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:36:12.286+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:36:12.357+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:36:12.830+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:12 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T04:36:13.104+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:13 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T04:36:14.185+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:14 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T04:36:14.185+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:14 INFO ParquetOutputFormat: Validation is off
[2023-01-31T04:36:14.185+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:14 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T04:36:14.186+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:14 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T04:36:14.186+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T04:36:14.186+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T04:36:14.186+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T04:36:14.186+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T04:36:14.186+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T04:36:14.186+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T04:36:14.186+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T04:36:14.187+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T04:36:14.187+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T04:36:14.187+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T04:36:14.187+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T04:36:14.187+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T04:36:14.187+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T04:36:14.187+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T04:36:14.812+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T04:36:14.813+0000] {spark_submit.py:495} INFO - {
[2023-01-31T04:36:14.813+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T04:36:14.813+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T04:36:14.813+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T04:36:14.813+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T04:36:14.813+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T04:36:14.813+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:14.813+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:14.814+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T04:36:14.814+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T04:36:14.814+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T04:36:14.814+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:14.814+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:14.814+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T04:36:14.814+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:14.814+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:14.814+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:14.814+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:14.814+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T04:36:14.815+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:14.815+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:14.815+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:14.815+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:14.815+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T04:36:14.815+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:14.815+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:14.815+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:14.815+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:14.815+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T04:36:14.815+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:14.815+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:14.816+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:14.816+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:14.816+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T04:36:14.816+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:14.816+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:14.816+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:14.816+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:14.816+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T04:36:14.816+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:14.818+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:14.818+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:14.819+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:14.819+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T04:36:14.819+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:14.819+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:14.819+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:14.819+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:14.819+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T04:36:14.819+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:14.819+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:14.820+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:14.820+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:14.821+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T04:36:14.822+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:14.823+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:14.840+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:14.841+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:14.842+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T04:36:14.843+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:14.921+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:14.926+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:14.931+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:14.931+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T04:36:14.931+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:14.931+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:14.938+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:14.938+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:14.938+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T04:36:14.938+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:14.938+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:14.951+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:14.951+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:14.954+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T04:36:14.954+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:14.954+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:14.954+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:14.958+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:14.958+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T04:36:14.959+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:14.961+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:14.965+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:14.970+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:14.981+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T04:36:14.981+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:14.981+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:14.982+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:14.982+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:14.982+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T04:36:14.982+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:14.982+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:14.982+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:14.982+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:14.983+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T04:36:14.983+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:14.983+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:14.983+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:14.983+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T04:36:14.983+0000] {spark_submit.py:495} INFO - }
[2023-01-31T04:36:14.984+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T04:36:14.984+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T04:36:14.984+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T04:36:14.984+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T04:36:14.984+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T04:36:14.984+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T04:36:14.985+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T04:36:14.986+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T04:36:14.988+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T04:36:14.990+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T04:36:14.991+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T04:36:14.992+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T04:36:14.993+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T04:36:14.994+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T04:36:14.995+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T04:36:14.996+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T04:36:14.996+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T04:36:14.996+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T04:36:14.996+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T04:36:14.997+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T04:36:14.997+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T04:36:14.997+0000] {spark_submit.py:495} INFO - }
[2023-01-31T04:36:14.997+0000] {spark_submit.py:495} INFO - 
[2023-01-31T04:36:14.997+0000] {spark_submit.py:495} INFO - 
[2023-01-31T04:36:18.257+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:18 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T04:36:48.400+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:48 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675139496456-59e71ced-d41b-41ef-9eed-a785570c379d/_temporary/0/_temporary/' directory.
[2023-01-31T04:36:48.413+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:48 INFO FileOutputCommitter: Saved output of task 'attempt_202301310436054043461086949233536_0079_m_000000_30' to gs://entsoe_temp_1009/.spark-bigquery-local-1675139496456-59e71ced-d41b-41ef-9eed-a785570c379d/_temporary/0/task_202301310436054043461086949233536_0079_m_000000
[2023-01-31T04:36:48.571+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:48 INFO SparkHadoopMapRedUtil: attempt_202301310436054043461086949233536_0079_m_000000_30: Committed. Elapsed time: 5605 ms.
[2023-01-31T04:36:49.164+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:49 INFO Executor: Finished task 0.0 in stage 79.0 (TID 30). 26967 bytes result sent to driver
[2023-01-31T04:36:49.415+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:49 INFO TaskSetManager: Finished task 0.0 in stage 79.0 (TID 30) in 41526 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:36:49.416+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:49 INFO TaskSchedulerImpl: Removed TaskSet 79.0, whose tasks have all completed, from pool
[2023-01-31T04:36:49.473+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:49 INFO DAGScheduler: ResultStage 79 (save at BigQueryWriteHelper.java:105) finished in 42.865 s
[2023-01-31T04:36:49.474+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:49 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:36:49.474+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 79: Stage finished
[2023-01-31T04:36:49.497+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:49 INFO DAGScheduler: Job 30 finished: save at BigQueryWriteHelper.java:105, took 42.895583 s
[2023-01-31T04:36:49.535+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:49 INFO FileFormatWriter: Start to commit write Job 52da47a8-26bd-4852-bbac-dd4a9acb7b57.
[2023-01-31T04:36:51.651+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:51 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675139496456-59e71ced-d41b-41ef-9eed-a785570c379d/_temporary/0/task_202301310436054043461086949233536_0079_m_000000/' directory.
[2023-01-31T04:36:52.503+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:52 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675139496456-59e71ced-d41b-41ef-9eed-a785570c379d/' directory.
[2023-01-31T04:36:54.140+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:54 INFO FileFormatWriter: Write Job 52da47a8-26bd-4852-bbac-dd4a9acb7b57 committed. Elapsed time: 4587 ms.
[2023-01-31T04:36:54.313+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:54 INFO FileFormatWriter: Finished processing stats for write job 52da47a8-26bd-4852-bbac-dd4a9acb7b57.
[2023-01-31T04:37:00.068+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:00 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675139496456-59e71ced-d41b-41ef-9eed-a785570c379d/part-00000-29a81366-09ba-4b9f-9927-4cc92d1eef8f-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=cfa6757a-d51e-436b-847d-ed109bde0fd6, location=US}
[2023-01-31T04:37:06.788+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:06 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=cfa6757a-d51e-436b-847d-ed109bde0fd6, location=US}
[2023-01-31T04:37:08.276+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T04:37:09.027+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:09 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T04:37:09.123+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:09 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4041
[2023-01-31T04:37:09.166+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T04:37:09.261+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:09 INFO MemoryStore: MemoryStore cleared
[2023-01-31T04:37:09.262+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:09 INFO BlockManager: BlockManager stopped
[2023-01-31T04:37:09.272+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:09 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T04:37:09.275+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T04:37:09.329+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:09 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T04:37:09.330+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:09 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T04:37:09.335+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-6fba33a3-aa11-4a53-92a7-90fd4f7400ba
[2023-01-31T04:37:09.356+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-2c499bcc-84ad-4b7c-8b46-f1f8a3bd0ac8
[2023-01-31T04:37:09.365+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-6fba33a3-aa11-4a53-92a7-90fd4f7400ba/pyspark-1a489313-b523-4f34-860a-db66ce165973
[2023-01-31T04:37:10.881+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata, task_id=stage_total_generation, execution_date=20210101T040000, start_date=20230131T043053, end_date=20230131T043710
[2023-01-31T04:37:11.289+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T04:37:11.425+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T04:48:29.041+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-31T04:48:29.122+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-31T04:48:29.125+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T04:48:29.126+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T04:48:29.129+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T04:48:29.252+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 04:00:00+00:00
[2023-01-31T04:48:29.272+0000] {standard_task_runner.py:55} INFO - Started process 7424 to run task
[2023-01-31T04:48:29.301+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T04:00:00+00:00', '--job-id', '870', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpkw4g5tth']
[2023-01-31T04:48:29.321+0000] {standard_task_runner.py:83} INFO - Job 870: Subtask stage_total_generation
[2023-01-31T04:48:29.631+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T04:48:30.120+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T04:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T04:00:00+00:00
[2023-01-31T04:48:30.205+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T04:48:30.216+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET
[2023-01-31T04:49:04.873+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:04 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T04:49:05.807+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T04:49:07.625+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:07 INFO ResourceUtils: ==============================================================
[2023-01-31T04:49:07.631+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:07 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T04:49:07.640+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:07 INFO ResourceUtils: ==============================================================
[2023-01-31T04:49:07.643+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:07 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T04:49:07.903+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T04:49:07.945+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:07 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T04:49:07.956+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:07 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T04:49:08.857+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:08 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T04:49:08.860+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:08 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T04:49:08.869+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:08 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T04:49:08.873+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:08 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T04:49:08.879+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T04:49:10.957+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:10 INFO Utils: Successfully started service 'sparkDriver' on port 42171.
[2023-01-31T04:49:11.166+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:11 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T04:49:11.528+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:11 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T04:49:11.675+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:11 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T04:49:11.679+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:11 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T04:49:11.762+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:11 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T04:49:11.827+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6b94c4df-dcb9-4cf5-af33-060fed049482
[2023-01-31T04:49:11.999+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:11 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T04:49:12.351+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:12 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T04:49:15.026+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T04:49:15.028+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:15 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T04:49:15.036+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:15 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T04:49:15.044+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:15 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T04:49:15.047+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:15 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-31T04:49:15.109+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:15 INFO Utils: Successfully started service 'SparkUI' on port 4045.
[2023-01-31T04:49:15.296+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:15 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:42171/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675140544655
[2023-01-31T04:49:15.302+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:15 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:42171/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675140544655
[2023-01-31T04:49:16.186+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:16 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T04:49:16.397+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:16 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T04:49:16.563+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:16 INFO Executor: Fetching spark://81d5fcd0285b:42171/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675140544655
[2023-01-31T04:49:17.236+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:17 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:42171 after 498 ms (0 ms spent in bootstraps)
[2023-01-31T04:49:17.336+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:17 INFO Utils: Fetching spark://81d5fcd0285b:42171/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-71899241-0b88-4429-922c-b3a43b10c3a7/userFiles-6b70ff5e-ecbc-46e2-84a0-f8ea2b394f27/fetchFileTemp1392427143109599643.tmp
[2023-01-31T04:49:20.662+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:20 INFO Executor: Adding file:/tmp/spark-71899241-0b88-4429-922c-b3a43b10c3a7/userFiles-6b70ff5e-ecbc-46e2-84a0-f8ea2b394f27/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T04:49:20.666+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:20 INFO Executor: Fetching spark://81d5fcd0285b:42171/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675140544655
[2023-01-31T04:49:20.681+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:20 INFO Utils: Fetching spark://81d5fcd0285b:42171/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-71899241-0b88-4429-922c-b3a43b10c3a7/userFiles-6b70ff5e-ecbc-46e2-84a0-f8ea2b394f27/fetchFileTemp1701323687367047023.tmp
[2023-01-31T04:49:21.861+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:21 INFO Executor: Adding file:/tmp/spark-71899241-0b88-4429-922c-b3a43b10c3a7/userFiles-6b70ff5e-ecbc-46e2-84a0-f8ea2b394f27/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T04:49:21.956+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45647.
[2023-01-31T04:49:21.956+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:21 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:45647
[2023-01-31T04:49:21.976+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T04:49:22.056+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 45647, None)
[2023-01-31T04:49:22.083+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:22 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:45647 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 45647, None)
[2023-01-31T04:49:22.149+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:22 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 45647, None)
[2023-01-31T04:49:22.155+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:22 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 45647, None)
[2023-01-31T04:49:28.465+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:28 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T04:49:28.511+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:28 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T04:49:52.812+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:52 INFO InMemoryFileIndex: It took 573 ms to list leaf files for 1 paths.
[2023-01-31T04:49:54.544+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T04:49:55.014+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T04:49:55.119+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:45647 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T04:49:55.213+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:55 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T04:49:59.361+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:59 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T04:49:59.532+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:59 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T04:49:59.713+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:59 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T04:50:00.203+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:00 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T04:50:00.210+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:00 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T04:50:00.236+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:00 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:50:00.369+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:00 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:50:00.610+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:00 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T04:50:01.545+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:01 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1675140600370,ArraySeq(org.apache.spark.scheduler.StageInfo@609306e),{spark.master=local, spark.driver.port=42171, spark.submit.pyFiles=, spark.app.startTime=1675140544655, spark.rdd.compress=True, spark.executor.id=driver, spark.app.name=gcp_playground, spark.submit.deployMode=client, spark.hadoop.google.cloud.auth.service.account.json.keyfile=/.google/credentials/google_credentials.json, spark.driver.host=81d5fcd0285b, spark.app.id=local-1675140555568, spark.app.submitTime=1675140532214, spark.hadoop.fs.AbstractFileSystem.gs.impl=google.cloud.hadoop.fs.gcs.GoogleHadoopFS, spark.sql.session.timeZone=UTC, spark.hadoop.google.cloud.auth.service.account.enable=true, spark.app.initial.jar.urls=spark://81d5fcd0285b:42171/jars/gcs-connector-hadoop3-latest.jar,spark://81d5fcd0285b:42171/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED, spark.jars=/opt/spark/jars/gcs-connector-hadoop3-latest.jar, /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.sql.warehouse.dir=file:/opt/***/spark-warehouse, spark.serializer.objectStreamReset=100, spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED}) by listener AppStatusListener took 1.073468655s.
[2023-01-31T04:50:01.808+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:01 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T04:50:01.834+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:01 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T04:50:01.842+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:01 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:45647 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T04:50:01.852+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:01 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:50:02.074+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:50:02.088+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:02 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T04:50:02.573+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:02 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T04:50:02.742+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:02 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T04:50:03.697+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:03 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010400__202101010500.json:0+9364
[2023-01-31T04:50:06.472+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:06 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T04:50:06.571+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:06 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4126 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:50:06.595+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:06 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T04:50:06.623+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:06 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 5.457 s
[2023-01-31T04:50:06.659+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:06 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:50:06.674+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T04:50:06.724+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:06 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 6.978478 s
[2023-01-31T04:50:07.793+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:07 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:45647 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T04:50:07.904+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:07 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:45647 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T04:50:31.093+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:31 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:50:31.097+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:31 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T04:50:31.102+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:31 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:50:33.085+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:33 INFO CodeGenerator: Code generated in 1520.733807 ms
[2023-01-31T04:50:33.186+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:33 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T04:50:33.359+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:33 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.2 MiB)
[2023-01-31T04:50:33.363+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:33 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:45647 (size: 34.4 KiB, free: 434.4 MiB)
[2023-01-31T04:50:33.381+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:33 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T04:50:33.452+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:50:33.787+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:33 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T04:50:33.806+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:33 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T04:50:33.806+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:33 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T04:50:33.806+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:33 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:50:33.807+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:33 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:50:33.810+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:33 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T04:50:33.867+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:33 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T04:50:33.897+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:33 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T04:50:33.900+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:33 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:45647 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T04:50:33.904+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:33 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:50:33.908+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:50:33.910+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:33 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T04:50:33.930+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:33 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:50:33.952+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:33 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T04:50:34.485+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:34 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T04:50:34.950+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:34 INFO CodeGenerator: Code generated in 367.556112 ms
[2023-01-31T04:50:35.500+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1671 bytes result sent to driver
[2023-01-31T04:50:35.519+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1601 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:50:35.520+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T04:50:35.525+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 1.681 s
[2023-01-31T04:50:35.525+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:50:35.525+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T04:50:35.526+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 1.717988 s
[2023-01-31T04:50:36.357+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:36 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:50:36.358+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:36 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T04:50:36.401+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:36 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:50:37.147+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO CodeGenerator: Code generated in 430.934986 ms
[2023-01-31T04:50:37.332+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T04:50:37.553+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.9 MiB)
[2023-01-31T04:50:37.556+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:45647 (size: 34.4 KiB, free: 434.3 MiB)
[2023-01-31T04:50:37.562+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T04:50:37.573+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:50:37.629+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T04:50:37.633+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T04:50:37.633+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T04:50:37.633+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:50:37.634+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:50:37.639+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T04:50:37.657+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T04:50:37.668+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T04:50:37.671+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:45647 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T04:50:37.677+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:50:37.681+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:50:37.685+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T04:50:37.692+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:50:37.693+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T04:50:37.730+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T04:50:38.000+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T04:50:38.012+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 320 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:50:38.015+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.372 s
[2023-01-31T04:50:38.016+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:50:38.021+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T04:50:38.025+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T04:50:38.026+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.394069 s
[2023-01-31T04:50:38.366+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:45647 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T04:50:38.941+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:50:38.943+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T04:50:38.944+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:50:39.547+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:39 INFO CodeGenerator: Code generated in 327.485761 ms
[2023-01-31T04:50:39.554+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:39 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T04:50:39.581+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:39 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.7 MiB)
[2023-01-31T04:50:39.583+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:39 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:45647 (size: 34.4 KiB, free: 434.3 MiB)
[2023-01-31T04:50:39.590+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:39 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T04:50:39.599+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:50:39.687+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:39 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T04:50:39.690+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:39 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T04:50:39.691+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:39 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T04:50:39.691+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:39 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:50:39.691+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:39 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:50:39.700+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:39 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T04:50:39.725+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:39 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T04:50:39.728+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:39 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T04:50:39.730+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:39 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:45647 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T04:50:39.749+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:39 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:50:39.754+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:50:39.754+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:39 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T04:50:39.757+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:39 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:50:39.759+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:39 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T04:50:39.782+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:39 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T04:50:39.979+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:39 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1755 bytes result sent to driver
[2023-01-31T04:50:39.989+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:39 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 232 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:50:39.995+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:39 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.287 s
[2023-01-31T04:50:39.996+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:39 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:50:39.997+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:39 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T04:50:40.000+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T04:50:40.001+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:39 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.310336 s
[2023-01-31T04:50:40.163+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:40 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:50:40.165+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:40 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T04:50:40.169+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:40 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:50:40.279+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:40 INFO CodeGenerator: Code generated in 61.788197 ms
[2023-01-31T04:50:40.307+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:40 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T04:50:40.380+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:40 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.4 MiB)
[2023-01-31T04:50:40.381+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:40 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:45647 (size: 34.4 KiB, free: 434.2 MiB)
[2023-01-31T04:50:40.386+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:40 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T04:50:40.399+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:50:40.531+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:40 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:45647 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T04:50:40.539+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:40 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T04:50:40.541+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:40 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T04:50:40.541+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:40 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T04:50:40.541+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:40 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:50:40.542+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:40 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:50:40.565+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:40 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T04:50:40.574+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:40 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T04:50:40.586+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:40 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T04:50:40.589+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:40 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:45647 (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T04:50:40.592+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:40 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:50:40.595+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:50:40.599+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:40 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T04:50:40.628+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:40 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:50:40.631+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:40 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T04:50:40.668+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:40 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T04:50:41.005+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:41 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T04:50:41.034+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:41 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 430 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:50:41.034+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:41 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T04:50:41.040+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:41 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.476 s
[2023-01-31T04:50:41.041+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:41 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:50:41.041+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T04:50:41.041+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:41 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.498669 s
[2023-01-31T04:50:41.652+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:41 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:45647 in memory (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T04:50:43.429+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:43 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:50:43.431+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:43 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T04:50:43.432+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:43 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:50:44.847+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:44 INFO CodeGenerator: Code generated in 1196.322979 ms
[2023-01-31T04:50:44.919+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:44 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T04:50:45.064+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:45 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.2 MiB)
[2023-01-31T04:50:45.069+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:45 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:45647 (size: 34.4 KiB, free: 434.2 MiB)
[2023-01-31T04:50:45.079+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:45 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T04:50:45.099+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:50:45.398+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:45 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T04:50:45.401+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:45 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T04:50:45.401+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:45 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T04:50:45.402+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:45 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:50:45.402+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:45 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:50:45.411+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:45 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T04:50:45.432+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:45 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T04:50:45.446+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:45 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T04:50:45.449+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:45 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:45647 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T04:50:45.453+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:45 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:50:45.457+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:50:45.461+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:45 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T04:50:45.470+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:45 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:50:45.471+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:45 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T04:50:45.516+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:45 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T04:50:45.759+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:45 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2537 bytes result sent to driver
[2023-01-31T04:50:45.763+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:45 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 293 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:50:45.764+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:45 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T04:50:45.770+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:45 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.353 s
[2023-01-31T04:50:45.770+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:45 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:50:45.771+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T04:50:45.771+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:45 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.368894 s
[2023-01-31T04:50:46.142+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:46 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:50:46.152+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:46 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T04:50:46.154+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:46 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:50:46.476+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:46 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:45647 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T04:50:46.816+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:46 INFO CodeGenerator: Code generated in 318.808926 ms
[2023-01-31T04:50:46.862+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:46 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T04:50:47.002+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.0 MiB)
[2023-01-31T04:50:47.007+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:45647 (size: 34.4 KiB, free: 434.2 MiB)
[2023-01-31T04:50:47.009+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T04:50:47.020+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:50:47.208+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T04:50:47.211+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T04:50:47.212+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T04:50:47.212+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:50:47.217+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:50:47.223+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T04:50:47.262+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 433.0 MiB)
[2023-01-31T04:50:47.284+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T04:50:47.290+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:45647 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T04:50:47.299+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:50:47.307+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:50:47.310+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T04:50:47.323+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:50:47.324+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T04:50:47.367+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T04:50:47.545+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T04:50:47.549+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 230 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:50:47.550+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T04:50:47.555+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.313 s
[2023-01-31T04:50:47.555+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:50:47.556+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T04:50:47.556+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.345800 s
[2023-01-31T04:50:47.986+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:50:47.989+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T04:50:47.991+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:47 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:50:48.160+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:48 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:45647 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T04:50:48.654+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:48 INFO CodeGenerator: Code generated in 500.854066 ms
[2023-01-31T04:50:48.689+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:48 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.8 MiB)
[2023-01-31T04:50:48.948+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:48 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 432.8 MiB)
[2023-01-31T04:50:48.954+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:48 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:45647 (size: 34.4 KiB, free: 434.2 MiB)
[2023-01-31T04:50:48.965+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:48 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T04:50:48.983+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:50:49.173+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:49 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T04:50:49.175+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:49 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T04:50:49.182+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:49 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T04:50:49.183+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:49 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:50:49.183+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:49 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:50:49.196+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:49 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T04:50:49.214+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:49 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T04:50:49.233+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:49 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.7 MiB)
[2023-01-31T04:50:49.242+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:49 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:45647 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T04:50:49.249+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:49 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:50:49.252+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:50:49.261+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:49 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T04:50:49.268+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:49 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:50:49.270+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:49 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T04:50:49.320+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:49 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T04:50:49.743+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:49 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T04:50:49.755+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:49 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 487 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:50:49.759+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:49 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.565 s
[2023-01-31T04:50:49.760+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:49 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:50:49.761+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:49 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T04:50:49.767+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T04:50:49.776+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:49 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.594454 s
[2023-01-31T04:50:50.246+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:50 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:45647 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T04:50:51.953+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:51 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:45647 in memory (size: 34.4 KiB, free: 434.2 MiB)
[2023-01-31T04:50:52.003+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:52 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:45647 in memory (size: 34.4 KiB, free: 434.2 MiB)
[2023-01-31T04:51:21.492+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:21 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:51:21.519+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:51:21.520+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:51:21.526+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:21 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:51:21.528+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:51:21.535+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:51:21.553+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:51:23.589+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:23 INFO CodeGenerator: Code generated in 167.490967 ms
[2023-01-31T04:51:24.523+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:24 INFO DAGScheduler: Registering RDD 118 (save at BigQueryWriteHelper.java:105) as input to shuffle 0
[2023-01-31T04:51:24.625+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:24 INFO DAGScheduler: Got map stage job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:24.637+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:24 INFO DAGScheduler: Final stage: ShuffleMapStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:24.646+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:24 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:24.663+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:24 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:24.704+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:24 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:24.938+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:24 INFO CodeGenerator: Code generated in 314.491755 ms
[2023-01-31T04:51:25.319+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:25 INFO CodeGenerator: Code generated in 199.572302 ms
[2023-01-31T04:51:25.335+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:25 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 11.1 KiB, free 433.2 MiB)
[2023-01-31T04:51:25.435+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:25 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 433.2 MiB)
[2023-01-31T04:51:25.440+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:25 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:45647 (size: 5.7 KiB, free: 434.2 MiB)
[2023-01-31T04:51:25.471+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:25 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:25.612+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:25 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:25.613+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:25 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T04:51:25.759+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:25 INFO DAGScheduler: Registering RDD 120 (save at BigQueryWriteHelper.java:105) as input to shuffle 1
[2023-01-31T04:51:25.760+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:25 INFO DAGScheduler: Got map stage job 9 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:25.761+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:25 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:25.762+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:25 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:25.764+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:25 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:25.801+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:25 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[120] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:25.807+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:25 INFO CodeGenerator: Code generated in 285.155294 ms
[2023-01-31T04:51:25.840+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:25 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7660 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:25.841+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:25 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T04:51:26.183+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 15.6 KiB, free 433.2 MiB)
[2023-01-31T04:51:26.255+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.2 MiB)
[2023-01-31T04:51:26.295+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 81d5fcd0285b:45647 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:51:26.304+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:26.304+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[120] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:26.305+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2023-01-31T04:51:26.319+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO CodeGenerator: Code generated in 248.317946 ms
[2023-01-31T04:51:26.330+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO DAGScheduler: Registering RDD 122 (save at BigQueryWriteHelper.java:105) as input to shuffle 2
[2023-01-31T04:51:26.358+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO DAGScheduler: Got map stage job 10 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:26.361+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO DAGScheduler: Final stage: ShuffleMapStage 10 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:26.362+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:26.365+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:26.380+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[122] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:26.510+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 15.7 KiB, free 433.2 MiB)
[2023-01-31T04:51:26.576+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.2 MiB)
[2023-01-31T04:51:26.584+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 81d5fcd0285b:45647 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:51:26.588+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO CodeGenerator: Code generated in 109.824464 ms
[2023-01-31T04:51:26.590+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:26.609+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[122] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:26.618+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2023-01-31T04:51:26.638+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO DAGScheduler: Registering RDD 124 (save at BigQueryWriteHelper.java:105) as input to shuffle 3
[2023-01-31T04:51:26.640+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO DAGScheduler: Got map stage job 11 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:26.645+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO DAGScheduler: Final stage: ShuffleMapStage 11 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:26.648+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:26.650+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:26.723+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[124] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:26.730+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO CodeGenerator: Code generated in 293.736529 ms
[2023-01-31T04:51:26.853+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 15.7 KiB, free 433.1 MiB)
[2023-01-31T04:51:26.925+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO CodeGenerator: Code generated in 151.552913 ms
[2023-01-31T04:51:26.942+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.1 MiB)
[2023-01-31T04:51:26.955+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 81d5fcd0285b:45647 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:51:26.962+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:26.970+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[124] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:26.973+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2023-01-31T04:51:26.993+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO DAGScheduler: Registering RDD 126 (save at BigQueryWriteHelper.java:105) as input to shuffle 4
[2023-01-31T04:51:26.995+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO DAGScheduler: Got map stage job 12 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:26.995+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:26 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:27.007+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:27.012+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:27.027+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[126] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:27.174+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 15.7 KiB, free 433.1 MiB)
[2023-01-31T04:51:27.193+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.1 MiB)
[2023-01-31T04:51:27.199+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 81d5fcd0285b:45647 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:51:27.204+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:27.210+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[126] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:27.212+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2023-01-31T04:51:27.219+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO CodeGenerator: Code generated in 169.454924 ms
[2023-01-31T04:51:27.224+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Registering RDD 128 (save at BigQueryWriteHelper.java:105) as input to shuffle 5
[2023-01-31T04:51:27.226+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Got map stage job 13 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:27.227+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Final stage: ShuffleMapStage 13 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:27.230+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:27.232+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:27.252+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[128] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:27.301+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 15.7 KiB, free 433.1 MiB)
[2023-01-31T04:51:27.321+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.1 MiB)
[2023-01-31T04:51:27.324+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 81d5fcd0285b:45647 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:51:27.330+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:27.334+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[128] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:27.337+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2023-01-31T04:51:27.347+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Registering RDD 130 (save at BigQueryWriteHelper.java:105) as input to shuffle 6
[2023-01-31T04:51:27.349+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Got map stage job 14 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:27.350+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Final stage: ShuffleMapStage 14 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:27.352+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:27.355+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:27.372+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[130] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:27.412+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO CodeGenerator: Code generated in 115.923443 ms
[2023-01-31T04:51:27.418+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 15.7 KiB, free 433.1 MiB)
[2023-01-31T04:51:27.431+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.1 MiB)
[2023-01-31T04:51:27.434+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 81d5fcd0285b:45647 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:51:27.436+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:27.441+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[130] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:27.442+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
[2023-01-31T04:51:27.449+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Registering RDD 132 (save at BigQueryWriteHelper.java:105) as input to shuffle 7
[2023-01-31T04:51:27.450+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Got map stage job 15 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:27.452+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Final stage: ShuffleMapStage 15 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:27.455+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:27.456+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:27.471+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[132] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:27.585+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 15.7 KiB, free 433.1 MiB)
[2023-01-31T04:51:27.607+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.0 MiB)
[2023-01-31T04:51:27.610+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 81d5fcd0285b:45647 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:51:27.617+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:27.627+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[132] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:27.628+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
[2023-01-31T04:51:27.647+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Registering RDD 134 (save at BigQueryWriteHelper.java:105) as input to shuffle 8
[2023-01-31T04:51:27.649+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Got map stage job 16 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:27.650+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Final stage: ShuffleMapStage 16 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:27.651+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:27.661+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO CodeGenerator: Code generated in 175.377536 ms
[2023-01-31T04:51:27.669+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:27.705+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[134] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:27.791+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 15.7 KiB, free 433.0 MiB)
[2023-01-31T04:51:27.877+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.0 MiB)
[2023-01-31T04:51:27.883+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 81d5fcd0285b:45647 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:51:27.884+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2130 bytes result sent to driver
[2023-01-31T04:51:27.892+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:27.899+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
[2023-01-31T04:51:27.903+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:27.906+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 2254 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:27.907+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T04:51:27.914+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[134] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:27.923+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
[2023-01-31T04:51:27.939+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Registering RDD 136 (save at BigQueryWriteHelper.java:105) as input to shuffle 9
[2023-01-31T04:51:27.942+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Got map stage job 17 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:27.948+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Final stage: ShuffleMapStage 17 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:27.949+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:27.954+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:27.979+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: Submitting ShuffleMapStage 17 (MapPartitionsRDD[136] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:28.012+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO CodeGenerator: Code generated in 148.114643 ms
[2023-01-31T04:51:28.017+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 15.7 KiB, free 433.0 MiB)
[2023-01-31T04:51:28.048+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.0 MiB)
[2023-01-31T04:51:28.052+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 81d5fcd0285b:45647 (size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T04:51:28.059+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:28.090+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[136] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:28.090+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
[2023-01-31T04:51:28.124+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: ShuffleMapStage 8 (save at BigQueryWriteHelper.java:105) finished in 3.370 s
[2023-01-31T04:51:28.136+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:28.143+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: running: HashSet(ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 9, ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15)
[2023-01-31T04:51:28.149+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:28.154+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:28.198+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO CodeGenerator: Code generated in 83.145406 ms
[2023-01-31T04:51:28.242+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Registering RDD 138 (save at BigQueryWriteHelper.java:105) as input to shuffle 10
[2023-01-31T04:51:28.243+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Got map stage job 18 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:28.243+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Final stage: ShuffleMapStage 18 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:28.244+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:28.245+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:28.255+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[138] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:28.271+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 15.7 KiB, free 433.0 MiB)
[2023-01-31T04:51:28.295+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.0 MiB)
[2023-01-31T04:51:28.297+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 81d5fcd0285b:45647 (size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T04:51:28.311+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:28.325+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO CodeGenerator: Code generated in 57.478316 ms
[2023-01-31T04:51:28.330+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[138] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:28.335+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
[2023-01-31T04:51:28.336+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Registering RDD 140 (save at BigQueryWriteHelper.java:105) as input to shuffle 11
[2023-01-31T04:51:28.336+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Got map stage job 19 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:28.337+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:28.340+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:28.340+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:28.349+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[140] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:28.365+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 15.7 KiB, free 433.0 MiB)
[2023-01-31T04:51:28.393+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.0 MiB)
[2023-01-31T04:51:28.399+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 81d5fcd0285b:45647 (size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T04:51:28.401+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:28.402+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[140] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:28.403+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
[2023-01-31T04:51:28.405+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Registering RDD 142 (save at BigQueryWriteHelper.java:105) as input to shuffle 12
[2023-01-31T04:51:28.406+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Got map stage job 20 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:28.411+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Final stage: ShuffleMapStage 20 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:28.412+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:28.412+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:28.426+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[142] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:28.434+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 15.7 KiB, free 432.9 MiB)
[2023-01-31T04:51:28.453+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO CodeGenerator: Code generated in 58.230909 ms
[2023-01-31T04:51:28.455+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 432.9 MiB)
[2023-01-31T04:51:28.457+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 81d5fcd0285b:45647 (size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T04:51:28.464+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:28.471+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[142] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:28.474+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
[2023-01-31T04:51:28.505+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Registering RDD 144 (save at BigQueryWriteHelper.java:105) as input to shuffle 13
[2023-01-31T04:51:28.506+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Got map stage job 21 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:28.506+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Final stage: ShuffleMapStage 21 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:28.507+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:28.507+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:28.529+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Submitting ShuffleMapStage 21 (MapPartitionsRDD[144] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:28.554+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 15.7 KiB, free 432.9 MiB)
[2023-01-31T04:51:28.638+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 432.9 MiB)
[2023-01-31T04:51:28.640+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 81d5fcd0285b:45647 (size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T04:51:28.644+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:28.648+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[144] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:28.664+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
[2023-01-31T04:51:28.665+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO CodeGenerator: Code generated in 119.851411 ms
[2023-01-31T04:51:28.666+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:45647 in memory (size: 5.7 KiB, free: 434.1 MiB)
[2023-01-31T04:51:28.692+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Registering RDD 146 (save at BigQueryWriteHelper.java:105) as input to shuffle 14
[2023-01-31T04:51:28.693+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Got map stage job 22 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:28.694+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Final stage: ShuffleMapStage 22 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:28.695+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:28.695+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:28.706+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[146] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:28.757+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 15.7 KiB, free 432.9 MiB)
[2023-01-31T04:51:28.770+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 432.9 MiB)
[2023-01-31T04:51:28.774+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 81d5fcd0285b:45647 (size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T04:51:28.776+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:28.783+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[146] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:28.784+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
[2023-01-31T04:51:28.819+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO CodeGenerator: Code generated in 51.737646 ms
[2023-01-31T04:51:28.842+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Registering RDD 148 (save at BigQueryWriteHelper.java:105) as input to shuffle 15
[2023-01-31T04:51:28.844+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Got map stage job 23 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:28.845+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Final stage: ShuffleMapStage 23 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:28.846+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:28.846+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:28.860+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[148] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:28.905+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 15.7 KiB, free 432.9 MiB)
[2023-01-31T04:51:28.920+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 432.9 MiB)
[2023-01-31T04:51:28.928+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 81d5fcd0285b:45647 (size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T04:51:28.932+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:28.938+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[148] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:28.942+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
[2023-01-31T04:51:29.006+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:29 INFO CodeGenerator: Code generated in 88.382694 ms
[2023-01-31T04:51:29.052+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:29 INFO DAGScheduler: Registering RDD 150 (save at BigQueryWriteHelper.java:105) as input to shuffle 16
[2023-01-31T04:51:29.053+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:29 INFO DAGScheduler: Got map stage job 24 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:29.054+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:29 INFO DAGScheduler: Final stage: ShuffleMapStage 24 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:29.055+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:29 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:29.056+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:29 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:29.063+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:29 INFO DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[150] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:29.109+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:29 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 15.7 KiB, free 432.9 MiB)
[2023-01-31T04:51:29.125+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:29 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 432.9 MiB)
[2023-01-31T04:51:29.130+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:29 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 81d5fcd0285b:45647 (size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T04:51:29.136+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:29 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:29.149+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:29 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[150] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:29.153+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:29 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
[2023-01-31T04:51:31.228+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:31 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2023-01-31T04:51:32.282+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:32 INFO CodeGenerator: Code generated in 239.318804 ms
[2023-01-31T04:51:32.719+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:32 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2023-01-31T04:51:32.722+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:32 INFO DAGScheduler: Got job 25 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2023-01-31T04:51:32.725+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:32 INFO DAGScheduler: Final stage: ResultStage 26 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2023-01-31T04:51:32.726+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)
[2023-01-31T04:51:32.727+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:32 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:32.734+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:32 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[154] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2023-01-31T04:51:32.780+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:32 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 21.6 KiB, free 432.8 MiB)
[2023-01-31T04:51:32.784+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:32 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 432.8 MiB)
[2023-01-31T04:51:32.786+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:32 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 81d5fcd0285b:45647 (size: 10.2 KiB, free: 434.1 MiB)
[2023-01-31T04:51:32.787+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:32 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:32.790+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[154] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:32.791+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:32 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
[2023-01-31T04:51:41.899+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:41 INFO CodeGenerator: Code generated in 491.008602 ms
[2023-01-31T04:51:42.958+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:42 INFO PythonRunner: Times: total = 14660, boot = 13010, init = 1649, finish = 1
[2023-01-31T04:51:43.032+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:43 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 2521 bytes result sent to driver
[2023-01-31T04:51:43.036+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:43 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:43.052+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:43 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 15159 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:43.053+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:43 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2023-01-31T04:51:43.057+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:43 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 52015
[2023-01-31T04:51:43.073+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:43 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
[2023-01-31T04:51:43.156+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:43 INFO DAGScheduler: ShuffleMapStage 9 (save at BigQueryWriteHelper.java:105) finished in 17.358 s
[2023-01-31T04:51:43.157+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:43 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:43.158+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:43 INFO DAGScheduler: running: HashSet(ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:43.158+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:43 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:43.159+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:43 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:44.518+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:44 INFO PythonRunner: Times: total = 1266, boot = -252, init = 1518, finish = 0
[2023-01-31T04:51:44.629+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:44 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 2478 bytes result sent to driver
[2023-01-31T04:51:44.634+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:44 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:44.645+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:44 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
[2023-01-31T04:51:44.647+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:44 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 1610 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:44.647+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:44 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2023-01-31T04:51:44.657+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:44 INFO DAGScheduler: ShuffleMapStage 10 (save at BigQueryWriteHelper.java:105) finished in 18.256 s
[2023-01-31T04:51:44.657+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:44 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:44.658+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:44 INFO DAGScheduler: running: HashSet(ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:44.687+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:44 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:44.688+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:44 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:46.135+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO PythonRunner: Times: total = 1283, boot = -172, init = 1455, finish = 0
[2023-01-31T04:51:46.191+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 2521 bytes result sent to driver
[2023-01-31T04:51:46.198+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:46.200+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
[2023-01-31T04:51:46.201+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 1568 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:46.201+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2023-01-31T04:51:46.202+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO DAGScheduler: ShuffleMapStage 11 (save at BigQueryWriteHelper.java:105) finished in 19.502 s
[2023-01-31T04:51:46.203+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:46.203+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO DAGScheduler: running: HashSet(ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:46.204+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:46.209+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:46.924+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO PythonRunner: Times: total = 642, boot = -65, init = 707, finish = 0
[2023-01-31T04:51:46.954+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 2478 bytes result sent to driver
[2023-01-31T04:51:46.963+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:46.965+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)
[2023-01-31T04:51:46.966+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 765 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:46.968+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2023-01-31T04:51:46.968+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO DAGScheduler: ShuffleMapStage 12 (save at BigQueryWriteHelper.java:105) finished in 19.923 s
[2023-01-31T04:51:46.981+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:46.982+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO DAGScheduler: running: HashSet(ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:46.982+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:46.983+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:47.849+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:47 INFO PythonRunner: Times: total = 737, boot = -40, init = 777, finish = 0
[2023-01-31T04:51:47.874+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:47 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 2521 bytes result sent to driver
[2023-01-31T04:51:47.883+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:47 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:47.891+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:47 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 930 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:47.892+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:47 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2023-01-31T04:51:47.892+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:47 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)
[2023-01-31T04:51:47.893+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:47 INFO DAGScheduler: ShuffleMapStage 13 (save at BigQueryWriteHelper.java:105) finished in 20.633 s
[2023-01-31T04:51:47.906+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:47 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:47.907+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:47 INFO DAGScheduler: running: HashSet(ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:47.907+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:47 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:47.908+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:47 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:49.775+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:49 INFO PythonRunner: Times: total = 1695, boot = -59, init = 1754, finish = 0
[2023-01-31T04:51:49.841+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:49 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 2478 bytes result sent to driver
[2023-01-31T04:51:49.868+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:49 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:49.897+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:49 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)
[2023-01-31T04:51:49.898+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:49 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 2019 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:49.899+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:49 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
[2023-01-31T04:51:49.902+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:49 INFO DAGScheduler: ShuffleMapStage 14 (save at BigQueryWriteHelper.java:105) finished in 22.526 s
[2023-01-31T04:51:49.902+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:49 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:49.949+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:49 INFO DAGScheduler: running: HashSet(ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:49.949+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:49 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:49.950+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:49 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:50.947+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:50 INFO PythonRunner: Times: total = 860, boot = -54, init = 914, finish = 0
[2023-01-31T04:51:50.995+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:50 INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 2478 bytes result sent to driver
[2023-01-31T04:51:51.001+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:51 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:51.006+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:51 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 1135 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:51.006+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:51 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2023-01-31T04:51:51.006+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:51 INFO DAGScheduler: ShuffleMapStage 15 (save at BigQueryWriteHelper.java:105) finished in 23.529 s
[2023-01-31T04:51:51.007+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:51 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:51.007+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:51 INFO DAGScheduler: running: HashSet(ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:51.014+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:51 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:51.015+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:51 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:51.015+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:51 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)
[2023-01-31T04:51:51.698+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:51 INFO PythonRunner: Times: total = 535, boot = 4, init = 531, finish = 0
[2023-01-31T04:51:51.730+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:51 INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 2521 bytes result sent to driver
[2023-01-31T04:51:51.732+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:51 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:51.734+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:51 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 735 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:51.735+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:51 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool
[2023-01-31T04:51:51.737+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:51 INFO Executor: Running task 0.0 in stage 17.0 (TID 17)
[2023-01-31T04:51:51.737+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:51 INFO DAGScheduler: ShuffleMapStage 16 (save at BigQueryWriteHelper.java:105) finished in 24.034 s
[2023-01-31T04:51:51.738+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:51 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:51.739+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:51 INFO DAGScheduler: running: HashSet(ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:51.747+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:51 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:51.748+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:51 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:53.276+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:53 INFO PythonRunner: Times: total = 1447, boot = -54, init = 1501, finish = 0
[2023-01-31T04:51:53.328+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:53 INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 2478 bytes result sent to driver
[2023-01-31T04:51:53.337+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:53 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:53.337+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:53 INFO Executor: Running task 0.0 in stage 18.0 (TID 18)
[2023-01-31T04:51:53.338+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:53 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 1602 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:53.338+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:53 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
[2023-01-31T04:51:53.339+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:53 INFO DAGScheduler: ShuffleMapStage 17 (save at BigQueryWriteHelper.java:105) finished in 25.355 s
[2023-01-31T04:51:53.339+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:53 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:53.339+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:53 INFO DAGScheduler: running: HashSet(ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:53.340+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:53 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:53.340+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:53 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:54.939+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:54 INFO PythonRunner: Times: total = 1392, boot = -37, init = 1429, finish = 0
[2023-01-31T04:51:55.070+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:55 INFO Executor: Finished task 0.0 in stage 18.0 (TID 18). 2478 bytes result sent to driver
[2023-01-31T04:51:55.077+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:55 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:55.084+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:55 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 1748 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:55.085+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:55 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
[2023-01-31T04:51:55.108+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:55 INFO DAGScheduler: ShuffleMapStage 18 (save at BigQueryWriteHelper.java:105) finished in 26.826 s
[2023-01-31T04:51:55.109+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:55 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:55.109+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:55 INFO DAGScheduler: running: HashSet(ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:55.110+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:55 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:55.110+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:55 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:55.128+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:55 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)
[2023-01-31T04:51:57.032+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:57 INFO PythonRunner: Times: total = 1727, boot = -79, init = 1805, finish = 1
[2023-01-31T04:51:57.075+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:57 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 2478 bytes result sent to driver
[2023-01-31T04:51:57.104+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:57 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:57.105+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:57 INFO Executor: Running task 0.0 in stage 20.0 (TID 20)
[2023-01-31T04:51:57.106+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:57 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 2034 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:57.107+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:57 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
[2023-01-31T04:51:57.141+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:57 INFO DAGScheduler: ShuffleMapStage 19 (save at BigQueryWriteHelper.java:105) finished in 28.781 s
[2023-01-31T04:51:57.141+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:57 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:57.141+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:57 INFO DAGScheduler: running: HashSet(ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:57.150+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:57 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:57.154+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:57 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:58.065+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:58 INFO PythonRunner: Times: total = 908, boot = -35, init = 943, finish = 0
[2023-01-31T04:51:58.077+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:58 INFO Executor: Finished task 0.0 in stage 20.0 (TID 20). 2521 bytes result sent to driver
[2023-01-31T04:51:58.086+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:58 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:58.086+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:58 INFO Executor: Running task 0.0 in stage 21.0 (TID 21)
[2023-01-31T04:51:58.087+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:58 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 980 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:58.087+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:58 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool
[2023-01-31T04:51:58.093+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:58 INFO DAGScheduler: ShuffleMapStage 20 (save at BigQueryWriteHelper.java:105) finished in 29.667 s
[2023-01-31T04:51:58.093+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:58 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:58.094+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:58 INFO DAGScheduler: running: HashSet(ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:58.096+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:58 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:58.097+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:58 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:58.527+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:58 INFO PythonRunner: Times: total = 372, boot = 43, init = 329, finish = 0
[2023-01-31T04:51:58.549+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:58 INFO Executor: Finished task 0.0 in stage 21.0 (TID 21). 2521 bytes result sent to driver
[2023-01-31T04:51:58.555+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:58 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:58.556+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:58 INFO Executor: Running task 0.0 in stage 22.0 (TID 22)
[2023-01-31T04:51:58.556+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:58 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 21) in 474 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:58.557+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:58 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
[2023-01-31T04:51:58.557+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:58 INFO DAGScheduler: ShuffleMapStage 21 (save at BigQueryWriteHelper.java:105) finished in 30.025 s
[2023-01-31T04:51:58.562+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:58 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:58.564+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:58 INFO DAGScheduler: running: HashSet(ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:58.564+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:58 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:58.566+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:58 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:59.234+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:59 INFO PythonRunner: Times: total = 579, boot = -4, init = 582, finish = 1
[2023-01-31T04:51:59.349+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:59 INFO Executor: Finished task 0.0 in stage 22.0 (TID 22). 2521 bytes result sent to driver
[2023-01-31T04:51:59.395+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:59 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 23) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:59.420+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:59 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 859 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:59.428+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:59 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool
[2023-01-31T04:51:59.433+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:59 INFO DAGScheduler: ShuffleMapStage 22 (save at BigQueryWriteHelper.java:105) finished in 30.710 s
[2023-01-31T04:51:59.435+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:59 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:59.461+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:59 INFO DAGScheduler: running: HashSet(ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:59.462+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:59 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:59.462+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:59 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:59.462+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:59 INFO Executor: Running task 0.0 in stage 23.0 (TID 23)
[2023-01-31T04:52:00.869+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:00 INFO PythonRunner: Times: total = 1184, boot = -337, init = 1521, finish = 0
[2023-01-31T04:52:00.884+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:00 INFO Executor: Finished task 0.0 in stage 23.0 (TID 23). 2478 bytes result sent to driver
[2023-01-31T04:52:00.908+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:00 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 24) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:52:00.917+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:00 INFO Executor: Running task 0.0 in stage 24.0 (TID 24)
[2023-01-31T04:52:00.917+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:00 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 23) in 1547 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:52:00.918+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:00 INFO DAGScheduler: ShuffleMapStage 23 (save at BigQueryWriteHelper.java:105) finished in 32.048 s
[2023-01-31T04:52:00.931+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:00 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:52:00.931+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:00 INFO DAGScheduler: running: HashSet(ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:52:00.932+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:00 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:52:00.937+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:00 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:52:00.960+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:00 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool
[2023-01-31T04:52:02.122+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:02 INFO PythonRunner: Times: total = 911, boot = -288, init = 1199, finish = 0
[2023-01-31T04:52:02.142+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:02 INFO Executor: Finished task 0.0 in stage 24.0 (TID 24). 2521 bytes result sent to driver
[2023-01-31T04:52:02.149+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:02 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 25) (81d5fcd0285b, executor driver, partition 0, NODE_LOCAL, 7399 bytes) taskResourceAssignments Map()
[2023-01-31T04:52:02.151+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:02 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 24) in 1249 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:52:02.153+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:02 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool
[2023-01-31T04:52:02.156+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:02 INFO DAGScheduler: ShuffleMapStage 24 (save at BigQueryWriteHelper.java:105) finished in 33.083 s
[2023-01-31T04:52:02.158+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:02 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:52:02.159+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:02 INFO DAGScheduler: running: HashSet(ResultStage 26)
[2023-01-31T04:52:02.161+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:02 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:52:02.161+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:02 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:52:02.163+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:02 INFO Executor: Running task 0.0 in stage 26.0 (TID 25)
[2023-01-31T04:52:02.556+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:02 INFO ShuffleBlockFetcherIterator: Getting 1 (97.0 B) non-empty blocks including 1 (97.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:02.595+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 122 ms
[2023-01-31T04:52:02.957+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:02 INFO CodeGenerator: Code generated in 213.727554 ms
[2023-01-31T04:52:03.147+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:03 INFO CodeGenerator: Code generated in 127.351068 ms
[2023-01-31T04:52:03.362+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:03 INFO CodeGenerator: Code generated in 38.990429 ms
[2023-01-31T04:52:03.410+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:03 INFO CodeGenerator: Code generated in 38.707349 ms
[2023-01-31T04:52:03.605+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:03 INFO CodeGenerator: Code generated in 12.289132 ms
[2023-01-31T04:52:03.680+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:03 INFO CodeGenerator: Code generated in 57.867185 ms
[2023-01-31T04:52:03.786+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:03 INFO CodeGenerator: Code generated in 90.101244 ms
[2023-01-31T04:52:03.874+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:03 INFO Executor: Finished task 0.0 in stage 26.0 (TID 25). 3217 bytes result sent to driver
[2023-01-31T04:52:03.907+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:03 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 25) in 1763 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:52:03.913+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:03 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool
[2023-01-31T04:52:03.913+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:03 INFO DAGScheduler: ResultStage 26 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 31.135 s
[2023-01-31T04:52:03.914+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:03 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:52:03.914+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
[2023-01-31T04:52:03.915+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:03 INFO DAGScheduler: Job 25 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 31.189679 s
[2023-01-31T04:52:04.511+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:04 INFO CodeGenerator: Code generated in 286.766648 ms
[2023-01-31T04:52:04.516+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:04 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 1024.1 KiB, free 431.8 MiB)
[2023-01-31T04:52:04.525+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:04 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 279.0 B, free 431.8 MiB)
[2023-01-31T04:52:04.527+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:04 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 81d5fcd0285b:45647 (size: 279.0 B, free: 434.1 MiB)
[2023-01-31T04:52:04.532+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:04 INFO SparkContext: Created broadcast 34 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2023-01-31T04:52:06.661+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:06 INFO CodeGenerator: Code generated in 228.551622 ms
[2023-01-31T04:52:06.872+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:06 INFO DAGScheduler: Registering RDD 156 (save at BigQueryWriteHelper.java:105) as input to shuffle 17
[2023-01-31T04:52:06.873+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:06 INFO DAGScheduler: Got map stage job 26 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:52:06.873+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:06 INFO DAGScheduler: Final stage: ShuffleMapStage 27 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:52:06.874+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:06 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:52:06.889+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:06 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:52:06.915+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:06 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[156] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:52:07.300+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:07 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 18.1 KiB, free 431.8 MiB)
[2023-01-31T04:52:07.322+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:07 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 431.8 MiB)
[2023-01-31T04:52:07.324+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:07 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 81d5fcd0285b:45647 (size: 9.2 KiB, free: 434.1 MiB)
[2023-01-31T04:52:07.328+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:07 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:52:07.353+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:07 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[156] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:52:07.353+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:07 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
[2023-01-31T04:52:07.355+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:07 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 26) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:52:07.357+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:07 INFO Executor: Running task 0.0 in stage 27.0 (TID 26)
[2023-01-31T04:52:07.802+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:07 INFO CodeGenerator: Code generated in 281.399411 ms
[2023-01-31T04:52:08.570+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO PythonRunner: Times: total = 1083, boot = -5300, init = 6382, finish = 1
[2023-01-31T04:52:08.654+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO Executor: Finished task 0.0 in stage 27.0 (TID 26). 2577 bytes result sent to driver
[2023-01-31T04:52:08.657+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 26) in 1303 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:52:08.658+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
[2023-01-31T04:52:08.672+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO DAGScheduler: ShuffleMapStage 27 (save at BigQueryWriteHelper.java:105) finished in 1.714 s
[2023-01-31T04:52:08.673+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:52:08.674+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO DAGScheduler: running: HashSet()
[2023-01-31T04:52:08.675+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:52:08.688+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:52:09.267+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:09 INFO ShufflePartitionsUtil: For shuffle(17, 1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2023-01-31T04:52:09.422+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:09 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 81d5fcd0285b:45647 in memory (size: 9.2 KiB, free: 434.1 MiB)
[2023-01-31T04:52:09.697+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:09 INFO CodeGenerator: Code generated in 89.149863 ms
[2023-01-31T04:52:09.746+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:09 INFO CodeGenerator: Code generated in 45.013782 ms
[2023-01-31T04:52:10.110+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:10 INFO CodeGenerator: Code generated in 83.994286 ms
[2023-01-31T04:52:10.428+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:10 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2023-01-31T04:52:10.429+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:10 INFO DAGScheduler: Got job 27 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2023-01-31T04:52:10.430+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:10 INFO DAGScheduler: Final stage: ResultStage 30 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2023-01-31T04:52:10.430+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 28, ShuffleMapStage 29)
[2023-01-31T04:52:10.430+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:10 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:52:10.444+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:10 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[163] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2023-01-31T04:52:10.485+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:10 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 48.6 KiB, free 431.8 MiB)
[2023-01-31T04:52:10.488+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:10 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 22.6 KiB, free 431.8 MiB)
[2023-01-31T04:52:10.491+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:10 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 81d5fcd0285b:45647 (size: 22.6 KiB, free: 434.1 MiB)
[2023-01-31T04:52:10.492+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:10 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:52:10.498+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[163] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:52:10.513+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:10 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
[2023-01-31T04:52:10.519+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:10 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 27) (81d5fcd0285b, executor driver, partition 0, NODE_LOCAL, 7897 bytes) taskResourceAssignments Map()
[2023-01-31T04:52:10.540+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:10 INFO Executor: Running task 0.0 in stage 30.0 (TID 27)
[2023-01-31T04:52:10.701+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:10 INFO ShuffleBlockFetcherIterator: Getting 1 (288.0 B) non-empty blocks including 1 (288.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:10.713+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
[2023-01-31T04:52:10.814+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:10 INFO CodeGenerator: Code generated in 85.824098 ms
[2023-01-31T04:52:10.899+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:10 INFO CodeGenerator: Code generated in 41.690018 ms
[2023-01-31T04:52:10.919+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:10 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:10.926+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
[2023-01-31T04:52:11.061+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:11 INFO CodeGenerator: Code generated in 96.773035 ms
[2023-01-31T04:52:11.127+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:11 INFO CodeGenerator: Code generated in 36.860845 ms
[2023-01-31T04:52:11.265+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:11 INFO Executor: Finished task 0.0 in stage 30.0 (TID 27). 6215 bytes result sent to driver
[2023-01-31T04:52:11.272+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:11 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 27) in 756 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:52:11.277+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:11 INFO DAGScheduler: ResultStage 30 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.817 s
[2023-01-31T04:52:11.278+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:11 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:52:11.283+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:11 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool
[2023-01-31T04:52:11.284+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
[2023-01-31T04:52:11.284+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:11 INFO DAGScheduler: Job 27 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.856776 s
[2023-01-31T04:52:11.375+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:11 INFO CodeGenerator: Code generated in 58.334908 ms
[2023-01-31T04:52:11.387+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:11 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 1024.1 KiB, free 430.8 MiB)
[2023-01-31T04:52:11.398+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:11 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 286.0 B, free 430.8 MiB)
[2023-01-31T04:52:11.400+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:11 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 81d5fcd0285b:45647 (size: 286.0 B, free: 434.1 MiB)
[2023-01-31T04:52:11.409+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:11 INFO SparkContext: Created broadcast 37 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2023-01-31T04:52:12.220+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:12 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 81d5fcd0285b:45647 in memory (size: 22.6 KiB, free: 434.1 MiB)
[2023-01-31T04:52:12.301+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:12 INFO ShufflePartitionsUtil: For shuffle(2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2023-01-31T04:52:13.730+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:13 INFO CodeGenerator: Code generated in 352.965004 ms
[2023-01-31T04:52:13.990+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:13 INFO CodeGenerator: Code generated in 200.917476 ms
[2023-01-31T04:52:14.471+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:14 INFO CodeGenerator: Code generated in 454.671505 ms
[2023-01-31T04:52:14.522+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:14 INFO CodeGenerator: Code generated in 40.418973 ms
[2023-01-31T04:52:14.579+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:14 INFO CodeGenerator: Code generated in 45.582184 ms
[2023-01-31T04:52:14.698+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:14 INFO CodeGenerator: Code generated in 97.582952 ms
[2023-01-31T04:52:15.232+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:15 INFO CodeGenerator: Code generated in 520.768111 ms
[2023-01-31T04:52:15.610+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:15 INFO CodeGenerator: Code generated in 240.959218 ms
[2023-01-31T04:52:16.582+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:16 INFO CodeGenerator: Code generated in 892.801531 ms
[2023-01-31T04:52:16.949+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:16 INFO CodeGenerator: Code generated in 276.839229 ms
[2023-01-31T04:52:17.208+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:17 INFO CodeGenerator: Code generated in 169.757056 ms
[2023-01-31T04:52:17.602+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:17 INFO CodeGenerator: Code generated in 356.792194 ms
[2023-01-31T04:52:17.948+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:17 INFO CodeGenerator: Code generated in 286.032598 ms
[2023-01-31T04:52:18.225+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:18 INFO CodeGenerator: Code generated in 267.653559 ms
[2023-01-31T04:52:18.280+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:18 INFO CodeGenerator: Code generated in 39.698774 ms
[2023-01-31T04:52:18.917+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:18 INFO CodeGenerator: Code generated in 272.734292 ms
[2023-01-31T04:52:19.101+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:19 INFO CodeGenerator: Code generated in 81.774892 ms
[2023-01-31T04:52:19.431+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:19 INFO CodeGenerator: Code generated in 37.788316 ms
[2023-01-31T04:52:19.532+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:19 INFO CodeGenerator: Code generated in 26.271787 ms
[2023-01-31T04:52:20.062+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:20 INFO CodeGenerator: Code generated in 154.020205 ms
[2023-01-31T04:52:20.377+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:20 INFO CodeGenerator: Code generated in 104.313011 ms
[2023-01-31T04:52:20.784+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:20 INFO CodeGenerator: Code generated in 72.126752 ms
[2023-01-31T04:52:21.001+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:21 INFO CodeGenerator: Code generated in 107.46562 ms
[2023-01-31T04:52:21.286+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:21 INFO CodeGenerator: Code generated in 58.786495 ms
[2023-01-31T04:52:21.415+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:21 INFO CodeGenerator: Code generated in 44.782522 ms
[2023-01-31T04:52:21.633+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:21 INFO CodeGenerator: Code generated in 77.665574 ms
[2023-01-31T04:52:21.748+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:21 INFO CodeGenerator: Code generated in 37.226752 ms
[2023-01-31T04:52:22.297+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:22 INFO CodeGenerator: Code generated in 200.393872 ms
[2023-01-31T04:52:22.417+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:22 INFO CodeGenerator: Code generated in 39.223472 ms
[2023-01-31T04:52:22.766+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:22 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T04:52:22.802+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:22 INFO DAGScheduler: Got job 28 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:52:22.807+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:22 INFO DAGScheduler: Final stage: ResultStage 46 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:52:22.808+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 32, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 37, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 31)
[2023-01-31T04:52:22.810+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:22 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:52:22.831+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:22 INFO DAGScheduler: Submitting ResultStage 46 (MapPartitionsRDD[224] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:52:22.990+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:22 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 276.5 KiB, free 430.6 MiB)
[2023-01-31T04:52:23.030+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:23 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 84.7 KiB, free 430.5 MiB)
[2023-01-31T04:52:23.034+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:23 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 81d5fcd0285b:45647 (size: 84.7 KiB, free: 434.0 MiB)
[2023-01-31T04:52:23.036+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:23 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:52:23.045+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (MapPartitionsRDD[224] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:52:23.046+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:23 INFO TaskSchedulerImpl: Adding task set 46.0 with 1 tasks resource profile 0
[2023-01-31T04:52:23.056+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:23 INFO TaskSetManager: Starting task 0.0 in stage 46.0 (TID 28) (81d5fcd0285b, executor driver, partition 0, NODE_LOCAL, 8781 bytes) taskResourceAssignments Map()
[2023-01-31T04:52:23.058+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:23 INFO Executor: Running task 0.0 in stage 46.0 (TID 28)
[2023-01-31T04:52:23.510+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:23 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:23.513+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
[2023-01-31T04:52:23.564+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:23 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:23.568+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
[2023-01-31T04:52:23.651+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:23 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:23.652+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms
[2023-01-31T04:52:23.904+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:23 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:23.905+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
[2023-01-31T04:52:24.059+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:24 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:24.060+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 43 ms
[2023-01-31T04:52:24.246+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:24 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:24.290+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 86 ms
[2023-01-31T04:52:24.473+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:24 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:24.474+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
[2023-01-31T04:52:24.527+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:24 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:24.528+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2023-01-31T04:52:24.582+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:24 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:24.582+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2023-01-31T04:52:24.660+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:24 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:24.673+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
[2023-01-31T04:52:24.733+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:24 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:24.738+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
[2023-01-31T04:52:24.806+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:24 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:24.807+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
[2023-01-31T04:52:24.909+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:24 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:24.927+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 51 ms
[2023-01-31T04:52:24.978+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:24 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:24.981+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
[2023-01-31T04:52:25.018+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:25 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:25.020+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2023-01-31T04:52:25.104+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:25 INFO CodeGenerator: Code generated in 40.781134 ms
[2023-01-31T04:52:26.366+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:26 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 81d5fcd0285b:45647 in memory (size: 8.1 KiB, free: 434.0 MiB)
[2023-01-31T04:52:26.734+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:26 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 81d5fcd0285b:45647 in memory (size: 10.2 KiB, free: 434.0 MiB)
[2023-01-31T04:52:27.082+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:27 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 81d5fcd0285b:45647 in memory (size: 8.1 KiB, free: 434.0 MiB)
[2023-01-31T04:52:27.230+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:27 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 81d5fcd0285b:45647 in memory (size: 8.1 KiB, free: 434.0 MiB)
[2023-01-31T04:52:27.338+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:27 INFO Executor: Finished task 0.0 in stage 46.0 (TID 28). 25165 bytes result sent to driver
[2023-01-31T04:52:27.350+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:27 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:45647 in memory (size: 34.4 KiB, free: 434.1 MiB)
[2023-01-31T04:52:27.363+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:27 INFO TaskSetManager: Finished task 0.0 in stage 46.0 (TID 28) in 4308 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:52:27.371+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:27 INFO DAGScheduler: ResultStage 46 (save at BigQueryWriteHelper.java:105) finished in 4.514 s
[2023-01-31T04:52:27.373+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:27 INFO TaskSchedulerImpl: Removed TaskSet 46.0, whose tasks have all completed, from pool
[2023-01-31T04:52:27.378+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:27 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:52:27.380+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 46: Stage finished
[2023-01-31T04:52:27.382+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:27 INFO DAGScheduler: Job 28 finished: save at BigQueryWriteHelper.java:105, took 4.613509 s
[2023-01-31T04:52:27.453+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:27 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 81d5fcd0285b:45647 in memory (size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T04:52:27.562+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:27 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 81d5fcd0285b:45647 in memory (size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T04:52:27.699+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:27 INFO DAGScheduler: Registering RDD 225 (save at BigQueryWriteHelper.java:105) as input to shuffle 18
[2023-01-31T04:52:27.708+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:27 INFO DAGScheduler: Got map stage job 29 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:52:27.710+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:27 INFO DAGScheduler: Final stage: ShuffleMapStage 62 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:52:27.711+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ShuffleMapStage 55, ShuffleMapStage 56, ShuffleMapStage 57, ShuffleMapStage 58, ShuffleMapStage 59, ShuffleMapStage 60, ShuffleMapStage 61)
[2023-01-31T04:52:27.714+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:27 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 81d5fcd0285b:45647 in memory (size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T04:52:27.716+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:27 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:52:27.748+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:27 INFO DAGScheduler: Submitting ShuffleMapStage 62 (MapPartitionsRDD[225] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:52:27.770+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:27 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 81d5fcd0285b:45647 in memory (size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T04:52:27.895+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:27 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 81d5fcd0285b:45647 in memory (size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T04:52:28.024+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 276.5 KiB, free 430.6 MiB)
[2023-01-31T04:52:28.034+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 81d5fcd0285b:45647 in memory (size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T04:52:28.128+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 84.7 KiB, free 430.6 MiB)
[2023-01-31T04:52:28.132+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 81d5fcd0285b:45647 (size: 84.7 KiB, free: 434.0 MiB)
[2023-01-31T04:52:28.174+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:52:28.182+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 81d5fcd0285b:45647 in memory (size: 84.7 KiB, free: 434.1 MiB)
[2023-01-31T04:52:28.183+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 62 (MapPartitionsRDD[225] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:52:28.188+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO TaskSchedulerImpl: Adding task set 62.0 with 1 tasks resource profile 0
[2023-01-31T04:52:28.202+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO TaskSetManager: Starting task 0.0 in stage 62.0 (TID 29) (81d5fcd0285b, executor driver, partition 0, NODE_LOCAL, 8770 bytes) taskResourceAssignments Map()
[2023-01-31T04:52:28.232+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO Executor: Running task 0.0 in stage 62.0 (TID 29)
[2023-01-31T04:52:28.246+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 81d5fcd0285b:45647 in memory (size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T04:52:28.294+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:45647 in memory (size: 34.4 KiB, free: 434.2 MiB)
[2023-01-31T04:52:28.377+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:45647 in memory (size: 34.4 KiB, free: 434.2 MiB)
[2023-01-31T04:52:28.470+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 81d5fcd0285b:45647 in memory (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:52:28.511+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:28.515+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2023-01-31T04:52:28.518+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 81d5fcd0285b:45647 in memory (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:52:28.529+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:28.529+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2023-01-31T04:52:28.545+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:28.546+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2023-01-31T04:52:28.560+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:28.569+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2023-01-31T04:52:28.569+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:45647 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T04:52:28.575+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:28.576+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2023-01-31T04:52:28.590+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:28.591+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2023-01-31T04:52:28.597+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:28.597+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2023-01-31T04:52:28.604+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:28.604+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2023-01-31T04:52:28.609+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:28.609+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2023-01-31T04:52:28.615+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:28.616+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2023-01-31T04:52:28.627+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:28.627+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2023-01-31T04:52:28.634+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:28.635+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2023-01-31T04:52:28.645+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:28.646+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2023-01-31T04:52:28.654+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:45647 in memory (size: 34.4 KiB, free: 434.3 MiB)
[2023-01-31T04:52:28.656+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:28.657+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2023-01-31T04:52:28.678+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:28.678+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2023-01-31T04:52:28.699+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 81d5fcd0285b:45647 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:52:28.745+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 81d5fcd0285b:45647 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:52:28.789+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 81d5fcd0285b:45647 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:52:28.821+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 81d5fcd0285b:45647 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:52:28.837+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO Executor: Finished task 0.0 in stage 62.0 (TID 29). 24984 bytes result sent to driver
[2023-01-31T04:52:28.847+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO TaskSetManager: Finished task 0.0 in stage 62.0 (TID 29) in 648 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:52:28.865+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO DAGScheduler: ShuffleMapStage 62 (save at BigQueryWriteHelper.java:105) finished in 1.061 s
[2023-01-31T04:52:28.866+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:52:28.867+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO DAGScheduler: running: HashSet()
[2023-01-31T04:52:28.868+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:52:28.868+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:52:28.869+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO TaskSchedulerImpl: Removed TaskSet 62.0, whose tasks have all completed, from pool
[2023-01-31T04:52:28.872+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:28 INFO ShufflePartitionsUtil: For shuffle(18), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2023-01-31T04:52:29.250+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:29 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 81d5fcd0285b:45647 in memory (size: 84.7 KiB, free: 434.4 MiB)
[2023-01-31T04:52:29.404+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:29 INFO CodeGenerator: Code generated in 67.554133 ms
[2023-01-31T04:52:30.008+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:30 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T04:52:30.018+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:30 INFO DAGScheduler: Got job 30 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:52:30.021+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:30 INFO DAGScheduler: Final stage: ResultStage 79 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:52:30.021+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 78)
[2023-01-31T04:52:30.022+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:52:30.033+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:30 INFO DAGScheduler: Submitting ResultStage 79 (MapPartitionsRDD[227] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:52:30.619+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:30 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 349.2 KiB, free 431.8 MiB)
[2023-01-31T04:52:30.664+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:30 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 109.6 KiB, free 431.7 MiB)
[2023-01-31T04:52:30.683+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:30 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 81d5fcd0285b:45647 (size: 109.6 KiB, free: 434.3 MiB)
[2023-01-31T04:52:30.685+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:30 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:52:30.689+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 79 (MapPartitionsRDD[227] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:52:30.689+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:30 INFO TaskSchedulerImpl: Adding task set 79.0 with 1 tasks resource profile 0
[2023-01-31T04:52:30.698+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:30 INFO TaskSetManager: Starting task 0.0 in stage 79.0 (TID 30) (81d5fcd0285b, executor driver, partition 0, NODE_LOCAL, 7399 bytes) taskResourceAssignments Map()
[2023-01-31T04:52:30.725+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:30 INFO Executor: Running task 0.0 in stage 79.0 (TID 30)
[2023-01-31T04:52:32.327+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:32 INFO ShuffleBlockFetcherIterator: Getting 1 (568.0 B) non-empty blocks including 1 (568.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:32.328+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2023-01-31T04:52:32.396+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:52:32.396+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:52:32.449+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:52:32.449+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:52:32.450+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:52:32.465+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:52:32.524+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:32 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T04:52:32.569+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:32 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T04:52:32.698+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:32 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T04:52:32.698+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:32 INFO ParquetOutputFormat: Validation is off
[2023-01-31T04:52:32.699+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:32 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T04:52:32.699+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:32 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T04:52:32.699+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T04:52:32.700+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T04:52:32.700+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T04:52:32.701+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T04:52:32.705+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T04:52:32.705+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T04:52:32.705+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T04:52:32.706+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T04:52:32.708+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T04:52:32.709+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T04:52:32.710+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T04:52:32.711+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T04:52:32.714+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T04:52:32.716+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T04:52:33.035+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:33 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T04:52:33.036+0000] {spark_submit.py:495} INFO - {
[2023-01-31T04:52:33.036+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T04:52:33.036+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T04:52:33.037+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T04:52:33.037+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T04:52:33.038+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T04:52:33.043+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:33.044+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:33.045+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T04:52:33.046+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T04:52:33.047+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T04:52:33.048+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:33.048+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:33.050+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T04:52:33.051+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:33.052+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:33.053+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:33.054+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:33.055+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T04:52:33.056+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:33.057+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:33.058+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:33.059+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:33.060+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T04:52:33.061+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:33.062+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:33.063+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:33.064+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:33.065+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T04:52:33.066+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:33.067+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:33.068+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:33.069+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:33.070+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T04:52:33.071+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:33.074+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:33.075+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:33.082+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:33.083+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T04:52:33.091+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:33.093+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:33.094+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:33.095+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:33.099+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T04:52:33.101+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:33.103+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:33.104+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:33.104+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:33.105+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T04:52:33.106+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:33.107+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:33.108+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:33.109+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:33.109+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T04:52:33.111+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:33.111+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:33.111+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:33.114+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:33.114+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T04:52:33.115+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:33.116+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:33.117+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:33.118+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:33.119+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T04:52:33.120+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:33.121+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:33.122+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:33.124+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:33.124+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T04:52:33.126+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:33.126+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:33.127+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:33.128+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:33.129+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T04:52:33.130+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:33.131+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:33.132+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:33.133+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:33.135+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T04:52:33.135+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:33.137+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:33.137+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:33.138+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:33.139+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T04:52:33.140+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:33.141+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:33.142+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:33.143+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:33.144+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T04:52:33.145+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:33.146+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:33.148+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:33.148+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:33.149+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T04:52:33.150+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:33.151+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:33.151+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:33.153+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T04:52:33.154+0000] {spark_submit.py:495} INFO - }
[2023-01-31T04:52:33.155+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T04:52:33.156+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T04:52:33.156+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T04:52:33.157+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T04:52:33.158+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T04:52:33.159+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T04:52:33.161+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T04:52:33.161+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T04:52:33.162+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T04:52:33.162+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T04:52:33.174+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T04:52:33.175+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T04:52:33.184+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T04:52:33.186+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T04:52:33.197+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T04:52:33.199+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T04:52:33.205+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T04:52:33.208+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T04:52:33.209+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T04:52:33.221+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T04:52:33.221+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T04:52:33.221+0000] {spark_submit.py:495} INFO - }
[2023-01-31T04:52:33.223+0000] {spark_submit.py:495} INFO - 
[2023-01-31T04:52:33.223+0000] {spark_submit.py:495} INFO - 
[2023-01-31T04:52:38.603+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:38 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T04:53:16.078+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:16 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675140555568-845b6b8c-298f-4d2e-a815-75286ffbf92c/_temporary/0/_temporary/' directory.
[2023-01-31T04:53:16.081+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:16 INFO FileOutputCommitter: Saved output of task 'attempt_202301310452298536695367741710929_0079_m_000000_30' to gs://entsoe_temp_1009/.spark-bigquery-local-1675140555568-845b6b8c-298f-4d2e-a815-75286ffbf92c/_temporary/0/task_202301310452298536695367741710929_0079_m_000000
[2023-01-31T04:53:16.087+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:16 INFO SparkHadoopMapRedUtil: attempt_202301310452298536695367741710929_0079_m_000000_30: Committed. Elapsed time: 1923 ms.
[2023-01-31T04:53:16.228+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:16 INFO Executor: Finished task 0.0 in stage 79.0 (TID 30). 26967 bytes result sent to driver
[2023-01-31T04:53:16.260+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:16 INFO TaskSetManager: Finished task 0.0 in stage 79.0 (TID 30) in 45558 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:53:16.261+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:16 INFO TaskSchedulerImpl: Removed TaskSet 79.0, whose tasks have all completed, from pool
[2023-01-31T04:53:16.272+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:16 INFO DAGScheduler: ResultStage 79 (save at BigQueryWriteHelper.java:105) finished in 46.223 s
[2023-01-31T04:53:16.273+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:16 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:53:16.274+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 79: Stage finished
[2023-01-31T04:53:16.297+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:16 INFO DAGScheduler: Job 30 finished: save at BigQueryWriteHelper.java:105, took 46.276037 s
[2023-01-31T04:53:16.327+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:16 INFO FileFormatWriter: Start to commit write Job bd6eae43-99c0-41af-8912-9360130f90b4.
[2023-01-31T04:53:17.641+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:17 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675140555568-845b6b8c-298f-4d2e-a815-75286ffbf92c/_temporary/0/task_202301310452298536695367741710929_0079_m_000000/' directory.
[2023-01-31T04:53:19.006+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:19 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675140555568-845b6b8c-298f-4d2e-a815-75286ffbf92c/' directory.
[2023-01-31T04:53:20.150+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:20 INFO FileFormatWriter: Write Job bd6eae43-99c0-41af-8912-9360130f90b4 committed. Elapsed time: 3811 ms.
[2023-01-31T04:53:20.202+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:20 INFO FileFormatWriter: Finished processing stats for write job bd6eae43-99c0-41af-8912-9360130f90b4.
[2023-01-31T04:53:22.397+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:22 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675140555568-845b6b8c-298f-4d2e-a815-75286ffbf92c/part-00000-b9c5f591-5676-4020-9fe8-7ad3fe48d2b6-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=15ff1960-d31c-40ba-a0fa-9d867c77a3fb, location=US}
[2023-01-31T04:53:28.595+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:28 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=15ff1960-d31c-40ba-a0fa-9d867c77a3fb, location=US}
[2023-01-31T04:53:29.359+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T04:53:29.549+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:29 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T04:53:29.571+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:29 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4045
[2023-01-31T04:53:29.624+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T04:53:29.748+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:29 INFO MemoryStore: MemoryStore cleared
[2023-01-31T04:53:29.750+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:29 INFO BlockManager: BlockManager stopped
[2023-01-31T04:53:29.758+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:29 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T04:53:29.775+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T04:53:29.799+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:29 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T04:53:29.800+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:29 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T04:53:29.802+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-71899241-0b88-4429-922c-b3a43b10c3a7/pyspark-b76b0b1f-7d60-423d-813e-920486eab4e7
[2023-01-31T04:53:29.810+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-71899241-0b88-4429-922c-b3a43b10c3a7
[2023-01-31T04:53:29.817+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-e1079268-2aa9-43fe-9d4a-4c1249585fdb
[2023-01-31T04:53:30.060+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata, task_id=stage_total_generation, execution_date=20210101T040000, start_date=20230131T044829, end_date=20230131T045330
[2023-01-31T04:53:30.119+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T04:53:30.150+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
