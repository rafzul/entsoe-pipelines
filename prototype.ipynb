{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from google.oauth2 import service_account\n",
    "import xmltodict\n",
    "import json\n",
    "import pandas as pd \n",
    "from requests import request\n",
    "import pytz\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Dict\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import storage\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, BooleanType\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "\n",
    "from entsoe import EntsoeRawClient\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load env variables\n",
    "load_dotenv('./creds/.env', verbose=True, override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(os.environ.get(\"PYSPARK_DRIVER_PYTHON\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entsoe_analytics_1009\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "----------------\n",
    "INIT VARIABLES\n",
    "----------------\n",
    "'''\n",
    "\n",
    "#setting up entsoe variables\n",
    "security_token = os.environ.get(\"SECURITY_TOKEN\")\n",
    "ENTSOE_URL = 'https://transparency.entsoe.eu/api'\n",
    "\n",
    "#setting up GCP variables\n",
    "service_account_file = os.environ.get(\"SERVICE_ACCOUNT_FILE\")\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    service_account_file\n",
    ")\n",
    "gcs_bucket = os.environ.get(\"CLOUD_STORAGE_BUCKET\")\n",
    "print(gcs_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# #setting up session\n",
    "# entsoe_session = requests.Session()\n",
    "\n",
    "# '''\n",
    "# ----------------\n",
    "# SETTING UP FUNCTION CALLS \n",
    "# ----------------\n",
    "# '''\n",
    "\n",
    "# #format dates\n",
    "# def datetime_to_str(dtm: pd.Timestamp) -> str:\n",
    "#     #convert timezone to UTC if it's exist and it's not on UTC already\n",
    "#     if dtm.tzinfo is not None and dtm.tzinfo != pytz.UTC:\n",
    "#         dtm = dtm.tz_convert(\"UTC\")\n",
    "#     fmt = '%Y%m%d%H%M'\n",
    "#     dtm_str = dtm.strftime(fmt)\n",
    "#     return dtm_str\n",
    "\n",
    "# #basic requests\n",
    "# def basic_requests(params: Dict, start:pd.Timestamp, end: pd.Timestamp) -> requests.Response:\n",
    "#     #setting up time intervals start and stop\n",
    "#     start_str = datetime_to_str(start)\n",
    "#     end_str = datetime_to_str(end)\n",
    "\n",
    "#     #setting up params and extending with custom parameter based\n",
    "#     base_params = {\n",
    "#         'securityToken': security_token,\n",
    "#         'periodStart': start_str,\n",
    "#         'periodEnd': end_str,\n",
    "#     }\n",
    "#     params.update(base_params)\n",
    "    \n",
    "#     #seting up sesssion\n",
    "#     session = requests.Session()\n",
    "#     response = session.get(url=ENTSOE_URL, params=params)\n",
    "\n",
    "#     return response\n",
    "\n",
    "# # upload data to GCS\n",
    "# def upload_blob_to_gcs(bucket_name, contents, destination_blob_name):\n",
    "#     # Upload file to bucket\"\"\"\n",
    "\n",
    "#     # ID of GCS bucket\n",
    "#     # bucket_name =\n",
    "\n",
    "#     # the contents from memory to be uploaded to file\n",
    "#     # contents =\n",
    "\n",
    "#     # the ID of your GCS object\n",
    "#     # destination_blob_name =\n",
    "\n",
    "#     storage_client = storage.Client(credentials=credentials)\n",
    "#     bucket = storage_client.bucket(bucket_name)\n",
    "#     blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "#     blob.upload_from_string(contents)\n",
    "\n",
    "\n",
    "# '''\n",
    "# ----------------\n",
    "# EXTRACTION\n",
    "# ----------------\n",
    "# '''\n",
    "# #for test, we'll be querying Actual Generation Output per Generation Unit\n",
    "\n",
    "# domain = '10YCZ-CEPS-----N'\n",
    "# params_requests = {\n",
    "#     'documentType': 'A73',\n",
    "#     'processType': 'A16',\n",
    "#     'in_Domain': {domain},\n",
    "# }\n",
    "# start=pd.Timestamp('202101011200', tz='Europe/Berlin')\n",
    "# end=pd.Timestamp('202101011300', tz='Europe/Berlin')\n",
    "\n",
    "# entsoe_data = basic_requests(params=params_requests, start=start, end=end)\n",
    "# # entsoe_dict = xmltodict.parse(entsoe_data.text)\n",
    "# # entsoe_json = json.dumps(entsoe_dict, indent=4)\n",
    "# # print(entsoe_json)\n",
    "# print(entsoe_data.text)\n",
    "\n",
    "# '''\n",
    "# ----------------\n",
    "# LOAD\n",
    "# ----------------\n",
    "# '''\n",
    "# # #upload to GCS\n",
    "# # landing_filename=f\"entsoe_data_{start}.json\"\n",
    "# # upload_blob_to_gcs(bucket_name=gcs_bucket, contents=entsoe_json, destination_blob_name=landing_filename)\n",
    "\n",
    "# #upload to GCS - XML\n",
    "# landing_filename=f\"entsoe_data_{start}_{domain}.xml\"\n",
    "# upload_blob_to_gcs(bucket_name=gcs_bucket, contents=entsoe_data.text, destination_blob_name=landing_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#setting up session\n",
    "entsoe_client = EntsoeRawClient(security_token)\n",
    "'''\n",
    "----------------\n",
    "SETTING UP FUNCTION CALLS \n",
    "----------------\n",
    "'''\n",
    "\n",
    "# upload data to GCS\n",
    "def upload_blob_to_gcs(bucket_name, contents, destination_blob_name):\n",
    "    # Upload file to bucket\"\"\"\n",
    "\n",
    "    # ID of GCS bucket\n",
    "    # bucket_name =\n",
    "\n",
    "    # the contents from memory to be uploaded to file\n",
    "    # contents =\n",
    "\n",
    "    # the ID of your GCS object\n",
    "    # destination_blob_name =\n",
    "\n",
    "    storage_client = storage.Client(credentials=credentials)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_string(contents)\n",
    "\n",
    "\n",
    "'''\n",
    "----------------\n",
    "EXTRACTION\n",
    "----------------\n",
    "'''\n",
    "#for test, we'll be querying Actual Generation Output per Generation Unit\n",
    "\n",
    "start=pd.Timestamp('202101011200', tz='Europe/Berlin')\n",
    "end=pd.Timestamp('202101011300', tz='Europe/Berlin')\n",
    "country_code= 'CZ'\n",
    "country_code_from=''\n",
    "country_code_to=''\n",
    "type_marketagreement_type=''\n",
    "contract_marketagreement_type=''\n",
    "\n",
    "\n",
    "entsoe_data = entsoe_client.query_generation_per_plant(country_code, start, end)\n",
    "entsoe_file_dict = xmltodict.parse(entsoe_data)\n",
    "entsoe_json = json.dumps(entsoe_file_dict)\n",
    "\n",
    "'''\n",
    "----------------\n",
    "LOAD\n",
    "----------------\n",
    "'''\n",
    "#upload to GCS\n",
    "landing_filename=f\"entsoe_data_{country_code}.json\"\n",
    "upload_blob_to_gcs(bucket_name=gcs_bucket, contents=entsoe_json, destination_blob_name=landing_filename)\n",
    "\n",
    "# #upload to GCS - XML\n",
    "# landing_filename=f\"entsoe_data_{start}_{country_code}.xml\"\n",
    "# upload_blob_to_gcs(bucket_name=gcs_bucket, contents=entsoe_data, destination_blob_name=landing_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark-3.3.1-bin-hadoop3-scala2.13\n"
     ]
    }
   ],
   "source": [
    "print(os.environ[\"SPARK_HOME\"])\n",
    "SPARK_HOME = os.environ[\"SPARK_HOME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #setup sparksession for entry point\n",
    "# SPARK_HOME = os.environ[\"SPARK_HOME\"]\n",
    "# spark = SparkSession.builder.appName(\"gcp_playground\").config(\"spark.jars\", f\"{SPARK_HOME}/jars/spark-xml_2.13-0.15.0.jar\").getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/27 13:38:08 WARN Utils: Your hostname, pop-os resolves to a loopback address: 127.0.1.1; using 192.168.18.2 instead (on interface wlp58s0)\n",
      "22/12/27 13:38:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/12/27 13:38:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "## COBA SPARK GCS CONNECTOR ##\n",
    "\n",
    "\n",
    "# berangkat pak haji\n",
    "# setup parameternya\n",
    "gcs_bucket = gcs_bucket\n",
    "path = f\"gs://{gcs_bucket}/{landing_filename}\"\n",
    "# path =\"/home/rafzul/projects/entsoe-pipelines/sample.xml\"\n",
    "\n",
    "#coba spark gcs connector\n",
    "#setup sparksession for entry point - COBA GCS CONNECTOR\n",
    "SPARK_HOME = os.environ[\"SPARK_HOME\"]\n",
    "spark = SparkSession.builder.appName(\"gcp_playground\") \\\n",
    "    .config(\"spark.jars\", f\"{SPARK_HOME}/jars/gcs-connector-hadoop3-latest.jar\") \\\n",
    "    .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", service_account_file) \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google.cloud.hadoop.fs.gcs.GoogleHadoopFS\n"
     ]
    }
   ],
   "source": [
    "print(spark.conf.get(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://entsoe_analytics_1009/entsoe_data_CZ.json\n",
      "+--------------------+\n",
      "|   GL_MarketDocument|\n",
      "+--------------------+\n",
      "|{urn:iec62325.351...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = f\"gs://{gcs_bucket}/{landing_filename}\"\n",
    "print(path)\n",
    "\n",
    "#create dataframe from gcs\n",
    "df_spark = spark.read.format(\"json\") \\\n",
    "   .option(\"header\",\"true\") \\\n",
    "   .option(\"multiLine\",\"true\") \\\n",
    "   .option(\"inferSchema\",\"true\") \\\n",
    "   .load(path)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|   GL_MarketDocument|\n",
      "+--------------------+\n",
      "|{urn:iec62325.351...|\n",
      "+--------------------+\n",
      "\n",
      "root\n",
      " |-- GL_MarketDocument: struct (nullable = true)\n",
      " |    |-- @xmlns: string (nullable = true)\n",
      " |    |-- TimeSeries: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- MktPSRType: struct (nullable = true)\n",
      " |    |    |    |    |-- PowerSystemResources: struct (nullable = true)\n",
      " |    |    |    |    |    |-- mRID: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- #text: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- @codingScheme: string (nullable = true)\n",
      " |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |-- psrType: string (nullable = true)\n",
      " |    |    |    |-- Period: struct (nullable = true)\n",
      " |    |    |    |    |-- Point: struct (nullable = true)\n",
      " |    |    |    |    |    |-- position: string (nullable = true)\n",
      " |    |    |    |    |    |-- quantity: string (nullable = true)\n",
      " |    |    |    |    |-- resolution: string (nullable = true)\n",
      " |    |    |    |    |-- timeInterval: struct (nullable = true)\n",
      " |    |    |    |    |    |-- end: string (nullable = true)\n",
      " |    |    |    |    |    |-- start: string (nullable = true)\n",
      " |    |    |    |-- businessType: string (nullable = true)\n",
      " |    |    |    |-- curveType: string (nullable = true)\n",
      " |    |    |    |-- inBiddingZone_Domain.mRID: struct (nullable = true)\n",
      " |    |    |    |    |-- #text: string (nullable = true)\n",
      " |    |    |    |    |-- @codingScheme: string (nullable = true)\n",
      " |    |    |    |-- mRID: string (nullable = true)\n",
      " |    |    |    |-- objectAggregation: string (nullable = true)\n",
      " |    |    |    |-- quantity_Measure_Unit.name: string (nullable = true)\n",
      " |    |    |    |-- registeredResource.mRID: struct (nullable = true)\n",
      " |    |    |    |    |-- #text: string (nullable = true)\n",
      " |    |    |    |    |-- @codingScheme: string (nullable = true)\n",
      " |    |-- createdDateTime: string (nullable = true)\n",
      " |    |-- mRID: string (nullable = true)\n",
      " |    |-- process.processType: string (nullable = true)\n",
      " |    |-- receiver_MarketParticipant.mRID: struct (nullable = true)\n",
      " |    |    |-- #text: string (nullable = true)\n",
      " |    |    |-- @codingScheme: string (nullable = true)\n",
      " |    |-- receiver_MarketParticipant.marketRole.type: string (nullable = true)\n",
      " |    |-- revisionNumber: string (nullable = true)\n",
      " |    |-- sender_MarketParticipant.mRID: struct (nullable = true)\n",
      " |    |    |-- #text: string (nullable = true)\n",
      " |    |    |-- @codingScheme: string (nullable = true)\n",
      " |    |-- sender_MarketParticipant.marketRole.type: string (nullable = true)\n",
      " |    |-- time_Period.timeInterval: struct (nullable = true)\n",
      " |    |    |-- end: string (nullable = true)\n",
      " |    |    |-- start: string (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inferring schema and get the data type of each column and turn it into spark dataframe\n",
    "datatype_infer = pd.DataFrame.from_dict(xml_file_dict[0], orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, map_keys, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"@xmlns: <class 'str'>\", \"mRID: <class 'str'>\", \"createdDateTime: <class 'str'>\", \"time_Period.timeInterval: <class 'str'>\", \"TimeSeries: <class 'str'>\"]\n",
      "root\n",
      " |-- @xmlns: string (nullable = true)\n",
      " |-- TimeSeries: array (nullable = true)\n",
      " |    |-- element: map (containsNull = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      " |-- createdDateTime: string (nullable = true)\n",
      " |-- mRID: string (nullable = true)\n",
      " |-- time_Period.timeInterval: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+--------------------+------------------------+\n",
      "|              @xmlns|          TimeSeries|     createdDateTime|                mRID|time_Period.timeInterval|\n",
      "+--------------------+--------------------+--------------------+--------------------+------------------------+\n",
      "|urn:iec62325.351:...|[{Period -> {time...|2022-12-23T08:05:44Z|dca3a3bd038f41818...|    {start -> 2021-01...|\n",
      "+--------------------+--------------------+--------------------+--------------------+------------------------+\n",
      "\n",
      "('@xmlns', 'string')\n",
      "('TimeSeries', 'array<map<string,string>>')\n",
      "('createdDateTime', 'string')\n",
      "('mRID', 'string')\n",
      "('time_Period.timeInterval', 'map<string,string>')\n"
     ]
    }
   ],
   "source": [
    "#coba upload ke dataframe dari xml\n",
    "\n",
    "\n",
    "with open(\"entsoe_analytics_1009/entsoe_data_simple.xml\") as source_file:\n",
    "    xml_file_dict = xmltodict.parse(source_file.read())\n",
    "    types = [(f\"{k}: {type(k)}\") for k in xml_file_dict[\"GL_MarketDocument\"]]\n",
    "    print(types)\n",
    "    df_spark = spark.createDataFrame([xml_file_dict[\"GL_MarketDocument\"]])\n",
    "    df_spark.printSchema()\n",
    "    df_spark.show()\n",
    "    for col in df_spark.dtypes:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df_spark.schema.prettyJson()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten nested df at every layer\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.import functions as f\n",
    "\n",
    "def flatten_structs(nested_df):\n",
    "    stack = [(), nested_df]\n",
    "    columns = []\n",
    "    \n",
    "    while len(stack) > 0:\n",
    "        parents\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
